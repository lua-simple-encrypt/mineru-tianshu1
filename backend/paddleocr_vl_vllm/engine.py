"""
PaddleOCR-VL-VLLM è§£æžå¼•æ“Ž (Optimized)
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡åŸºç¡€ç‰ˆé¢è¯†åˆ«æ¨¡åž‹, OCRéƒ¨åˆ†è°ƒç”¨é…ç½®çš„API

ðŸš¨ CRITICAL FIX APPLIED: 
å¼ºåˆ¶å•çº¿ç¨‹æŽ¨ç†ä»¥è§£å†³ vLLM Tokenizer "Already borrowed" ç«žæ€å´©æºƒé—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼šhttps://www.paddleocr.ai/latest/version3.x/pipeline_usage/PaddleOCR-VL.html
"""

import os
import gc
import json
import time
import requests
import traceback
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger

# ==============================================================================
# ðŸš¨ å…¨å±€çŽ¯å¢ƒé…ç½® (å¿…é¡»åœ¨å¯¼å…¥ paddle/paddlex ä¹‹å‰è®¾ç½®)
# ==============================================================================
# 1. é™åˆ¶ PaddleX å†…éƒ¨æŽ¨ç†å¹¶å‘æ•°ä¸º 1ï¼Œé˜²æ­¢é«˜å¹¶å‘è¯·æ±‚å†²åž® vLLM çš„ Tokenizer
os.environ["PADDLEX_INFERENCE_PARALLEL_WORKER_NUM"] = "1"
# 2. ç¦ç”¨æ¨¡åž‹æºæ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦ (å†…ç½‘çŽ¯å¢ƒå¿…å¤‡)
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True"
# ==============================================================================

class PaddleOCRVLVLLMEngine:
    """
    PaddleOCR-VL-VLLM è§£æžå¼•æ“Žï¼ˆä¼ä¸šçº§ä¼˜åŒ–ç‰ˆï¼‰

    ç‰¹æ€§ï¼š
    - ç¨³å®šä¼˜å…ˆï¼šå¼ºåˆ¶ä¸²è¡Œè¯·æ±‚ï¼Œæ¶ˆé™¤åº•å±‚ Rust Tokenizer å´©æºƒ
    - è‡ªåŠ¨å¤šè¯­è¨€ï¼šæ”¯æŒ 109+ ç§è¯­è¨€è‡ªåŠ¨è¯†åˆ«
    - æ˜¾å­˜ä¿æŠ¤ï¼šæµå¼å¤„ç† + æ¿€è¿›çš„ GC ç­–ç•¥
    - æ•…éšœéš”ç¦»ï¼šé¢„æ£€æŸ¥ VLLM æœåŠ¡çŠ¶æ€
    """

    _instance: Optional["PaddleOCRVLVLLMEngine"] = None
    _lock = Lock()
    _pipeline = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, 
                 device: str = "cuda:0", 
                 vllm_api_base: str = None, 
                 model_name: str = "PaddleOCR-VL-1.5-0.9B"):
        """
        åˆå§‹åŒ–å¼•æ“Ž
        """
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå…¶æ¬¡çŽ¯å¢ƒå˜é‡ï¼Œæœ€åŽé»˜è®¤ Docker å†…éƒ¨åœ°å€
            self.vllm_api_base = vllm_api_base or os.getenv("VLLM_API_BASE", "http://vllm-paddleocr:30023/v1")
            self.model_name = model_name

            # æå– GPU ID
            if "cuda:" in device:
                try:
                    self.gpu_id = int(device.split(":")[-1])
                except ValueError:
                    self.gpu_id = 0
            else:
                self.gpu_id = 0

            self._check_gpu_availability()
            self._initialized = True

            logger.info("ðŸ”§ PaddleOCR-VL-VLLM Engine Initialized")
            logger.info(f"   Device: {self.device} (Physical GPU: {self.gpu_id})")
            logger.info(f"   VLLM API: {self.vllm_api_base}")
            logger.info(f"   Concurrency: Serial Mode (Safe)")

    def _check_gpu_availability(self):
        try:
            import paddle
            if not paddle.is_compiled_with_cuda():
                logger.error("âŒ PaddlePaddle is running on CPU! This model requires GPU.")
                return
            
            gpu_name = paddle.device.cuda.get_device_name(self.gpu_id)
            logger.info(f"âœ… GPU Detected: {gpu_name}")
        except Exception:
            logger.warning("âš ï¸ Could not verify GPU status via PaddlePaddle")

    def _check_vllm_health(self) -> bool:
        """æ£€æŸ¥ VLLM æœåŠ¡æ˜¯å¦å¥åº·"""
        try:
            # æž„é€ å¥åº·æ£€æŸ¥ URL (åŽ»é™¤ /v1 åŽç¼€)
            base_url = self.vllm_api_base.replace("/v1", "")
            health_url = f"{base_url}/health"
            
            # å°è¯•è¯·æ±‚ /health æˆ– /v1/models
            try:
                requests.get(health_url, timeout=2)
                return True
            except:
                # å›žé€€å°è¯• models æŽ¥å£
                models_url = f"{self.vllm_api_base}/models"
                resp = requests.get(models_url, timeout=2)
                return resp.status_code == 200
        except Exception as e:
            logger.warning(f"âš ï¸ VLLM service check failed: {e}")
            return False

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ç®¡é“"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            # 1. é¢„æ£€æŸ¥ VLLM æœåŠ¡
            if not self._check_vllm_health():
                logger.error(f"âŒ VLLM service unreachable at {self.vllm_api_base}")
                logger.error("   Please ensure the 'vllm-paddleocr' container is running.")
                # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå°è¯•ç»§ç»­åŠ è½½ï¼Œå› ä¸ºæœ‰æ—¶ç½‘ç»œå¯èƒ½çŸ­æš‚æ³¢åŠ¨

            logger.info("=" * 60)
            logger.info("ðŸ“¥ Loading PaddleOCR-VL-VLLM Pipeline...")
            logger.info("=" * 60)

            try:
                import paddle
                from paddleocr import PaddleOCRVL

                if paddle.is_compiled_with_cuda():
                    paddle.set_device(f"gpu:{self.gpu_id}")

                # è®¾ç½® PaddleX ä¸»ç›®å½•
                pdx_home = os.environ.get("PADDLEX_HOME", "/root/.paddlex")
                
                # åˆå§‹åŒ–ç®¡é“
                self._pipeline = PaddleOCRVL(
                    vl_rec_backend="vllm-server",
                    vl_rec_server_url=self.vllm_api_base,
                )
                
                logger.info("âœ… Pipeline loaded successfully (Serial Mode Active)")
                return self._pipeline

            except Exception as e:
                logger.error(f"âŒ Pipeline load failed: {e}")
                logger.error(traceback.format_exc())
                raise

    def cleanup(self):
        """æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†"""
        try:
            import paddle
            if paddle.device.is_compiled_with_cuda():
                paddle.device.cuda.empty_cache()
            gc.collect()
        except:
            pass

    def parse(self, file_path: str, output_path: str, **kwargs) -> Dict[str, Any]:
        """
        è§£æžæ–‡æ¡£å…¥å£
        """
        file_path = Path(file_path)
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"ðŸ¤– Processing: {file_path.name}")
        
        pipeline = self._load_pipeline()

        # å‚æ•°æ˜ å°„
        param_mapping = {
            "useDocOrientationClassify": "use_doc_orientation_classify",
            "useDocUnwarping": "use_doc_unwarping",
            "useLayoutDetection": "use_layout_parsing",
            "useChartRecognition": "use_chart_recognition",
            "useSealRecognition": "use_seal_recognition",
            "useOcrForImageBlock": "use_ocr_for_image_block",
            "layoutNms": "layout_nms",
            "markdownIgnoreLabels": "markdown_ignore_labels",
            "mergeTables": "merge_tables",
            "relevelTitles": "relevel_titles",
            "restructurePages": "restructure_pages",
            "minPixels": "min_pixels",
            "maxPixels": "max_pixels",
        }

        predict_params = {"input": str(file_path)}
        
        # é»˜è®¤å‚æ•°
        defaults = {
            "use_layout_parsing": True,
            "use_doc_orientation_classify": False,  # é»˜è®¤å…³é—­ä»¥é˜²å´©æºƒ
            "use_doc_unwarping": False,
            "use_seal_recognition": True
        }
        
        # å¡«å……å‚æ•°
        for k, v in kwargs.items():
            if k in param_mapping:
                predict_params[param_mapping[k]] = v
        
        for k, v in defaults.items():
            if k not in predict_params:
                predict_params[k] = v

        try:
            # ðŸš€ æ‰§è¡ŒæŽ¨ç†
            # æ³¨æ„ï¼šç”±äºŽæˆ‘ä»¬åœ¨æ–‡ä»¶å¤´è®¾ç½®äº† PARALLEL_WORKER_NUM=1
            # è¿™é‡Œå³ä½¿æ˜¯å¤§æ–‡ä»¶ï¼Œä¹Ÿä¼šä¸€é¡µé¡µä¸²è¡Œå‘é€ç»™ VLLMï¼Œä¸ä¼šå†è§¦å‘ 400 é”™è¯¯
            output_generator = pipeline.predict(**predict_params)

            markdown_pages = []
            markdown_list_obj = []
            json_list = []
            page_count = 0

            for res in output_generator:
                page_count += 1
                
                # ðŸ›¡ï¸ é˜²å¾¡æ€§æ£€æŸ¥ï¼šé˜²æ­¢ NoneType é”™è¯¯
                if res is None:
                    logger.error(f"âŒ Page {page_count} returned None result")
                    continue

                page_output_dir = output_path / f"page_{page_count}"
                page_output_dir.mkdir(parents=True, exist_ok=True)

                # ä¿å­˜ä¸­é—´å›¾å’ŒJSON
                try:
                    if hasattr(res, "save_to_img"): res.save_to_img(str(page_output_dir))
                    if hasattr(res, "save_to_json"): res.save_to_json(str(page_output_dir))
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to save intermediate files for page {page_count}: {e}")

                # æ”¶é›†æ•°æ®
                if hasattr(res, "json") and res.json:
                    json_list.append(res.json)

                if hasattr(res, "markdown") and res.markdown:
                    markdown_list_obj.append(res.markdown)

                # æå– Markdown æ–‡æœ¬
                page_md = ""
                try:
                    if hasattr(res, "markdown") and res.markdown:
                        if isinstance(res.markdown, dict):
                            page_md = res.markdown.get('markdown_texts', '') or res.markdown.get('text', '')
                        elif hasattr(res.markdown, 'markdown_texts'):
                            page_md = res.markdown.markdown_texts
                        else:
                            page_md = str(res.markdown)
                    elif hasattr(res, "str") and res.str:
                        page_md = str(res.str)
                except Exception as e:
                    logger.warning(f"âš ï¸ Error extracting markdown from page {page_count}: {e}")

                if page_md:
                    markdown_pages.append(page_md)
                else:
                    # å…œåº•ï¼šå°è¯•è¯»å–ç”±äºŽ save_to_markdown ç”Ÿæˆçš„æ–‡ä»¶
                    try:
                         if hasattr(res, "save_to_markdown"):
                            res.save_to_markdown(str(page_output_dir))
                            saved = list(page_output_dir.glob("*.md"))
                            if saved:
                                markdown_pages.append(saved[0].read_text(encoding="utf-8"))
                    except:
                        pass
                
                logger.info(f"âœ… Processed Page {page_count}")

            # åˆå¹¶ç»“æžœ
            logger.info(f"ðŸŽ‰ Processing complete. Total pages: {page_count}")

            markdown_text = ""
            # å°è¯•ä½¿ç”¨å®˜æ–¹åˆå¹¶ç®—æ³•
            if hasattr(pipeline, "concatenate_markdown_pages") and markdown_list_obj:
                try:
                    markdown_text = pipeline.concatenate_markdown_pages(markdown_list_obj)
                except Exception as e:
                    logger.warning(f"Official concat failed: {e}, falling back to simple join")
                    markdown_text = "\n\n---\n\n".join(markdown_pages)
            else:
                markdown_text = "\n\n---\n\n".join(markdown_pages)

            # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
            markdown_file = output_path / "result.md"
            markdown_file.write_text(markdown_text, encoding="utf-8")
            
            json_file = output_path / "result.json"
            combined_json = {"pages": json_list, "total_pages": page_count}
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(combined_json, f, ensure_ascii=False, indent=2)

            return {
                "success": True,
                "output_path": str(output_path),
                "markdown": markdown_text,
                "markdown_file": str(markdown_file),
                "json_file": str(json_file),
            }

        except Exception as e:
            logger.error(f"âŒ OCR Pipeline Critical Error: {e}")
            logger.error(traceback.format_exc())
            raise
        finally:
            self.cleanup()

# å…¨å±€å•ä¾‹
_engine = None

def get_engine(vllm_api_base: str = None, model_name: str = "PaddleOCR-VL-1.5-0.9B") -> PaddleOCRVLVLLMEngine:
    global _engine
    if _engine is None:
        _engine = PaddleOCRVLVLLMEngine(vllm_api_base=vllm_api_base, model_name=model_name)
    return _engine
