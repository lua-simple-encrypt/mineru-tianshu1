"""
PaddleOCR-VL-VLLM è§£æžå¼•æ“Ž (Optimized + Bidirectional Layout Support)
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡åŸºç¡€ç‰ˆé¢è¯†åˆ«æ¨¡åž‹, OCRéƒ¨åˆ†è°ƒç”¨é…ç½®çš„API

åŠŸèƒ½å¢žå¼º (2026-02-15):
1. [åŒå‘å®šä½] è¾“å‡ºåŒ…å« bbox çš„ç»“æž„åŒ–æ•°æ® (json_content)ï¼Œæ”¯æŒå‰ç«¯ç‚¹å‡»è·³è½¬ã€‚
2. [èµ„æºç®¡ç†] åŒ…å«æ™ºèƒ½æ˜¾å­˜ä¼‘çœ  (Auto-Sleep) å’Œè‡ªåŠ¨å”¤é†’ (Auto-Wakeup)ã€‚
3. [ç¨³å®šæ€§] å¼ºåˆ¶å•çº¿ç¨‹æŽ¨ç†ä»¥è§£å†³ vLLM Tokenizer ç«žæ€å´©æºƒã€‚
"""

import os
import gc
import json
import time
import requests
import traceback
import threading
import uuid
from pathlib import Path
from typing import Optional, Dict, Any, List
from threading import Lock
from loguru import logger

# ==============================================================================
# ðŸš¨ å…¨å±€çŽ¯å¢ƒé…ç½® (å¿…é¡»åœ¨å¯¼å…¥ paddle/paddlex ä¹‹å‰è®¾ç½®)
# ==============================================================================
# 1. é™åˆ¶ PaddleX å†…éƒ¨æŽ¨ç†å¹¶å‘æ•°ä¸º 1ï¼Œé˜²æ­¢é«˜å¹¶å‘è¯·æ±‚å†²åž® vLLM çš„ Tokenizer
os.environ["PADDLEX_INFERENCE_PARALLEL_WORKER_NUM"] = "1"
# 2. ç¦ç”¨æ¨¡åž‹æºæ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦ (å†…ç½‘çŽ¯å¢ƒå¿…å¤‡)
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True"
# ==============================================================================

class PaddleOCRVLVLLMEngine:
    """
    PaddleOCR-VL-VLLM è§£æžå¼•æ“Žï¼ˆæ”¯æŒåŒå‘å®šä½æ•°æ®è¾“å‡ºï¼‰
    """

    _instance: Optional["PaddleOCRVLVLLMEngine"] = None
    _lock = Lock()
    _pipeline = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, 
                 device: str = "cuda:0", 
                 vllm_api_base: str = None, 
                 model_name: str = "PaddleOCR-VL-1.5-0.9B"):
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            self.vllm_api_base = vllm_api_base or os.getenv("VLLM_API_BASE", "http://vllm-paddleocr:30023/v1")
            self.model_name = model_name

            if "cuda:" in device:
                try:
                    self.gpu_id = int(device.split(":")[-1])
                except ValueError:
                    self.gpu_id = 0
            else:
                self.gpu_id = 0

            self._check_gpu_availability()
            
            # =========================================================
            # [èµ„æºç®¡ç†] æ™ºèƒ½æ˜¾å­˜ç®¡ç†çŠ¶æ€å˜é‡
            # =========================================================
            self.last_active_time = time.time()
            self.is_processing = False
            self.is_offloaded = True 
            self.idle_timeout = 300  # 5åˆ†é’Ÿæ— æ“ä½œè‡ªåŠ¨å¸è½½

            # å¯åŠ¨ç›‘æŽ§çº¿ç¨‹
            self._monitor_thread = threading.Thread(target=self._auto_sleep_monitor, daemon=True)
            self._monitor_thread.start()

            self._initialized = True

            logger.info("ðŸ”§ PaddleOCR-VL-VLLM Engine Initialized")
            logger.info(f"   Device: {self.device} (Physical GPU: {self.gpu_id})")
            logger.info(f"   VLLM API: {self.vllm_api_base}")
            logger.info(f"   Auto-Sleep: Enabled ({self.idle_timeout}s)")

    def _check_gpu_availability(self):
        try:
            import paddle
            if not paddle.is_compiled_with_cuda():
                logger.error("âŒ PaddlePaddle is running on CPU! This model requires GPU.")
                return
            gpu_name = paddle.device.cuda.get_device_name(self.gpu_id)
            logger.info(f"âœ… GPU Detected: {gpu_name}")
        except Exception:
            logger.warning("âš ï¸ Could not verify GPU status via PaddlePaddle")

    def _check_vllm_health(self) -> bool:
        """æ£€æŸ¥ VLLM æœåŠ¡æ˜¯å¦å¥åº·"""
        try:
            base_url = self.vllm_api_base.replace("/v1", "")
            health_url = f"{base_url}/health"
            try:
                requests.get(health_url, timeout=2)
                return True
            except:
                models_url = f"{self.vllm_api_base}/models"
                resp = requests.get(models_url, timeout=2)
                return resp.status_code == 200
        except Exception as e:
            logger.warning(f"âš ï¸ VLLM service check failed: {e}")
            return False

    def _auto_sleep_monitor(self):
        """[åŽå°çº¿ç¨‹] ç›‘æŽ§ç©ºé—²çŠ¶æ€"""
        while True:
            time.sleep(10)
            try:
                if self.is_processing or self.is_offloaded:
                    continue
                
                if time.time() - self.last_active_time > self.idle_timeout:
                    logger.info(f"ðŸ’¤ PaddleOCR-VLLM idle for {self.idle_timeout}s. Unloading pipeline...")
                    self.cleanup()
                    self.is_offloaded = True
            except Exception as e:
                logger.error(f"Monitor error: {e}")

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ç®¡é“"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            if not self._check_vllm_health():
                logger.error(f"âŒ VLLM service unreachable at {self.vllm_api_base}")

            logger.info("=" * 60)
            logger.info("ðŸ“¥ Loading PaddleOCR-VL-VLLM Pipeline...")
            logger.info("=" * 60)

            try:
                import paddle
                from paddleocr import PaddleOCRVL

                if paddle.is_compiled_with_cuda():
                    paddle.set_device(f"gpu:{self.gpu_id}")

                # è®¾ç½® PaddleX ä¸»ç›®å½•
                pdx_home = os.environ.get("PADDLEX_HOME", "/root/.paddlex")
                
                # åˆå§‹åŒ–ç®¡é“
                self._pipeline = PaddleOCRVL(
                    vl_rec_backend="vllm-server",
                    vl_rec_server_url=self.vllm_api_base,
                )
                
                logger.info("âœ… Pipeline loaded successfully")
                return self._pipeline

            except Exception as e:
                logger.error(f"âŒ Pipeline load failed: {e}")
                logger.error(traceback.format_exc())
                raise

    def cleanup(self):
        """é‡Šæ”¾æ˜¾å­˜"""
        with self._lock:
            self._pipeline = None
            try:
                import paddle
                if paddle.device.is_compiled_with_cuda():
                    paddle.device.cuda.empty_cache()
                gc.collect()
                logger.info("âœ… VRAM released.")
            except:
                pass

    def parse(self, file_path: str, output_path: str, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œè§£æžå¹¶è¿”å›žç»“æž„åŒ–æ•°æ®ï¼ˆåŒ…å« bboxï¼‰
        """
        # 1. è‡ªåŠ¨å”¤é†’
        self.is_processing = True
        self.last_active_time = time.time()
        
        if self.is_offloaded:
            logger.info("ðŸš€ New task received. Waking up PaddleOCR-VLLM engine...")
            self.is_offloaded = False

        try:
            file_path = Path(file_path)
            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)

            logger.info(f"ðŸ¤– Processing: {file_path.name}")
            
            pipeline = self._load_pipeline()

            # å‚æ•°æ˜ å°„ (ä¿æŒä¸Ž Worker ä¸€è‡´)
            param_mapping = {
                "useDocOrientationClassify": "use_doc_orientation_classify",
                "useDocUnwarping": "use_doc_unwarping",
                "useLayoutDetection": "use_layout_parsing",
                "useChartRecognition": "use_chart_recognition",
                "useSealRecognition": "use_seal_recognition",
                "useOcrForImageBlock": "use_ocr_for_image_block",
                "layoutNms": "layout_nms",
                "markdownIgnoreLabels": "markdown_ignore_labels",
                "mergeTables": "merge_tables",
                "relevelTitles": "relevel_titles",
                "restructurePages": "restructure_pages",
                "minPixels": "min_pixels",
                "maxPixels": "max_pixels",
            }

            predict_params = {"input": str(file_path)}
            for k, v in kwargs.items():
                if k in param_mapping:
                    predict_params[param_mapping[k]] = v
            
            # é»˜è®¤å‚æ•°
            if "use_layout_parsing" not in predict_params: predict_params["use_layout_parsing"] = True
            if "use_doc_orientation_classify" not in predict_params: predict_params["use_doc_orientation_classify"] = False
            
            # ðŸš€ æ‰§è¡ŒæŽ¨ç†
            output_generator = pipeline.predict(**predict_params)

            markdown_pages = []
            markdown_list_obj = []
            
            # [å…³é”®] ç”¨äºŽå­˜å‚¨æ‰€æœ‰é¡µé¢çš„ç»“æž„åŒ–æ•°æ® (å« bbox)
            full_content_list = [] 
            page_count = 0

            for res in output_generator:
                page_count += 1
                if res is None: continue

                page_output_dir = output_path / f"page_{page_count}"
                page_output_dir.mkdir(parents=True, exist_ok=True)

                # 1. ä¿å­˜å›¾ç‰‡å’Œ JSON
                if hasattr(res, "save_to_img"): res.save_to_img(str(page_output_dir))
                if hasattr(res, "save_to_json"): res.save_to_json(str(page_output_dir))

                # 2. æå–ç»“æž„åŒ–æ•°æ® (åŒ…å« BBox)
                # PaddleX çš„ res.json é€šå¸¸åŒ…å« 'res' åˆ—è¡¨ï¼Œé‡Œé¢æœ‰ layout_bbox å’Œ text
                if hasattr(res, "json") and res.json:
                    page_data = res.json
                    
                    # å°è¯•ä»Ž PaddleX ç»“æžœä¸­æå– blocks
                    # ç»“æž„é€šå¸¸æ˜¯: {'res': [{'bbox': [x,y,x,y], 'text': '...', 'type': '...'}, ...]}
                    if isinstance(page_data, dict) and 'res' in page_data:
                        blocks = page_data['res']
                        for block in blocks:
                            # è§„èŒƒåŒ– Block æ•°æ®ä¾›å‰ç«¯ä½¿ç”¨
                            clean_block = {
                                "id": len(full_content_list) + 1, # å…¨å±€å”¯ä¸€ ID
                                "page_idx": page_count - 1,       # 0-based page index
                                "type": block.get('type', 'text'),
                                "text": block.get('text', ''),
                                "bbox": block.get('layout_bbox') or block.get('bbox') or [], # ç¡®ä¿æœ‰åæ ‡
                                "score": block.get('score', 0)
                            }
                            # åªæœ‰æœ‰åæ ‡å’Œå†…å®¹çš„å—æ‰æ·»åŠ 
                            if clean_block['bbox'] and (clean_block['text'] or clean_block['type'] in ['image', 'table']):
                                full_content_list.append(clean_block)

                # 3. æå– Markdown
                if hasattr(res, "markdown") and res.markdown:
                    markdown_list_obj.append(res.markdown)
                    # å°è¯•æå–å­—ç¬¦ä¸² markdown
                    if hasattr(res.markdown, 'markdown_texts'):
                        markdown_pages.append(res.markdown.markdown_texts)
                    elif isinstance(res.markdown, dict):
                        markdown_pages.append(res.markdown.get('markdown_texts', ''))
                    else:
                        markdown_pages.append(str(res.markdown))
                
                logger.info(f"âœ… Processed Page {page_count}")

            # åˆå¹¶ Markdown
            if hasattr(pipeline, "concatenate_markdown_pages") and markdown_list_obj:
                try:
                    markdown_text = pipeline.concatenate_markdown_pages(markdown_list_obj)
                except:
                    markdown_text = "\n\n---\n\n".join(markdown_pages)
            else:
                markdown_text = "\n\n---\n\n".join(markdown_pages)

            # ä¿å­˜ç»“æžœ
            (output_path / "result.md").write_text(markdown_text, encoding="utf-8")
            
            # [å…³é”®] ç”Ÿæˆ content_list.json (æ‰å¹³åŒ–ç»“æž„ï¼Œä¾›å‰ç«¯å®šä½ä½¿ç”¨)
            # å¦‚æžœ full_content_list ä¸ºç©ºï¼ˆæŸäº›æ¨¡åž‹æ¨¡å¼ä¸‹ï¼‰ï¼Œå°è¯•ç”¨ json_list å…œåº•ï¼Œæˆ–è€…å‰ç«¯åšå…¼å®¹
            final_json_data = full_content_list
            
            # ä¿å­˜ detailed JSON
            json_file = output_path / "result.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(final_json_data, f, ensure_ascii=False, indent=2)

            return {
                "success": True,
                "output_path": str(output_path),
                "markdown": markdown_text,
                "markdown_file": str(output_path / "result.md"),
                "json_file": str(json_file),
                # è¿”å›ž json_content ç»™ workerï¼Œworker ä¼šå°†å…¶å­˜å…¥ DB
                "json_content": final_json_data 
            }

        except Exception as e:
            logger.error(f"âŒ OCR Pipeline Critical Error: {e}")
            logger.error(traceback.format_exc())
            raise
        finally:
            self.is_processing = False
            self.last_active_time = time.time()
            logger.info("ðŸ Task finished. Pipeline remains loaded.")

# å…¨å±€å•ä¾‹
_engine = None

def get_engine(vllm_api_base: str = None, model_name: str = "PaddleOCR-VL-1.5-0.9B") -> PaddleOCRVLVLLMEngine:
    global _engine
    if _engine is None:
        _engine = PaddleOCRVLVLLMEngine(vllm_api_base=vllm_api_base, model_name=model_name)
    return _engine
