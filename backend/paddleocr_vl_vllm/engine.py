"""
PaddleOCR-VL-VLLM è§£æžå¼•æ“Ž (Ultimate Optimized Edition)
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡åŸºç¡€ç‰ˆé¢è¯†åˆ«æ¨¡åž‹, OCRéƒ¨åˆ†è°ƒç”¨é…ç½®çš„API

åŠŸèƒ½å¢žå¼º:
1. [ç¨³å®šæ€§] å¼ºåˆ¶å•çº¿ç¨‹æŽ¨ç†ä»¥è§£å†³ vLLM Tokenizer "Already borrowed" ç«žæ€å´©æºƒé—®é¢˜
2. [é˜²å´©æºƒ] å¢žåŠ  VLM NoneType å¼‚å¸¸æ•èŽ·ä¸Žé™çº§é‡è¯•æœºåˆ¶ (Fallback)
3. [åŒå‘å®šä½] è¾“å‡ºåŒ…å« bbox çš„ç»“æž„åŒ–æ•°æ® (json_content)ï¼Œä¾›å‰ç«¯åŒå±è”åŠ¨ï¼Œå·²é€‚é… block_order æŽ’åº
4. [èµ„æºç®¡ç†] æ™ºèƒ½æ˜¾å­˜ä¼‘çœ  (Auto-Sleep) å’Œè‡ªåŠ¨å”¤é†’ (Auto-Wakeup)
5. [é«˜å¯ç”¨] èžåˆ MD æ–‡ä»¶æœ¬åœ°æå–å…œåº•ä¸Ž PADDLEX_HOME çŽ¯å¢ƒé”å®š
6. [å¹¶å‘æŽ§åˆ¶] æ‹¦æˆªåº•å±‚ HTTP å®¢æˆ·ç«¯å¼ºåˆ¶ä¸²è¡ŒåŒ–ï¼Œå½»åº•è§£å†³ vLLM ç«¯ Tokenizer å´©æºƒ
"""

import os
import gc
import json
import time
import requests
import traceback
import threading
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger

# ==============================================================================
# ðŸš¨ å…¨å±€çŽ¯å¢ƒé…ç½® (å¿…é¡»åœ¨å¯¼å…¥ paddle/paddlex ä¹‹å‰è®¾ç½®)
# ==============================================================================
# 1. é™åˆ¶ PaddleX å†…éƒ¨æŽ¨ç†å¹¶å‘æ•°ä¸º 1ï¼Œé˜²æ­¢é«˜å¹¶å‘è¯·æ±‚å†²åž® vLLM çš„ Tokenizer
os.environ["PADDLEX_INFERENCE_PARALLEL_WORKER_NUM"] = "1"
os.environ["PADDLEX_API_MAX_WORKERS"] = "1"
# 2. ç¦ç”¨æ¨¡åž‹æºæ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦ (å†…ç½‘çŽ¯å¢ƒå¿…å¤‡)
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True"

# ==============================================================================
# ðŸš¨ ç»ˆæžé˜²å¾¡ï¼šæ‹¦æˆª HTTP å®¢æˆ·ç«¯ï¼Œé™åˆ¶ VLLM é«˜å¹¶å‘è¯·æ±‚
# å½»åº•è§£å†³ vLLM Tokenizer "RuntimeError: Already borrowed" å´©æºƒé—®é¢˜
# ==============================================================================
try:
    import httpx

    # å…¨å±€ä¿¡å·é‡ï¼Œå¼ºåˆ¶å®Œå…¨ä¸²è¡Œï¼Œæœç»è¿œç«¯ Tokenizer çš„ Rust å€Ÿç”¨å†²çª
    _vllm_semaphore = threading.Semaphore(1)  

    # 1. Patch HTTPX (Sync - OpenAI SDK åº•å±‚ä½¿ç”¨)
    _original_httpx_send = httpx.Client.send
    def _throttled_httpx_send(self, request, *args, **kwargs):
        if "chat/completions" in str(request.url):
            with _vllm_semaphore:
                return _original_httpx_send(self, request, *args, **kwargs)
        return _original_httpx_send(self, request, *args, **kwargs)
    httpx.Client.send = _throttled_httpx_send

    # 2. Patch HTTPX (Async)
    _original_async_send = httpx.AsyncClient.send
    _async_semaphores = {}
    _async_sem_lock = threading.Lock()

    def _get_async_sem():
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            return asyncio.Semaphore(1)
            
        with _async_sem_lock:
            if loop not in _async_semaphores:
                _async_semaphores[loop] = asyncio.Semaphore(1)
            return _async_semaphores[loop]

    async def _throttled_async_send(self, request, *args, **kwargs):
        if "chat/completions" in str(request.url):
            sem = _get_async_sem()
            async with sem:
                return await _original_async_send(self, request, *args, **kwargs)
        return await _original_async_send(self, request, *args, **kwargs)
    httpx.AsyncClient.send = _throttled_async_send

    # 3. Patch Requests (Sync - å…¼å®¹æ—§ç‰ˆæˆ–ç¬¬ä¸‰æ–¹åº“)
    _original_requests_send = requests.Session.send
    def _throttled_requests_send(self, request, **kwargs):
        if hasattr(request, 'url') and "chat/completions" in str(request.url):
            with _vllm_semaphore:
                return _original_requests_send(self, request, **kwargs)
        return _original_requests_send(self, request, **kwargs)
    requests.Session.send = _throttled_requests_send

    logger.info("ðŸ›¡ï¸ VLLM Network Throttling Patch applied successfully.")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to patch HTTP clients: {e}")
# ==============================================================================

class PaddleOCRVLVLLMEngine:
    """
    PaddleOCR-VL-VLLM è§£æžå¼•æ“Žï¼ˆä¼ä¸šçº§é«˜å¯ç”¨ç‰ˆï¼‰
    """

    _instance: Optional["PaddleOCRVLVLLMEngine"] = None
    _lock = Lock()
    _pipeline = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, 
                 device: str = "cuda:0", 
                 vllm_api_base: str = None, 
                 model_name: str = "PaddleOCR-VL-1.5-0.9B"):
        """
        åˆå§‹åŒ–å¼•æ“Ž
        """
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå…¶æ¬¡çŽ¯å¢ƒå˜é‡ï¼Œæœ€åŽé»˜è®¤ Docker å†…éƒ¨åœ°å€
            self.vllm_api_base = vllm_api_base or os.getenv("VLLM_API_BASE", "http://vllm-paddleocr:30023/v1")
            self.model_name = model_name

            # æå– GPU ID
            if "cuda:" in device:
                try:
                    self.gpu_id = int(device.split(":")[-1])
                except ValueError:
                    self.gpu_id = 0
            else:
                self.gpu_id = 0

            self._check_gpu_availability()
            
            # =========================================================
            # [èµ„æºç®¡ç†] æ™ºèƒ½æ˜¾å­˜ç®¡ç†çŠ¶æ€å˜é‡
            # =========================================================
            self.last_active_time = time.time()
            self.is_processing = False
            self.is_offloaded = True 
            self.idle_timeout = 300  # 5åˆ†é’Ÿæ— æ“ä½œè‡ªåŠ¨å¸è½½

            # å¯åŠ¨ç›‘æŽ§çº¿ç¨‹
            self._monitor_thread = threading.Thread(target=self._auto_sleep_monitor, daemon=True)
            self._monitor_thread.start()

            self._initialized = True

            logger.info("ðŸ”§ PaddleOCR-VL-VLLM Engine Initialized")
            logger.info(f"   Device: {self.device} (Physical GPU: {self.gpu_id})")
            logger.info(f"   VLLM API: {self.vllm_api_base}")
            logger.info(f"   Concurrency: Serial Mode (Safe Network Patch Active)")
            logger.info(f"   Auto-Sleep: Enabled ({self.idle_timeout}s)")

    def _check_gpu_availability(self):
        try:
            import paddle
            if not paddle.is_compiled_with_cuda():
                logger.error("âŒ PaddlePaddle is running on CPU! This model requires GPU.")
                return
            
            gpu_name = paddle.device.cuda.get_device_name(self.gpu_id)
            logger.info(f"âœ… GPU Detected: {gpu_name}")
        except Exception:
            logger.warning("âš ï¸ Could not verify GPU status via PaddlePaddle")

    def _check_vllm_health(self) -> bool:
        """æ£€æŸ¥ VLLM æœåŠ¡æ˜¯å¦å¥åº·"""
        try:
            # æž„é€ å¥åº·æ£€æŸ¥ URL (åŽ»é™¤ /v1 åŽç¼€)
            base_url = self.vllm_api_base.replace("/v1", "")
            health_url = f"{base_url}/health"
            
            # å°è¯•è¯·æ±‚ /health æˆ– /v1/models
            try:
                requests.get(health_url, timeout=2)
                return True
            except:
                # å›žé€€å°è¯• models æŽ¥å£
                models_url = f"{self.vllm_api_base}/models"
                resp = requests.get(models_url, timeout=2)
                return resp.status_code == 200
        except Exception as e:
            logger.warning(f"âš ï¸ VLLM service check failed: {e}")
            return False

    def _auto_sleep_monitor(self):
        """
        [åŽå°çº¿ç¨‹] ç›‘æŽ§ç©ºé—²çŠ¶æ€
        """
        while True:
            time.sleep(10)
            try:
                if self.is_processing or self.is_offloaded:
                    continue
                
                if time.time() - self.last_active_time > self.idle_timeout:
                    logger.info(f"ðŸ’¤ PaddleOCR-VLLM idle for {self.idle_timeout}s. Unloading pipeline...")
                    self.cleanup()
                    self.is_offloaded = True
            except Exception as e:
                logger.error(f"Monitor error: {e}")

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ç®¡é“"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            # 1. é¢„æ£€æŸ¥ VLLM æœåŠ¡
            if not self._check_vllm_health():
                logger.error(f"âŒ VLLM service unreachable at {self.vllm_api_base}")
                logger.error("   Please ensure the 'vllm-paddleocr' container is running.")

            logger.info("=" * 60)
            logger.info("ðŸ“¥ Loading PaddleOCR-VL-VLLM Pipeline (Auto-Wakeup)...")
            logger.info("=" * 60)

            try:
                import paddle
                from paddleocr import PaddleOCRVL

                if paddle.is_compiled_with_cuda():
                    paddle.set_device(f"gpu:{self.gpu_id}")

                # è®¾ç½® PaddleX ä¸»ç›®å½•
                pdx_home = os.environ.get("PADDLEX_HOME", "/root/.paddlex")
                
                # åˆå§‹åŒ–ç®¡é“
                self._pipeline = PaddleOCRVL(
                    vl_rec_backend="vllm-server",
                    vl_rec_server_url=self.vllm_api_base,
                )
                
                logger.info("âœ… Pipeline loaded successfully (Serial Mode Active)")
                return self._pipeline

            except Exception as e:
                logger.error(f"âŒ Pipeline load failed: {e}")
                logger.error(traceback.format_exc())
                raise

    def cleanup(self):
        """æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†"""
        with self._lock:
            self._pipeline = None # é‡Šæ”¾å¼•ç”¨
            try:
                import paddle
                if paddle.device.is_compiled_with_cuda():
                    paddle.device.cuda.empty_cache()
                gc.collect()
                logger.info("âœ… VRAM released.")
            except:
                pass

    def parse(self, file_path: str, output_path: str, **kwargs) -> Dict[str, Any]:
        """
        è§£æžæ–‡æ¡£å…¥å£ (å¢žå¼ºç‰ˆï¼šè‡ªåŠ¨å”¤é†’ + çŠ¶æ€ç»´æŠ¤ + é˜²å´©æºƒé™çº§ + åŒå‘å®šä½æå–)
        """
        # =========================================================
        # 1. çŠ¶æ€æ›´æ–°ä¸Žè‡ªåŠ¨å”¤é†’
        # =========================================================
        self.is_processing = True
        self.last_active_time = time.time()
        
        if self.is_offloaded:
            logger.info("ðŸš€ New task received. Waking up PaddleOCR-VLLM engine...")
            self.is_offloaded = False
            # _load_pipeline() ä¼šè‡ªåŠ¨é‡å»º

        try:
            file_path = Path(file_path)
            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)

            logger.info(f"ðŸ¤– Processing: {file_path.name}")
            
            pipeline = self._load_pipeline()

            # å‚æ•°æ˜ å°„
            param_mapping = {
                "useDocOrientationClassify": "use_doc_orientation_classify",
                "useDocUnwarping": "use_doc_unwarping",
                "useLayoutDetection": "use_layout_parsing",
                "useChartRecognition": "use_chart_recognition",
                "useSealRecognition": "use_seal_recognition",
                "useOcrForImageBlock": "use_ocr_for_image_block",
                "layoutNms": "layout_nms",
                "markdownIgnoreLabels": "markdown_ignore_labels",
                "mergeTables": "merge_tables",
                "relevelTitles": "relevel_titles",
                "restructurePages": "restructure_pages",
                "minPixels": "min_pixels",
                "maxPixels": "max_pixels",
            }

            predict_params = {"input": str(file_path)}
            
            # é»˜è®¤å‚æ•°
            defaults = {
                "use_layout_parsing": True,
                "use_doc_orientation_classify": False,  # é»˜è®¤å…³é—­ä»¥é˜²å´©æºƒ
                "use_doc_unwarping": False,
                "use_seal_recognition": True
            }
            
            # å¡«å……å‚æ•°
            for k, v in kwargs.items():
                if k in param_mapping:
                    predict_params[param_mapping[k]] = v
            
            for k, v in defaults.items():
                if k not in predict_params:
                    predict_params[k] = v

            # =========================================================
            # ðŸš¨ 2. æ‰§è¡ŒæŽ¨ç†ï¼Œå¢žåŠ é˜²å´©æºƒé‡è¯•æœºåˆ¶ (Fallback)
            # =========================================================
            try:
                # å¼ºåˆ¶è½¬ä¸º listï¼Œç«‹å³è§¦å‘åº•å±‚å¯èƒ½å­˜åœ¨çš„ NoneType é”™è¯¯
                output_generator = list(pipeline.predict(**predict_params))
            except Exception as e:
                logger.warning(f"âš ï¸ Standard prediction failed (likely VLM empty output/400 Error): {e}")
                logger.info("ðŸ”„ Retrying with fallback parameters (disabling complex layout parsing) in 2 seconds...")
                
                # è®©è¿œç«¯ vLLM æœåŠ¡å™¨å–˜æ¯æ¢å¤
                time.sleep(2)
                
                # é™çº§ç­–ç•¥ï¼šå…³é—­å®¹æ˜“å¼•èµ·æ¨¡åž‹å¹»è§‰æˆ–ç©ºè¾“å‡ºçš„é«˜çº§ç‰ˆé¢åˆ†æž
                predict_params["use_layout_parsing"] = False
                predict_params["use_chart_recognition"] = False
                predict_params["use_seal_recognition"] = False
                
                try:
                    output_generator = list(pipeline.predict(**predict_params))
                except Exception as fallback_e:
                    logger.error(f"âŒ Fallback prediction also failed: {fallback_e}")
                    raise RuntimeError(f"VLM Worker crashed on this document. Internal Error: {fallback_e}")

            markdown_pages = []
            markdown_list_obj = []
            json_list = []
            full_content_list = [] # ç”¨äºŽå‰ç«¯åŒå‘å®šä½çš„é«˜äº®æ¡†æ•°æ®
            page_count = 0

            for res in output_generator:
                page_count += 1
                
                # ðŸ›¡ï¸ é˜²å¾¡æ€§æ£€æŸ¥
                if res is None:
                    logger.error(f"âŒ Page {page_count} returned None result")
                    continue

                page_output_dir = output_path / f"page_{page_count}"
                page_output_dir.mkdir(parents=True, exist_ok=True)

                # ä¿å­˜ä¸­é—´å›¾å’ŒJSON
                try:
                    if hasattr(res, "save_to_img"): res.save_to_img(str(page_output_dir))
                    if hasattr(res, "save_to_json"): res.save_to_json(str(page_output_dir))
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to save intermediate files for page {page_count}: {e}")

                # =========================================================
                # 3. [æ ¸å¿ƒåŠŸèƒ½] æå–ç»“æž„åŒ–æ•°æ® (BBox) ç”¨äºŽåŒå‘å®šä½
                # =========================================================
                if hasattr(res, "json") and res.json:
                    json_list.append(res.json)
                    if isinstance(res.json, dict):
                        # å…¼å®¹ PaddleX çš„ä¸åŒè¿”å›žæ ¼å¼ ('res' æˆ– 'parsing_res_list')
                        blocks = res.json.get('res') or res.json.get('parsing_res_list') or []
                        
                        # ä¸¥æ ¼ç±»åž‹æ£€æŸ¥ï¼Œé˜²æ­¢å´©æºƒ
                        if not isinstance(blocks, list):
                            if isinstance(blocks, dict) and ('bbox' in blocks or 'layout_bbox' in blocks or 'block_bbox' in blocks):
                                blocks = [blocks]
                            else:
                                blocks = []

                        for block in blocks:
                            if not isinstance(block, dict): continue

                            clean_block = {
                                "id": len(full_content_list) + 1,
                                "page_idx": page_count - 1,
                                "type": block.get('type') or block.get('block_label') or 'text',
                                "text": block.get('text') or block.get('block_content') or '',
                                "bbox": block.get('layout_bbox') or block.get('block_bbox') or block.get('bbox') or [],
                                "score": block.get('score', 0),
                                "order": block.get('block_order') # æå– block_order ä¿è¯å‰ç«¯æŽ’åºæ­£ç¡®
                            }
                            if clean_block['bbox']:
                                full_content_list.append(clean_block)

                # =========================================================
                # 4. æå– Markdown (å¸¦æœ¬åœ°æ–‡ä»¶è¯»å–å…œåº•æœºåˆ¶)
                # =========================================================
                if hasattr(res, "markdown") and res.markdown:
                    markdown_list_obj.append(res.markdown)

                page_md = ""
                try:
                    if hasattr(res, "markdown") and res.markdown:
                        if isinstance(res.markdown, dict):
                            page_md = res.markdown.get('markdown_texts', '') or res.markdown.get('text', '')
                        elif hasattr(res.markdown, 'markdown_texts'):
                            page_md = res.markdown.markdown_texts
                        else:
                            page_md = str(res.markdown)
                    elif hasattr(res, "str") and res.str:
                        page_md = str(res.str)
                except Exception as e:
                    logger.warning(f"âš ï¸ Error extracting markdown from page {page_count}: {e}")

                if page_md:
                    markdown_pages.append(page_md)
                else:
                    # å…œåº•ï¼šå°è¯•è¯»å–ç”±äºŽ save_to_markdown ç”Ÿæˆçš„æ–‡ä»¶
                    try:
                         if hasattr(res, "save_to_markdown"):
                            res.save_to_markdown(str(page_output_dir))
                            saved = list(page_output_dir.glob("*.md"))
                            if saved:
                                markdown_pages.append(saved[0].read_text(encoding="utf-8"))
                    except:
                        pass
                
                logger.info(f"âœ… Processed Page {page_count}")

            # åˆå¹¶ç»“æžœ
            logger.info(f"ðŸŽ‰ Processing complete. Total pages: {page_count}")

            markdown_text = ""
            # å°è¯•ä½¿ç”¨å®˜æ–¹åˆå¹¶ç®—æ³•
            if hasattr(pipeline, "concatenate_markdown_pages") and markdown_list_obj:
                try:
                    markdown_text = pipeline.concatenate_markdown_pages(markdown_list_obj)
                except Exception as e:
                    logger.warning(f"Official concat failed: {e}, falling back to simple join")
                    markdown_text = "\n\n---\n\n".join(markdown_pages)
            else:
                markdown_text = "\n\n---\n\n".join(markdown_pages)

            # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
            markdown_file = output_path / "result.md"
            markdown_file.write_text(markdown_text, encoding="utf-8")
            
            # æž„é€  result.json (åŒ…å«ç»™å‰ç«¯å®šä½ç”¨çš„ full_content_list)
            json_file = output_path / "result.json"
            final_json_data = full_content_list if full_content_list else {
                "pages": json_list, 
                "total_pages": page_count
            }
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(final_json_data, f, ensure_ascii=False, indent=2)

            return {
                "success": True,
                "output_path": str(output_path),
                "markdown": markdown_text,
                "markdown_file": str(markdown_file),
                "json_file": str(json_file),
                "json_content": final_json_data
            }

        except Exception as e:
            logger.error(f"âŒ OCR Pipeline Critical Error: {e}")
            logger.error(traceback.format_exc())
            raise
        finally:
            # =========================================================
            # [æ€§èƒ½ä¼˜åŒ–]
            # ç§»é™¤å¼ºåˆ¶ cleanup()ï¼Œè®©æ¨¡åž‹ä¿æŒåŠ è½½çŠ¶æ€
            # æ›´æ–°æ—¶é—´æˆ³ï¼Œè®©åŽå°çº¿ç¨‹åœ¨ç©ºé—²5åˆ†é’ŸåŽå¤„ç†é‡Šæ”¾
            # =========================================================
            self.is_processing = False
            self.last_active_time = time.time()
            logger.info("ðŸ Task finished. Pipeline remains loaded for fast reuse.")

# å…¨å±€å•ä¾‹
_engine = None

def get_engine(vllm_api_base: str = None, model_name: str = "PaddleOCR-VL-1.5-0.9B") -> PaddleOCRVLVLLMEngine:
    global _engine
    if _engine is None:
        _engine = PaddleOCRVLVLLMEngine(vllm_api_base=vllm_api_base, model_name=model_name)
    return _engine
