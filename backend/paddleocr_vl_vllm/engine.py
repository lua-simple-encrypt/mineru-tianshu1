"""
PaddleOCR-VL-VLLM è§£æžå¼•æ“Ž (Optimized + Bidirectional Layout Support)
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡åŸºç¡€ç‰ˆé¢è¯†åˆ«æ¨¡åž‹, OCRéƒ¨åˆ†è°ƒç”¨é…ç½®çš„API

åŠŸèƒ½å¢žå¼º:
1. [ä¿®å¤] ä¿®å¤ res['res'] ç±»åž‹ä¸ä¸€è‡´å¯¼è‡´çš„ AttributeError å´©æºƒ
2. [åŒå‘å®šä½] è¾“å‡ºåŒ…å« bbox çš„ç»“æž„åŒ–æ•°æ® (json_content)
3. [èµ„æºç®¡ç†] æ™ºèƒ½æ˜¾å­˜ä¼‘çœ  (Auto-Sleep) å’Œè‡ªåŠ¨å”¤é†’ (Auto-Wakeup)
4. [ç¨³å®šæ€§] å¼ºåˆ¶å•çº¿ç¨‹æŽ¨ç†ä»¥è§£å†³ vLLM Tokenizer ç«žæ€å´©æºƒ
5. [é˜²å´©æºƒ] å¢žåŠ  VLM NoneType å¼‚å¸¸æ•èŽ·ä¸Žé™çº§é‡è¯•æœºåˆ¶ (Fallback)
"""

import os
import gc
import json
import time
import requests
import traceback
import threading
from pathlib import Path
from typing import Optional, Dict, Any, List
from threading import Lock
from loguru import logger

# ==============================================================================
# ðŸš¨ å…¨å±€çŽ¯å¢ƒé…ç½® (å¿…é¡»åœ¨å¯¼å…¥ paddle/paddlex ä¹‹å‰è®¾ç½®)
# ==============================================================================
# 1. é™åˆ¶ PaddleX å†…éƒ¨æŽ¨ç†å¹¶å‘æ•°ä¸º 1ï¼Œé˜²æ­¢é«˜å¹¶å‘è¯·æ±‚å†²åž® vLLM çš„ Tokenizer
os.environ["PADDLEX_INFERENCE_PARALLEL_WORKER_NUM"] = "1"
# 2. ç¦ç”¨æ¨¡åž‹æºæ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦ (å†…ç½‘çŽ¯å¢ƒå¿…å¤‡)
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True"
# ==============================================================================

class PaddleOCRVLVLLMEngine:
    """
    PaddleOCR-VL-VLLM è§£æžå¼•æ“Žï¼ˆæ”¯æŒåŒå‘å®šä½æ•°æ®è¾“å‡ºï¼‰
    """

    _instance: Optional["PaddleOCRVLVLLMEngine"] = None
    _lock = Lock()
    _pipeline = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, 
                 device: str = "cuda:0", 
                 vllm_api_base: str = None, 
                 model_name: str = "PaddleOCR-VL-1.5-0.9B"):
        """
        åˆå§‹åŒ–å¼•æ“Ž
        """
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå…¶æ¬¡çŽ¯å¢ƒå˜é‡ï¼Œæœ€åŽé»˜è®¤ Docker å†…éƒ¨åœ°å€
            self.vllm_api_base = vllm_api_base or os.getenv("VLLM_API_BASE", "http://vllm-paddleocr:30023/v1")
            self.model_name = model_name

            # æå– GPU ID
            if "cuda:" in device:
                try:
                    self.gpu_id = int(device.split(":")[-1])
                except ValueError:
                    self.gpu_id = 0
            else:
                self.gpu_id = 0

            self._check_gpu_availability()
            
            # =========================================================
            # [èµ„æºç®¡ç†] æ™ºèƒ½æ˜¾å­˜ç®¡ç†çŠ¶æ€å˜é‡
            # =========================================================
            self.last_active_time = time.time()
            self.is_processing = False
            self.is_offloaded = True 
            self.idle_timeout = 300  # 5åˆ†é’Ÿæ— æ“ä½œè‡ªåŠ¨å¸è½½

            # å¯åŠ¨ç›‘æŽ§çº¿ç¨‹
            self._monitor_thread = threading.Thread(target=self._auto_sleep_monitor, daemon=True)
            self._monitor_thread.start()

            self._initialized = True

            logger.info("ðŸ”§ PaddleOCR-VL-VLLM Engine Initialized")
            logger.info(f"   Device: {self.device} (Physical GPU: {self.gpu_id})")
            logger.info(f"   VLLM API: {self.vllm_api_base}")
            logger.info(f"   Auto-Sleep: Enabled ({self.idle_timeout}s)")

    def _check_gpu_availability(self):
        try:
            import paddle
            if not paddle.is_compiled_with_cuda():
                logger.error("âŒ PaddlePaddle is running on CPU! This model requires GPU.")
                return
            gpu_name = paddle.device.cuda.get_device_name(self.gpu_id)
            logger.info(f"âœ… GPU Detected: {gpu_name}")
        except Exception:
            logger.warning("âš ï¸ Could not verify GPU status via PaddlePaddle")

    def _check_vllm_health(self) -> bool:
        """æ£€æŸ¥ VLLM æœåŠ¡æ˜¯å¦å¥åº·"""
        try:
            base_url = self.vllm_api_base.replace("/v1", "")
            health_url = f"{base_url}/health"
            try:
                requests.get(health_url, timeout=2)
                return True
            except:
                models_url = f"{self.vllm_api_base}/models"
                resp = requests.get(models_url, timeout=2)
                return resp.status_code == 200
        except Exception as e:
            logger.warning(f"âš ï¸ VLLM service check failed: {e}")
            return False

    def _auto_sleep_monitor(self):
        """[åŽå°çº¿ç¨‹] ç›‘æŽ§ç©ºé—²çŠ¶æ€"""
        while True:
            time.sleep(10)
            try:
                if self.is_processing or self.is_offloaded:
                    continue
                
                if time.time() - self.last_active_time > self.idle_timeout:
                    logger.info(f"ðŸ’¤ PaddleOCR-VLLM idle for {self.idle_timeout}s. Unloading pipeline...")
                    self.cleanup()
                    self.is_offloaded = True
            except Exception as e:
                logger.error(f"Monitor error: {e}")

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ç®¡é“"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            if not self._check_vllm_health():
                logger.error(f"âŒ VLLM service unreachable at {self.vllm_api_base}")

            logger.info("ðŸ“¥ Loading PaddleOCR-VL-VLLM Pipeline (Auto-Wakeup)...")
            try:
                import paddle
                from paddleocr import PaddleOCRVL
                if paddle.is_compiled_with_cuda():
                    paddle.set_device(f"gpu:{self.gpu_id}")

                self._pipeline = PaddleOCRVL(
                    vl_rec_backend="vllm-server",
                    vl_rec_server_url=self.vllm_api_base,
                )
                logger.info("âœ… Pipeline loaded successfully")
                return self._pipeline
            except Exception as e:
                logger.error(f"âŒ Pipeline load failed: {e}")
                raise

    def cleanup(self):
        """é‡Šæ”¾æ˜¾å­˜"""
        with self._lock:
            self._pipeline = None 
            try:
                import paddle
                if paddle.device.is_compiled_with_cuda():
                    paddle.device.cuda.empty_cache()
                gc.collect()
                logger.info("âœ… GPU Memory released.")
            except:
                pass

    def parse(self, file_path: str, output_path: str, **kwargs) -> Dict[str, Any]:
        """
        è§£æžæ–‡æ¡£å…¥å£å¹¶æå–å¸ƒå±€æ•°æ® (Phase 5 æ”¯æŒ)
        """
        self.is_processing = True
        self.last_active_time = time.time()
        
        if self.is_offloaded:
            logger.info("ðŸš€ New task received. Waking up engine...")
            self.is_offloaded = False

        try:
            file_path = Path(file_path)
            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)

            logger.info(f"ðŸ¤– Processing: {file_path.name}")

            pipeline = self._load_pipeline()

            # =========================================================
            # å‚æ•°ç™½åå•è¿‡æ»¤ (ä¿®å¤ NoneType error)
            # =========================================================
            allowed_params = {
                "use_doc_orientation_classify",
                "use_doc_unwarping",
                "use_layout_parsing",
                "use_chart_recognition",
                "use_seal_recognition",
                "use_ocr_for_image_block",
            }
            
            param_mapping = {
                "useDocOrientationClassify": "use_doc_orientation_classify",
                "useDocUnwarping": "use_doc_unwarping",
                "useLayoutDetection": "use_layout_parsing",
                "useChartRecognition": "use_chart_recognition",
                "useSealRecognition": "use_seal_recognition",
                "useOcrForImageBlock": "use_ocr_for_image_block",
            }

            predict_params = {"input": str(file_path)}
            
            for k, v in kwargs.items():
                target_key = param_mapping.get(k, k)
                if target_key in allowed_params:
                    predict_params[target_key] = v
                else:
                    logger.debug(f"â„¹ï¸ Filtered param for VLLM mode: {k}={v}")
            
            # å¼ºåˆ¶é»˜è®¤å€¼
            predict_params["use_layout_parsing"] = True
            predict_params["use_doc_orientation_classify"] = False
            predict_params["use_doc_unwarping"] = False

            # =========================================================
            # ðŸš¨ [å…³é”®ä¿®å¤] æ‰§è¡ŒæŽ¨ç†ï¼Œå¢žåŠ é˜²å´©æºƒé‡è¯•æœºåˆ¶ (Fallback)
            # =========================================================
            try:
                # å¼ºåˆ¶è½¬ä¸º listï¼Œç«‹å³è§¦å‘åº•å±‚å¯èƒ½å­˜åœ¨çš„ NoneType é”™è¯¯
                output_generator = list(pipeline.predict(**predict_params))
            except Exception as e:
                logger.warning(f"âš ï¸ Standard prediction failed (likely VLM empty output): {e}")
                logger.info("ðŸ”„ Retrying with fallback parameters (disabling complex layout parsing)...")
                
                # é™çº§ç­–ç•¥ï¼šå…³é—­å®¹æ˜“å¼•èµ·æ¨¡åž‹å¹»è§‰æˆ–ç©ºè¾“å‡ºçš„é«˜çº§ç‰ˆé¢åˆ†æž
                predict_params["use_layout_parsing"] = False
                predict_params["use_chart_recognition"] = False
                predict_params["use_seal_recognition"] = False
                
                try:
                    output_generator = list(pipeline.predict(**predict_params))
                except Exception as fallback_e:
                    logger.error(f"âŒ Fallback prediction also failed: {fallback_e}")
                    raise RuntimeError(f"VLM Worker crashed on this document. Internal Error: {fallback_e}")

            markdown_pages = []
            markdown_list_obj = []
            json_list = []
            full_content_list = [] # [æ–°å¢ž] ç”¨äºŽåŒå‘å®šä½
            page_count = 0

            for res in output_generator:
                page_count += 1
                if res is None: continue

                page_dir = output_path / f"page_{page_count}"
                page_dir.mkdir(parents=True, exist_ok=True)

                # 1. ä¿å­˜å›¾ç‰‡å’ŒåŽŸå§‹ JSON
                try:
                    if hasattr(res, "save_to_img"): res.save_to_img(str(page_dir))
                    if hasattr(res, "save_to_json"): res.save_to_json(str(page_dir))
                except Exception as e:
                    logger.warning(f"Page {page_count} save error: {e}")

                # 2. [æ ¸å¿ƒä¿®å¤] æå–ç»“æž„åŒ–æ•°æ® (BBox) ç”¨äºŽåŒå‘å®šä½
                if hasattr(res, "json") and res.json:
                    json_list.append(res.json)
                    if isinstance(res.json, dict) and 'res' in res.json:
                        blocks = res.json['res']
                        
                        # [FIX] ä¸¥æ ¼ç±»åž‹æ£€æŸ¥ï¼Œé˜²æ­¢å´©æºƒ
                        if not isinstance(blocks, list):
                            # å¦‚æžœæ˜¯å•ä¸ªå¯¹è±¡ä¸”æœ‰bboxï¼ŒåŒ…è£…æˆåˆ—è¡¨
                            if isinstance(blocks, dict) and ('bbox' in blocks or 'layout_bbox' in blocks):
                                blocks = [blocks]
                            else:
                                # å¯èƒ½æ˜¯å…ƒæ•°æ®ï¼ˆå¦‚ 'input_path'ï¼‰ï¼Œè·³è¿‡
                                blocks = []

                        for block in blocks:
                            if not isinstance(block, dict): continue

                            clean_block = {
                                "id": len(full_content_list) + 1,
                                "page_idx": page_count - 1,
                                "type": block.get('type', 'text'),
                                "text": block.get('text', ''),
                                "bbox": block.get('layout_bbox') or block.get('bbox') or [],
                                "score": block.get('score', 0)
                            }
                            if clean_block['bbox']:
                                full_content_list.append(clean_block)

                # 3. æå– Markdown
                page_md = ""
                if hasattr(res, "markdown") and res.markdown:
                    markdown_list_obj.append(res.markdown)
                    if isinstance(res.markdown, dict):
                        page_md = res.markdown.get('markdown_texts', '')
                    elif hasattr(res.markdown, 'markdown_texts'):
                        page_md = res.markdown.markdown_texts
                    else:
                        page_md = str(res.markdown)
                
                if page_md:
                    markdown_pages.append(page_md)
                
                logger.info(f"âœ… Processed Page {page_count}")

            # åˆå¹¶ Markdown
            if hasattr(pipeline, "concatenate_markdown_pages") and markdown_list_obj:
                try:
                    markdown_text = pipeline.concatenate_markdown_pages(markdown_list_obj)
                except:
                    markdown_text = "\n\n---\n\n".join(markdown_pages)
            else:
                markdown_text = "\n\n---\n\n".join(markdown_pages)

            # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
            (output_path / "result.md").write_text(markdown_text, encoding="utf-8")
            
            # [å…³é”®] æž„é€  result.json
            final_json_data = full_content_list if full_content_list else {
                "total_pages": page_count,
                "pages": json_list
            }
            
            json_file = output_path / "result.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(final_json_data, f, ensure_ascii=False, indent=2)

            return {
                "success": True,
                "output_path": str(output_path),
                "markdown": markdown_text,
                "markdown_file": str(output_path / "result.md"),
                "json_file": str(json_file),
                "json_content": full_content_list
            }

        except Exception as e:
            logger.error(f"âŒ OCR Pipeline Error: {e}")
            logger.error(traceback.format_exc())
            raise
        finally:
            self.is_processing = False
            self.last_active_time = time.time()
            logger.info("ðŸ Task finished. Model stays loaded (5min auto-sleep).")

# å…¨å±€å•ä¾‹
_engine = None

def get_engine(vllm_api_base: str = None, model_name: str = "PaddleOCR-VL-1.5-0.9B") -> PaddleOCRVLVLLMEngine:
    global _engine
    if _engine is None:
        _engine = PaddleOCRVLVLLMEngine(vllm_api_base=vllm_api_base, model_name=model_name)
    return _engine
