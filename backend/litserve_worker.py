"""
MinerU Tianshu - LitServe Worker
å¤©æž¢ LitServe Worker

ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å° - GPU Worker
æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†
ä½¿ç”¨ LitServe å®žçŽ° GPU èµ„æºçš„è‡ªåŠ¨è´Ÿè½½å‡è¡¡
Worker ä¸»åŠ¨å¾ªçŽ¯æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†

ä¼˜åŒ–æ—¥å¿— (2026-02-16):
1. [ä¿®å¤] å¼ºåˆ¶å›žå†™æº PDF åˆ° output ç›®å½•ï¼Œè§£å†³å‰ç«¯æ— æ³•é¢„è§ˆæºæ–‡ä»¶çš„é—®é¢˜
2. [ä¿®å¤] PaddleOCR/MinerU è¿”å›žç»“æžœä¸­è¡¥å…¨ json_content å’Œ pdf_path ä»¥æ”¯æŒåŒå‘å®šä½
3. [æ€§èƒ½] ç§»é™¤å•æ¬¡ä»»åŠ¡åŽçš„å¼ºåˆ¶æ˜¾å­˜æ¸…ç† (clean_memory)ï¼Œä¾èµ–å¼•æ“Žçš„æ™ºèƒ½ä¼‘çœ æœºåˆ¶
4. [ç¨³å®š] å¢žå¼º VLLM å®¹å™¨äº’æ–¥åˆ‡æ¢çš„å¥å£®æ€§
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
import shutil
import socket
import multiprocessing
import requests
import warnings
import subprocess
import tempfile
from pathlib import Path
from typing import Optional
from contextlib import asynccontextmanager

# ==============================================================================
# 1. LitServe MCP Patch (Disable Internal MCP)
# ==============================================================================
try:
    import litserve.mcp as ls_mcp

    if not hasattr(ls_mcp, "MCPServer"):
        class DummyMCPServer:
            def __init__(self, *args, **kwargs): pass
        ls_mcp.MCPServer = DummyMCPServer
        if "litserve.mcp" in sys.modules:
            sys.modules["litserve.mcp"].MCPServer = DummyMCPServer

    if not hasattr(ls_mcp, "StreamableHTTPSessionManager"):
        class DummyStreamableHTTPSessionManager:
            def __init__(self, *args, **kwargs): pass
        ls_mcp.StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager
        if "litserve.mcp" in sys.modules:
            sys.modules["litserve.mcp"].StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager

    class DummyMCPConnector:
        """å®Œå…¨ç¦ç”¨ LitServe å†…ç½® MCP çš„ Dummy å®žçŽ°"""
        def __init__(self, *args, **kwargs):
            self.mcp_server = None
            self.session_manager = None
            self.request_handler = None

        @asynccontextmanager
        async def lifespan(self, app):
            yield

        def connect_mcp_server(self, *args, **kwargs):
            pass

    ls_mcp._LitMCPServerConnector = DummyMCPConnector
    if "litserve.mcp" in sys.modules:
        sys.modules["litserve.mcp"]._LitMCPServerConnector = DummyMCPConnector

except Exception as e:
    warnings.warn(f"Failed to patch litserve.mcp (MCP will be disabled): {e}")

import litserve as ls
from litserve.connector import check_cuda_with_nvidia_smi
from loguru import logger

# Add parent dir to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Local imports
from task_db import TaskDB
from output_normalizer import normalize_output
from utils import parse_list_arg
import importlib.util

# ==============================================================================
# 2. Dependency Checks & Global Configurations
# ==============================================================================
def check_dependency(module_name: str, display_name: str) -> bool:
    """Helper to check if a module is installed and log the result."""
    available = importlib.util.find_spec(module_name) is not None
    icon = "âœ…" if available else "â„¹ï¸ "
    msg = "available" if available else "not available (optional)"
    logger.info(f"{icon} {display_name} {msg}")
    return available

# Check optional dependencies
MARKITDOWN_AVAILABLE = False
try:
    from markitdown import MarkItDown
    MARKITDOWN_AVAILABLE = True
    logger.info("âœ… MarkItDown available")
except ImportError:
    logger.info("â„¹ï¸  MarkItDown not available (optional)")

PADDLEOCR_VL_AVAILABLE = check_dependency("paddleocr_vl", "PaddleOCR-VL")
PADDLEOCR_VL_VLLM_AVAILABLE = check_dependency("paddleocr_vl_vllm", "PaddleOCR-VL-VLLM")
MINERU_PIPELINE_AVAILABLE = check_dependency("mineru_pipeline", "MinerU Pipeline")
SENSEVOICE_AVAILABLE = check_dependency("audio_engines", "SenseVoice")
VIDEO_ENGINE_AVAILABLE = check_dependency("video_engines", "Video Engine")
WATERMARK_REMOVAL_AVAILABLE = check_dependency("remove_watermark", "Watermark Removal")

FORMAT_ENGINES_AVAILABLE = False
try:
    from format_engines import FormatEngineRegistry, FASTAEngine, GenBankEngine
    FormatEngineRegistry.register(FASTAEngine())
    FormatEngineRegistry.register(GenBankEngine())
    FORMAT_ENGINES_AVAILABLE = True
    logger.info(f"âœ… Format Engines available: {', '.join(FormatEngineRegistry.get_supported_extensions())}")
except ImportError as e:
    logger.info(f"â„¹ï¸  Format Engines not available: {e}")


# ==============================================================================
# 3. VLLM Container Controller
# ==============================================================================
class VLLMController:
    """ç®¡ç† vLLM Docker å®¹å™¨çš„äº’æ–¥å¯åŠ¨"""
    
    def __init__(self):
        pass

    def _get_client(self):
        """æŒ‰éœ€èŽ·å– Docker å®¢æˆ·ç«¯"""
        try:
            import docker
            return docker.from_env()
        except Exception as e:
            logger.warning(f"âš ï¸  Docker client init failed: {e}")
            return None

    def ensure_service(self, target_container: str, conflict_container: str):
        """
        ç¡®ä¿ç›®æ ‡å®¹å™¨è¿è¡Œï¼Œå¹¶å…³é—­å†²çªå®¹å™¨ (ä¸¥æ ¼äº’æ–¥é€»è¾‘)
        """
        client = self._get_client()
        if not client:
            return
        
        try:
            # 1. æ£€æŸ¥å¹¶å…³é—­å†²çªå®¹å™¨
            try:
                conflict = client.containers.get(conflict_container)
                if conflict.status == 'running':
                    logger.info(f"ðŸ›‘ Stopping conflicting service {conflict_container} to free VRAM...")
                    conflict.stop()
                    time.sleep(2) # ç­‰å¾…é‡Šæ”¾
                    logger.info(f"âœ… Service {conflict_container} stopped.")
            except Exception:
                pass

            # 2. æ£€æŸ¥å¹¶å¯åŠ¨ç›®æ ‡å®¹å™¨
            try:
                target = client.containers.get(target_container)
                if target.status == 'running':
                    logger.info(f"âœ… Target service {target_container} is already running.")
                    return
                
                logger.info(f"ðŸš€ Starting service {target_container} (Manual/Cold Start)...")
                target.start()
                logger.info(f"âš ï¸ Service start triggered. Assuming {target_container} will be ready shortly.")
                
            except Exception as e:
                logger.error(f"âŒ Failed to start target container {target_container}: {e}")
                raise e
        finally:
            try:
                client.close()
            except:
                pass


# ==============================================================================
# 4. MinerU Worker API
# ==============================================================================
class MinerUWorkerAPI(ls.LitAPI):
    def __init__(
        self,
        paddleocr_vl_vllm_api_list=None,
        mineru_vllm_api_list=None,
        output_dir=None,
        poll_interval=0.5,
        enable_worker_loop=True,
        paddleocr_vl_vllm_engine_enabled=False,
    ):
        super().__init__()
        
        # è·¯å¾„é…ç½®
        project_root = Path(__file__).parent.parent
        default_output = project_root / "data" / "output"
        self.output_dir = output_dir or os.getenv("OUTPUT_PATH", str(default_output))
        
        # è¿è¡Œé…ç½®
        self.poll_interval = poll_interval
        self.enable_worker_loop = enable_worker_loop
        
        # API é…ç½®
        self.paddleocr_vl_vllm_engine_enabled = paddleocr_vl_vllm_engine_enabled
        self.paddleocr_vl_vllm_api_list = paddleocr_vl_vllm_api_list or []
        self.mineru_vllm_api_list = mineru_vllm_api_list or []
        
        # è¿›ç¨‹é—´å…±äº«è®¡æ•°å™¨
        ctx = multiprocessing.get_context("spawn")
        self._global_worker_counter = ctx.Value("i", 0)

        # åˆå§‹åŒ–æŽ§åˆ¶å™¨
        self.vllm_controller = VLLMController()

    def setup(self, device):
        """åˆå§‹åŒ– Worker (æ¯ä¸ª GPU è¿›ç¨‹è°ƒç”¨ä¸€æ¬¡)"""
        with self._global_worker_counter.get_lock():
            my_global_index = self._global_worker_counter.value
            self._global_worker_counter.value += 1
        
        logger.info(f"ðŸ”¢ [Init] I am Global Worker #{my_global_index} (on {device})")
        
        # API åˆ†é…
        self.paddleocr_vl_vllm_api = None
        if self.paddleocr_vl_vllm_engine_enabled and self.paddleocr_vl_vllm_api_list:
            assigned_api = self.paddleocr_vl_vllm_api_list[my_global_index % len(self.paddleocr_vl_vllm_api_list)]
            self.paddleocr_vl_vllm_api = assigned_api
            logger.info(f"ðŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: {assigned_api}")

        self.mineru_vllm_api = None
        if self.mineru_vllm_api_list:
            assigned_mineru_api = self.mineru_vllm_api_list[my_global_index % len(self.mineru_vllm_api_list)]
            self.mineru_vllm_api = assigned_mineru_api
            logger.info(f"ðŸ”§ Worker #{my_global_index} assigned MinerU VLLM API: {assigned_mineru_api}")

        # è®¾ç½® CUDA éš”ç¦»
        if "cuda:" in str(device):
            gpu_id = str(device).split(":")[-1]
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            os.environ["MINERU_DEVICE_MODE"] = "cuda:0"
            logger.info(f"ðŸŽ¯ [GPU Isolation] Set CUDA_VISIBLE_DEVICES={gpu_id}")

        # é…ç½®æ¨¡åž‹æº
        model_source = os.getenv("MODEL_DOWNLOAD_SOURCE", "auto").lower()
        if model_source in ["modelscope", "auto"]:
            try:
                importlib.util.find_spec("modelscope")
                os.environ["MINERU_MODEL_SOURCE"] = "modelscope"
            except ImportError:
                if model_source == "modelscope":
                    logger.warning("âš ï¸  ModelScope not available, falling back to HuggingFace")

        if model_source == "huggingface":
            hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
            os.environ.setdefault("HF_ENDPOINT", hf_endpoint)

        # è®¾å¤‡é…ç½®
        self.device = device
        if "cuda" in str(device):
            self.accelerator = "cuda"
            self.engine_device = "cuda:0"
        else:
            self.accelerator = "cpu"
            self.engine_device = "cpu"

        # MinerU VRAM è®¾ç½®
        from mineru.utils.model_utils import get_vram, clean_memory
        if os.getenv("MINERU_VIRTUAL_VRAM_SIZE", None) is None:
            if self.accelerator == "cuda":
                try:
                    vram = round(get_vram("cuda:0"))
                    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = str(vram)
                except Exception:
                    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "8"
            else:
                os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "1"

        # åˆå§‹åŒ–æ•°æ®åº“
        db_path_env = os.getenv("DATABASE_PATH")
        if db_path_env:
            db_path = Path(db_path_env).resolve()
        else:
            project_root = Path(__file__).parent.parent
            db_path = (project_root / "data" / "db" / "mineru_tianshu.db").resolve()
        
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self.task_db = TaskDB(str(db_path))

        # åˆå§‹åŒ–çŠ¶æ€
        self.running = True
        self.current_task_id = None
        hostname = socket.gethostname()
        pid = os.getpid()
        self.worker_id = f"tianshu-{hostname}-{device}-{pid}"

        # å¼•æ“Žå ä½ç¬¦
        self.markitdown = MarkItDown() if MARKITDOWN_AVAILABLE else None
        self.mineru_pipeline_engine = None
        self.paddleocr_vl_engine = None
        self.paddleocr_vl_vllm_engine = None
        self.sensevoice_engine = None
        self.video_engine = None
        self.watermark_handler = None

        logger.info(f"ðŸš€ Worker Setup Complete: {self.worker_id}")

        if WATERMARK_REMOVAL_AVAILABLE and self.accelerator == "cuda":
            try:
                from remove_watermark.pdf_watermark_handler import PDFWatermarkHandler
                self.watermark_handler = PDFWatermarkHandler(device="cuda:0", use_lama=True)
                logger.info(f"âœ… Watermark engine initialized")
            except Exception as e:
                logger.error(f"âŒ Failed to init watermark engine: {e}")

        if self.enable_worker_loop:
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()

    def _worker_loop(self):
        logger.info(f"ðŸ” {self.worker_id} started task polling loop")
        loop_count = 0
        last_stats_log = 0

        while self.running:
            try:
                loop_count += 1
                task = self.task_db.get_next_task(worker_id=self.worker_id)

                if task:
                    task_id = task["task_id"]
                    self.current_task_id = task_id
                    logger.info(f"ðŸ“¥ {self.worker_id} pulled task: {task_id}")

                    try:
                        self._process_task(task)
                        logger.info(f"âœ… {self.worker_id} completed task: {task_id}")
                    except Exception as e:
                        logger.error(f"âŒ {self.worker_id} failed task {task_id}: {e}")
                        logger.exception(e)
                    finally:
                        self.current_task_id = None
                else:
                    if loop_count - last_stats_log >= 20:
                        try:
                            stats = self.task_db.get_queue_stats()
                            if loop_count % 100 == 0:
                                logger.info(f"ðŸ’¤ {self.worker_id} idle. Queue stats: {stats}")
                        except: pass
                        last_stats_log = loop_count
                    time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"âŒ Worker loop error: {e}")
                time.sleep(self.poll_interval)

    def _process_task(self, task: dict):
        """å¤„ç†ä»»åŠ¡"""
        task_id = task["task_id"]
        file_path = task["file_path"]
        options = json.loads(task.get("options", "{}"))
        parent_task_id = task.get("parent_task_id")
        backend = task.get("backend", "auto")

        try:
            # 1. æ™ºèƒ½æœåŠ¡åˆ‡æ¢
            paddle_container = "tianshu-vllm-paddleocr"
            mineru_container = "tianshu-vllm-mineru"

            if backend == "paddleocr-vl-vllm" and self.paddleocr_vl_vllm_api:
                logger.info(f"ðŸ”„ Ensuring PaddleOCR-VL VLLM service is running")
                self.vllm_controller.ensure_service(target_container=paddle_container, conflict_container=mineru_container)
            elif backend in ["vlm-auto-engine", "hybrid-auto-engine"] and self.mineru_vllm_api:
                logger.info(f"ðŸ”„ Ensuring MinerU VLLM service is running")
                self.vllm_controller.ensure_service(target_container=mineru_container, conflict_container=paddle_container)

            file_ext = Path(file_path).suffix.lower()

            # 2. é¢„å¤„ç†
            if file_ext in [".docx", ".xlsx", ".pptx", ".doc", ".xls", ".ppt"] and options.get("convert_office_to_pdf", False):
                try:
                    pdf_path = self._convert_office_to_pdf(file_path)
                    file_path = pdf_path
                    file_ext = ".pdf"
                    logger.info(f"âœ… Office converted to PDF: {pdf_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Office conversion failed, falling back: {e}")

            # 3. PDF æ‹†åˆ†
            if file_ext == ".pdf" and not parent_task_id:
                if self._should_split_pdf(task_id, file_path, task, options):
                    return

            # 4. åŽ»æ°´å°
            if file_ext == ".pdf" and options.get("remove_watermark", False) and self.watermark_handler:
                try:
                    cleaned_path = self._preprocess_remove_watermark(file_path, options)
                    file_path = str(cleaned_path)
                except Exception as e:
                    logger.warning(f"âš ï¸ Watermark removal failed: {e}")

            # 5. å¼•æ“Žè·¯ç”±
            result = None

            if backend == "sensevoice":
                if not SENSEVOICE_AVAILABLE: raise ValueError("SenseVoice not available")
                result = self._process_audio(file_path, options)

            elif backend == "video":
                if not VIDEO_ENGINE_AVAILABLE: raise ValueError("Video engine not available")
                result = self._process_video(file_path, options)

            elif backend == "paddleocr-vl":
                if not PADDLEOCR_VL_AVAILABLE: raise ValueError("PaddleOCR-VL not available")
                result = self._process_with_paddleocr_vl(file_path, options)

            elif backend == "paddleocr-vl-vllm":
                if not PADDLEOCR_VL_VLLM_AVAILABLE: raise ValueError("PaddleOCR-VL-VLLM not available")
                result = self._process_with_paddleocr_vl_vllm(file_path, options)

            elif "pipeline" in backend or "vlm-" in backend or "hybrid-" in backend:
                if not MINERU_PIPELINE_AVAILABLE: raise ValueError("MinerU Pipeline not available")
                options["parse_mode"] = backend
                result = self._process_with_mineru(file_path, options)

            elif backend == "auto":
                if FORMAT_ENGINES_AVAILABLE and FormatEngineRegistry.is_supported(file_path):
                    result = self._process_with_format_engine(file_path, options)
                elif file_ext in [".wav", ".mp3", ".flac", ".m4a", ".ogg"] and SENSEVOICE_AVAILABLE:
                    result = self._process_audio(file_path, options)
                elif file_ext in [".mp4", ".avi", ".mkv", ".mov"] and VIDEO_ENGINE_AVAILABLE:
                    result = self._process_video(file_path, options)
                elif file_ext in [".pdf", ".png", ".jpg", ".jpeg"] and MINERU_PIPELINE_AVAILABLE:
                    options["parse_mode"] = "pipeline"
                    result = self._process_with_mineru(file_path, options)
                elif self.markitdown:
                    result = self._process_with_markitdown(file_path)
                else:
                    raise ValueError(f"Unsupported file type for Auto mode: {file_ext}")

            else:
                if FORMAT_ENGINES_AVAILABLE:
                    engine = FormatEngineRegistry.get_engine(backend)
                    if engine:
                        result = self._process_with_format_engine(file_path, options, engine_name=backend)
                    else:
                        raise ValueError(f"Unknown backend: {backend}")
                else:
                    raise ValueError(f"Unknown backend: {backend}")

            if not result:
                raise ValueError("No result generated by engine")

            # 6. ä¿å­˜å®Œæ•´ç»“æžœåˆ°æ•°æ®åº“ (åŒ…å« json_content å’Œ pdf_path)
            # æ³¨æ„ï¼šresult å­—å…¸ä¸­å¿…é¡»åŒ…å« pdf_pathï¼Œå¦åˆ™å‰ç«¯æ— æ³•é¢„è§ˆæºæ–‡ä»¶
            self.task_db.update_task_status(
                task_id=task_id,
                status="completed",
                result_path=result["result_path"],
                error_message=None,
                data=json.dumps({
                    "pdf_path": result.get("pdf_path"),      # å…³é”®ï¼šä¾›å‰ç«¯å·¦ä¾§é¢„è§ˆä½¿ç”¨
                    "json_content": result.get("json_content"), # å…³é”®ï¼šä¾›å‰ç«¯å³ä¾§å¸ƒå±€æ¸²æŸ“ä½¿ç”¨
                    "markdown": result.get("content"),
                    "markdown_file": result.get("markdown_file") # å¯é€‰ï¼šä¸‹è½½æ–‡ä»¶å
                })
            )

            # 7. åˆå¹¶å­ä»»åŠ¡
            if parent_task_id:
                parent_id_to_merge = self.task_db.on_child_task_completed(task_id)
                if parent_id_to_merge:
                    try:
                        self._merge_parent_task_results(parent_id_to_merge)
                    except Exception as e:
                        self.task_db.update_task_status(parent_id_to_merge, "failed", error_message=str(e))

        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            self.task_db.update_task_status(task_id, "failed", error_message=error_msg)
            if parent_task_id:
                self.task_db.on_child_task_failed(task_id, error_msg)
            raise

    # -------------------------------------------------------------------------
    # Helper: ç¡®ä¿ PDF å­˜åœ¨äºŽ Output ç›®å½•
    # -------------------------------------------------------------------------
    def _ensure_pdf_in_output(self, file_path: str, output_dir: Path, preferred_name: str = None) -> str:
        """
        ç¡®ä¿è¾“å‡ºç›®å½•ä¸­æœ‰ PDF æ–‡ä»¶ï¼Œä¾›å‰ç«¯é¢„è§ˆä½¿ç”¨ã€‚
        è¿”å›žç›¸å¯¹äºŽ output_dir çš„æ–‡ä»¶åã€‚
        """
        output_dir = Path(output_dir)
        source_file = Path(file_path)
        
        # 1. å¦‚æžœæºæ–‡ä»¶ä¸æ˜¯ PDF (å¯èƒ½æ˜¯å›¾ç‰‡)ï¼Œå°è¯•æ‰¾è½¬æ¢åŽçš„ PDF
        if source_file.suffix.lower() != ".pdf":
             # æ£€æŸ¥æ˜¯å¦æœ‰ layout.pdf
            layout_pdfs = list(output_dir.glob("*_layout.pdf"))
            if layout_pdfs:
                return layout_pdfs[0].name
            # å¦‚æžœæ˜¯å›¾ç‰‡ï¼Œå¯èƒ½æ²¡æœ‰ PDFï¼Œè¿”å›ž None æˆ–è½¬æ¢
            return None

        # 2. å¦‚æžœæºæ–‡ä»¶æ˜¯ PDF
        # ä¼˜å…ˆæŸ¥æ‰¾ MinerU ç”Ÿæˆçš„å¸¦å¸ƒå±€ä¿¡æ¯çš„ PDF
        layout_pdfs = list(output_dir.glob("*_layout.pdf"))
        if layout_pdfs:
            return layout_pdfs[0].name
        
        # 3. å¦‚æžœæ²¡æœ‰å¸ƒå±€ PDFï¼Œåˆ™å¤åˆ¶æº PDF åˆ°è¾“å‡ºç›®å½•
        target_name = preferred_name or source_file.name
        target_path = output_dir / target_name
        
        # å¦‚æžœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…å¤§å°ä¸åŒï¼ˆç®€å•æ£€æŸ¥ï¼‰ï¼Œåˆ™å¤åˆ¶
        if not target_path.exists():
            try:
                shutil.copy2(source_file, target_path)
                logger.info(f"ðŸ“„ Copied source PDF to output: {target_name}")
            except Exception as e:
                logger.warning(f"Failed to copy source PDF: {e}")
                return None
        
        return target_name

    # -------------------------------------------------------------------------
    # Engine Processor Implementations
    # -------------------------------------------------------------------------
    def _process_with_mineru(self, file_path: str, options: dict) -> dict:
        if self.mineru_pipeline_engine is None:
            from mineru_pipeline import MinerUPipelineEngine
            self.mineru_pipeline_engine = MinerUPipelineEngine(
                device=self.engine_device,
                vlm_api_base=self.mineru_vllm_api
            )

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        if "http-client" in options.get("parse_mode", "") and self.mineru_vllm_api:
            options.setdefault("server_url", self.mineru_vllm_api.replace("/v1", ""))

        result = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)
        
        actual_output = Path(result["result_path"])
        normalize_output(actual_output)

        if actual_output.resolve() != output_dir.resolve():
            try:
                for item in actual_output.iterdir():
                    dest = output_dir / item.name
                    if dest.exists():
                        if dest.is_dir(): shutil.rmtree(dest)
                        else: dest.unlink()
                    shutil.move(str(item), str(dest))
                shutil.rmtree(actual_output)
            except Exception as e:
                logger.warning(f"Flattening warning: {e}")
        
        # [ä¿®å¤] ç¡®ä¿ PDF å­˜åœ¨å¹¶è¿”å›žè·¯å¾„
        pdf_path = self._ensure_pdf_in_output(file_path, output_dir)

        return {
            "result_path": str(output_dir),
            "content": result.get("markdown", ""),
            "json_content": result.get("json_content"),
            "pdf_path": pdf_path, # è¿”å›žç»™å‰ç«¯
            "markdown_file": result.get("markdown_file")
        }

    def _process_with_paddleocr_vl(self, file_path: str, options: dict) -> dict:
        if self.accelerator == "cpu":
            raise RuntimeError("PaddleOCR-VL requires GPU")
            
        if self.paddleocr_vl_engine is None:
            from paddleocr_vl import PaddleOCRVLEngine
            self.paddleocr_vl_engine = PaddleOCRVLEngine(device="cuda:0", model_name="PaddleOCR-VL-1.5")

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        result = self.paddleocr_vl_engine.parse(file_path, output_path=str(output_dir), **options)
        
        # [ä¿®å¤] å¤åˆ¶æºæ–‡ä»¶ä»¥ä¾¿é¢„è§ˆ
        pdf_path = self._ensure_pdf_in_output(file_path, output_dir)

        normalize_output(output_dir)
        
        return {
            "result_path": str(output_dir), 
            "content": result.get("markdown", ""),
            "json_content": result.get("json_content"), # å¿…é¡»ä¼ é€’
            "pdf_path": pdf_path
        }

    def _process_with_paddleocr_vl_vllm(self, file_path: str, options: dict) -> dict:
        if self.accelerator == "cpu":
            raise RuntimeError("PaddleOCR-VL-VLLM requires GPU")

        if self.paddleocr_vl_vllm_engine is None:
            from paddleocr_vl_vllm import PaddleOCRVLVLLMEngine
            self.paddleocr_vl_vllm_engine = PaddleOCRVLVLLMEngine(
                device="cuda:0",
                vllm_api_base=self.paddleocr_vl_vllm_api,
                model_name="PaddleOCR-VL-1.5-0.9B"
            )

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        result = self.paddleocr_vl_vllm_engine.parse(file_path, output_path=str(output_dir), **options)
        
        # [ä¿®å¤] å¤åˆ¶æºæ–‡ä»¶ä»¥ä¾¿é¢„è§ˆ (å…³é”®ä¿®å¤)
        pdf_path = self._ensure_pdf_in_output(file_path, output_dir)
        
        normalize_output(output_dir, handle_method="paddleocr-vl")
        
        return {
            "result_path": str(output_dir), 
            "content": result.get("markdown", ""),
            "json_content": result.get("json_content"), # å…³é”®ï¼šæ”¯æŒå³ä¾§é«˜äº®
            "pdf_path": pdf_path # å…³é”®ï¼šæ”¯æŒå·¦ä¾§é¢„è§ˆ
        }

    def _process_audio(self, file_path: str, options: dict) -> dict:
        if self.sensevoice_engine is None:
            from audio_engines import SenseVoiceEngine
            self.sensevoice_engine = SenseVoiceEngine(device=self.engine_device)

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        result = self.sensevoice_engine.parse(
            audio_path=file_path,
            output_path=str(output_dir),
            language=options.get("lang", "auto"),
            use_itn=options.get("use_itn", True),
            enable_speaker_diarization=options.get("enable_speaker_diarization", False)
        )
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_video(self, file_path: str, options: dict) -> dict:
        if self.video_engine is None:
            from video_engines import VideoProcessingEngine
            self.video_engine = VideoProcessingEngine(device=self.engine_device)

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        result = self.video_engine.parse(
            video_path=file_path,
            output_path=str(output_dir),
            language=options.get("lang", "auto"),
            use_itn=options.get("use_itn", True),
            keep_audio=options.get("keep_audio", False),
            enable_keyframe_ocr=options.get("enable_keyframe_ocr", False),
            ocr_backend=options.get("ocr_backend", "paddleocr-vl"),
            keep_keyframes=options.get("keep_keyframes", False)
        )
        
        (output_dir / f"{Path(file_path).stem}_video_analysis.md").write_text(result["markdown"], encoding="utf-8")
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": result["markdown"]}

    def _process_with_markitdown(self, file_path: str) -> dict:
        if not self.markitdown:
            raise RuntimeError("MarkItDown not available")

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        result = self.markitdown.convert(file_path)
        markdown_content = result.text_content

        if Path(file_path).suffix.lower() == ".docx":
            try:
                from utils.docx_image_extractor import extract_images_from_docx, append_images_to_markdown
                images_dir = output_dir / "images"
                images = extract_images_from_docx(file_path, str(images_dir))
                if images:
                    markdown_content = append_images_to_markdown(markdown_content, images)
            except Exception as e:
                logger.warning(f"DOCX image extraction failed: {e}")

        (output_dir / f"{Path(file_path).stem}_markitdown.md").write_text(markdown_content, encoding="utf-8")
        normalize_output(output_dir)
        
        # MarkItDown ä¹Ÿå¯ä»¥å°è¯•ç”Ÿæˆ PDF é¢„è§ˆ (å¦‚æžœæºæ–‡ä»¶æ˜¯ PDF)
        pdf_path = self._ensure_pdf_in_output(file_path, output_dir)
        
        return {"result_path": str(output_dir), "content": markdown_content, "pdf_path": pdf_path}

    def _process_with_format_engine(self, file_path: str, options: dict, engine_name: Optional[str] = None) -> dict:
        lang = options.get("language", "en")
        
        if engine_name:
            engine = FormatEngineRegistry.get_engine(engine_name)
        else:
            engine = FormatEngineRegistry.get_engine_by_extension(file_path)
            
        if not engine:
            raise ValueError("No format engine available")

        result = engine.parse(file_path, options={"language": lang})
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        (output_dir / "result.md").write_text(result["markdown"], encoding="utf-8")
        (output_dir / "result.json").write_text(json.dumps(result["json_content"], indent=2, ensure_ascii=False), encoding="utf-8")
        
        normalize_output(output_dir)
        return {
            "result_path": str(output_dir),
            "content": result["content"],
            "json_content": result["json_content"]
        }

    # -------------------------------------------------------------------------
    # Utilities
    # -------------------------------------------------------------------------
    def _convert_office_to_pdf(self, file_path: str) -> str:
        input_file = Path(file_path)
        final_pdf = input_file.parent / f"{input_file.stem}.pdf"
        if final_pdf.exists(): final_pdf.unlink()

        try:
            with tempfile.TemporaryDirectory(prefix="libreoffice_") as temp_dir:
                temp_path = Path(temp_dir)
                temp_input = temp_path / input_file.name
                shutil.copy2(input_file, temp_input)

                cmd = [
                    "libreoffice", "--headless", "--convert-to", "pdf",
                    "--outdir", str(temp_path), str(temp_input)
                ]
                subprocess.run(cmd, check=True, timeout=120, capture_output=True)
                
                temp_pdf = temp_path / f"{input_file.stem}.pdf"
                if not temp_pdf.exists(): raise RuntimeError("PDF output missing")
                shutil.move(str(temp_pdf), str(final_pdf))
                return str(final_pdf)
        except Exception as e:
            raise RuntimeError(f"Office conversion failed: {e}")

    def _preprocess_remove_watermark(self, file_path: str, options: dict) -> Path:
        if not self.watermark_handler: raise RuntimeError("Watermark handler missing")
        output_file = Path(self.output_dir) / f"{Path(file_path).stem}_no_watermark.pdf"
        
        kwargs = {}
        for k in ["auto_detect", "force_scanned", "remove_text", "remove_images", 
                  "remove_annotations", "watermark_keywords", "watermark_dpi", 
                  "watermark_conf_threshold", "watermark_dilation"]:
            if k in options: kwargs[k.replace("watermark_", "")] = options[k]

        return self.watermark_handler.remove_watermark(input_path=file_path, output_path=str(output_file), **kwargs)

    def _should_split_pdf(self, task_id, file_path, task, options):
        from utils.pdf_utils import get_pdf_page_count, split_pdf_file
        if os.getenv("PDF_SPLIT_ENABLED", "true").lower() != "true": return False
        
        threshold = int(os.getenv("PDF_SPLIT_THRESHOLD_PAGES", "500"))
        chunk_size = int(os.getenv("PDF_SPLIT_CHUNK_SIZE", "500"))
        
        try:
            pages = get_pdf_page_count(Path(file_path))
            if pages <= threshold: return False
            
            logger.info(f"ðŸ”€ Splitting PDF ({pages} pages)...")
            self.task_db.convert_to_parent_task(task_id, child_count=0)
            split_dir = Path(self.output_dir) / "splits" / task_id
            split_dir.mkdir(parents=True, exist_ok=True)
            
            chunks = split_pdf_file(Path(file_path), split_dir, chunk_size, task_id)
            
            for chunk in chunks:
                c_ops = options.copy()
                c_ops["chunk_info"] = {k: chunk[k] for k in ["start_page", "end_page", "page_count"]}
                self.task_db.create_child_task(
                    parent_task_id=task_id,
                    file_name=f"{Path(file_path).stem}_p{chunk['start_page']}-{chunk['end_page']}.pdf",
                    file_path=chunk["path"],
                    backend=task.get("backend", "auto"),
                    options=c_ops,
                    priority=task.get("priority", 0),
                    user_id=task.get("user_id")
                )
            
            self.task_db.convert_to_parent_task(task_id, child_count=len(chunks))
            logger.info(f"âœ‚ï¸  Split into {len(chunks)} subtasks")
            return True
        except Exception as e:
            logger.error(f"âŒ PDF split failed: {e}")
            return False

    def _merge_parent_task_results(self, parent_task_id):
        parent_task = self.task_db.get_task_with_children(parent_task_id)
        children = parent_task.get("children", [])
        if not children: return

        children.sort(key=lambda x: json.loads(x.get("options", "{}")).get("chunk_info", {}).get("start_page", 0))
        
        parent_out = Path(self.output_dir) / Path(parent_task["file_path"]).stem
        parent_out.mkdir(parents=True, exist_ok=True)
        
        md_parts, json_pages = [], []
        
        for child in children:
            if child["status"] != "completed": continue
            res_dir = Path(child["result_path"])
            
            md_file = next((f for f in res_dir.rglob("*.md") if f.name == "result.md"), None) or list(res_dir.rglob("*.md"))[0]
            md_parts.append(md_file.read_text(encoding="utf-8"))
            
            json_file = next((f for f in res_dir.rglob("*.json") if "result" in f.name or "content" in f.name), None)
            if json_file:
                try:
                    data = json.loads(json_file.read_text(encoding="utf-8"))
                    offset = json.loads(child.get("options", "{}")).get("chunk_info", {}).get("start_page", 1) - 1
                    for p in data.get("pages", []):
                        if "page_number" in p: p["page_number"] += offset
                        json_pages.append(p)
                except: pass

        (parent_out / "result.md").write_text("\n\n\n\n".join(md_parts), encoding="utf-8")
        if json_pages:
            (parent_out / "result.json").write_text(json.dumps({"pages": json_pages}, indent=2, ensure_ascii=False), encoding="utf-8")
        
        # [ä¿®å¤] å¤åˆ¶çˆ¶ä»»åŠ¡çš„æºæ–‡ä»¶åˆ°è¾“å‡º
        self._ensure_pdf_in_output(parent_task["file_path"], parent_out)

        normalize_output(parent_out)
        self.task_db.update_task_status(parent_task_id, "completed", result_path=str(parent_out))
        self._cleanup_child_task_files(children)

    def _cleanup_child_task_files(self, children):
        for child in children:
            try:
                if child.get("file_path"): Path(child["file_path"]).unlink(missing_ok=True)
            except: pass

    # LitServe Interfaces
    def decode_request(self, request): return request.get("action", "health")
    def predict(self, action):
        if action == "health":
            return {"status": "healthy", "worker_id": self.worker_id}
        elif action == "poll":
            if self.enable_worker_loop:
                return {"status": "skipped", "message": "Auto-loop active"}
            task = self.task_db.pull_task()
            if task:
                try:
                    self._process_task(task)
                    return {"status": "completed", "task_id": task["task_id"]}
                except Exception as e:
                    return {"status": "failed", "error": str(e)}
            return {"status": "empty"}
        return {"status": "error", "message": "Invalid action"}
    def encode_response(self, response): return response
    def teardown(self):
        self.running = False
        if hasattr(self, "worker_thread"): self.worker_thread.join(timeout=2)


def start_litserve_workers(
    output_dir=None, accelerator="auto", devices="auto", workers_per_device=1,
    port=8001, poll_interval=0.5, enable_worker_loop=True,
    paddleocr_vl_vllm_engine_enabled=False, paddleocr_vl_vllm_api_list=[],
    mineru_vllm_api_list=[]
):
    def resolve_auto_accelerator():
        try:
            from importlib.metadata import distribution
            distribution("torch")
            if check_cuda_with_nvidia_smi() > 0: return "cuda"
        except: pass
        return "cpu"

    if output_dir is None:
        output_dir = os.getenv("OUTPUT_PATH", str(Path(__file__).parent.parent / "data" / "output"))

    if accelerator == "auto":
        accelerator = resolve_auto_accelerator()

    logger.info(f"ðŸš€ Starting Worker | Acc: {accelerator} | Devices: {devices} | Out: {output_dir}")

    api = MinerUWorkerAPI(
        output_dir=output_dir,
        poll_interval=poll_interval,
        enable_worker_loop=enable_worker_loop,
        paddleocr_vl_vllm_engine_enabled=paddleocr_vl_vllm_engine_enabled,
        paddleocr_vl_vllm_api_list=paddleocr_vl_vllm_api_list,
        mineru_vllm_api_list=mineru_vllm_api_list,
    )

    server = ls.LitServer(
        api,
        accelerator=accelerator,
        devices=devices,
        workers_per_device=workers_per_device,
        timeout=False,
    )

    def graceful_shutdown(signum=None, frame=None):
        if hasattr(api, "teardown"): api.teardown()
        sys.exit(0)

    signal.signal(signal.SIGINT, graceful_shutdown)
    signal.signal(signal.SIGTERM, graceful_shutdown)
    atexit.register(lambda: api.teardown() if hasattr(api, "teardown") else None)

    server.run(port=port, generate_client_file=False)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--output-dir", type=str, default=None)
    parser.add_argument("--port", type=int, default=8001)
    parser.add_argument("--accelerator", type=str, default="auto")
    parser.add_argument("--workers-per-device", type=int, default=1)
    parser.add_argument("--devices", type=str, default="auto")
    parser.add_argument("--poll-interval", type=float, default=0.5)
    parser.add_argument("--disable-worker-loop", action="store_true")
    parser.add_argument("--paddleocr-vl-vllm-engine-enabled", action="store_true")
    parser.add_argument("--paddleocr-vl-vllm-api-list", type=parse_list_arg, default=[])
    parser.add_argument("--mineru-vllm-api-list", type=parse_list_arg, default=[])
    args = parser.parse_args()

    # Env Var Fallbacks
    devices = args.devices
    if devices == "auto":
        env_dev = os.getenv("CUDA_VISIBLE_DEVICES")
        if env_dev: devices = env_dev

    port = args.port
    if port == 8001:
        port = int(os.getenv("WORKER_PORT", "8001"))

    start_litserve_workers(
        output_dir=args.output_dir,
        accelerator=args.accelerator,
        devices=devices,
        workers_per_device=args.workers_per_device,
        port=port,
        poll_interval=args.poll_interval,
        enable_worker_loop=not args.disable_worker_loop,
        paddleocr_vl_vllm_engine_enabled=args.paddleocr_vl_vllm_engine_enabled,
        paddleocr_vl_vllm_api_list=args.paddleocr_vl_vllm_api_list,
        mineru_vllm_api_list=args.mineru_vllm_api_list,
    )
