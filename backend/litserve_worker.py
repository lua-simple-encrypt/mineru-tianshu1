"""
MinerU Tianshu - LitServe Worker
å¤©æ¢ LitServe Worker

ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å° - GPU Worker
æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†
ä½¿ç”¨ LitServe å®ç° GPU èµ„æºçš„è‡ªåŠ¨è´Ÿè½½å‡è¡¡
Worker ä¸»åŠ¨å¾ªç¯æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
from pathlib import Path
from typing import Optional
import multiprocessing
import requests

# Fix litserve MCP compatibility with mcp>=1.1.0
# Completely disable LitServe's internal MCP to avoid conflicts with our standalone MCP Server
import litserve as ls
from litserve.connector import check_cuda_with_nvidia_smi
from utils import parse_list_arg

try:
    # Patch LitServe's MCP module to disable it completely
    import litserve.mcp as ls_mcp
    import sys
    from contextlib import asynccontextmanager

    # Inject MCPServer (mcp.server.lowlevel.Server) as dummy
    if not hasattr(ls_mcp, "MCPServer"):

        class DummyMCPServer:
            def __init__(self, *args, **kwargs):
                pass

        ls_mcp.MCPServer = DummyMCPServer
        if "litserve.mcp" in sys.modules:
            sys.modules["litserve.mcp"].MCPServer = DummyMCPServer

    # Inject StreamableHTTPSessionManager as dummy
    if not hasattr(ls_mcp, "StreamableHTTPSessionManager"):

        class DummyStreamableHTTPSessionManager:
            def __init__(self, *args, **kwargs):
                pass

        ls_mcp.StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager
        if "litserve.mcp" in sys.modules:
            sys.modules["litserve.mcp"].StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager

    # Replace _LitMCPServerConnector with a complete dummy implementation
    class DummyMCPConnector:
        """å®Œå…¨ç¦ç”¨ LitServe å†…ç½® MCP çš„ Dummy å®ç°"""

        def __init__(self, *args, **kwargs):
            self.mcp_server = None
            self.session_manager = None
            self.request_handler = None

        @asynccontextmanager
        async def lifespan(self, app):
            """ç©ºçš„ lifespan context managerï¼Œä¸åšä»»ä½•äº‹æƒ…"""
            yield  # ä»€ä¹ˆéƒ½ä¸åšï¼Œç›´æ¥è®©æœåŠ¡å™¨å¯åŠ¨

        def connect_mcp_server(self, *args, **kwargs):
            """ç©ºçš„ connect_mcp_server æ–¹æ³•ï¼Œä¸åšä»»ä½•äº‹æƒ…"""
            pass  # ä»€ä¹ˆéƒ½ä¸åšï¼Œè·³è¿‡ MCP åˆå§‹åŒ–

    # æ›¿æ¢ _LitMCPServerConnector ç±»
    ls_mcp._LitMCPServerConnector = DummyMCPConnector

    # åŒæ—¶æ›´æ–° sys.modules ä¸­çš„å¼•ç”¨
    if "litserve.mcp" in sys.modules:
        sys.modules["litserve.mcp"]._LitMCPServerConnector = DummyMCPConnector

except Exception as e:
    # If patching fails, log warning and continue
    # The server might still work or fail with a clearer error message
    import warnings

    warnings.warn(f"Failed to patch litserve.mcp (MCP will be disabled): {e}")

from loguru import logger

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ MinerU
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from task_db import TaskDB
from output_normalizer import normalize_output

# å»¶è¿Ÿå¯¼å…¥ MinerUï¼Œé¿å…è¿‡æ—©åˆå§‹åŒ– CUDA
# MinerU ä¼šåœ¨ setup() è®¾ç½® CUDA_VISIBLE_DEVICES åå†å¯¼å…¥
# from mineru.cli.common import do_parse
# from mineru.utils.model_utils import get_vram, clean_memory

# å¯¼å…¥ importlib ç”¨äºæ£€æŸ¥æ¨¡å—å¯ç”¨æ€§
import importlib.util

# å°è¯•å¯¼å…¥ markitdown
try:
    from markitdown import MarkItDown

    MARKITDOWN_AVAILABLE = True
except ImportError:
    MARKITDOWN_AVAILABLE = False
    logger.warning("âš ï¸  markitdown not available, Office format parsing will be disabled")

# æ£€æŸ¥ PaddleOCR-VL æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
PADDLEOCR_VL_AVAILABLE = importlib.util.find_spec("paddleocr_vl") is not None
if PADDLEOCR_VL_AVAILABLE:
    logger.info("âœ… PaddleOCR-VL engine available")
else:
    logger.info("â„¹ï¸  PaddleOCR-VL not available (optional)")

# æ£€æŸ¥ PaddleOCR-VL-VLLM æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
PADDLEOCR_VL_VLLM_AVAILABLE = importlib.util.find_spec("paddleocr_vl_vllm") is not None
if PADDLEOCR_VL_VLLM_AVAILABLE:
    logger.info("âœ… PaddleOCR-VL-VLLM engine available")
else:
    logger.info("â„¹ï¸  PaddleOCR-VL-VLLM not available (optional)")

# æ£€æŸ¥ MinerU Pipeline æ˜¯å¦å¯ç”¨
MINERU_PIPELINE_AVAILABLE = importlib.util.find_spec("mineru_pipeline") is not None
if MINERU_PIPELINE_AVAILABLE:
    logger.info("âœ… MinerU Pipeline engine available")
else:
    logger.info("â„¹ï¸  MinerU Pipeline not available (optional)")

# å°è¯•å¯¼å…¥ SenseVoice éŸ³é¢‘å¤„ç†
SENSEVOICE_AVAILABLE = importlib.util.find_spec("audio_engines") is not None
if SENSEVOICE_AVAILABLE:
    logger.info("âœ… SenseVoice audio engine available")
else:
    logger.info("â„¹ï¸  SenseVoice not available (optional)")

# å°è¯•å¯¼å…¥è§†é¢‘å¤„ç†å¼•æ“
VIDEO_ENGINE_AVAILABLE = importlib.util.find_spec("video_engines") is not None
if VIDEO_ENGINE_AVAILABLE:
    logger.info("âœ… Video processing engine available")
else:
    logger.info("â„¹ï¸  Video processing engine not available (optional)")

# æ£€æŸ¥æ°´å°å»é™¤å¼•æ“æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
WATERMARK_REMOVAL_AVAILABLE = importlib.util.find_spec("remove_watermark") is not None
if WATERMARK_REMOVAL_AVAILABLE:
    logger.info("âœ… Watermark removal engine available")
else:
    logger.info("â„¹ï¸  Watermark removal engine not available (optional)")

# å°è¯•å¯¼å…¥æ ¼å¼å¼•æ“ï¼ˆä¸“ä¸šé¢†åŸŸæ ¼å¼æ”¯æŒï¼‰
try:
    from format_engines import FormatEngineRegistry, FASTAEngine, GenBankEngine

    # æ³¨å†Œæ‰€æœ‰å¼•æ“
    FormatEngineRegistry.register(FASTAEngine())
    FormatEngineRegistry.register(GenBankEngine())

    FORMAT_ENGINES_AVAILABLE = True
    logger.info("âœ… Format engines available")
    logger.info(f"   Supported extensions: {', '.join(FormatEngineRegistry.get_supported_extensions())}")
except ImportError as e:
    FORMAT_ENGINES_AVAILABLE = False
    logger.info(f"â„¹ï¸  Format engines not available (optional): {e}")


# ==============================================================================
# VLLM Container Controller (äº’æ–¥åˆ‡æ¢ç‰ˆ + Pickle ä¿®å¤)
# ==============================================================================
class VLLMController:
    """ç®¡ç† vLLM Docker å®¹å™¨çš„äº’æ–¥å¯åŠ¨"""
    
    def __init__(self):
        # ä¸åœ¨ __init__ ä¸­åˆ›å»º clientï¼Œç¡®ä¿å¯¹è±¡æ˜¯å¯åºåˆ—åŒ–çš„ (Pickle Safe)
        pass

    def _get_client(self):
        """æŒ‰éœ€è·å– Docker å®¢æˆ·ç«¯"""
        try:
            import docker
            # è¿æ¥åˆ°æŒ‚è½½çš„ /var/run/docker.sock
            return docker.from_env()
        except Exception as e:
            logger.warning(f"âš ï¸  Docker client init failed: {e}")
            return None

    def ensure_service(self, target_container: str, conflict_container: str, health_url: str, timeout: int = 300):
        """
        ç¡®ä¿ç›®æ ‡å®¹å™¨è¿è¡Œï¼Œå¹¶å…³é—­å†²çªå®¹å™¨ (äº’æ–¥é€»è¾‘)
        
        Args:
            target_container: éœ€è¦è¿è¡Œçš„å®¹å™¨å
            conflict_container: éœ€è¦å…³é—­çš„äº’æ–¥å®¹å™¨å
            health_url: ç›®æ ‡å®¹å™¨çš„å¥åº·æ£€æŸ¥åœ°å€
            timeout: è¶…æ—¶æ—¶é—´
        """
        client = self._get_client()
        if not client:
            return
        
        try:
            # 1. æ£€æŸ¥ç›®æ ‡å®¹å™¨æ˜¯å¦å·²ç»åœ¨è¿è¡Œ
            try:
                target = client.containers.get(target_container)
                if target.status == 'running':
                    # å¦‚æœå·²ç»åœ¨è¿è¡Œï¼Œç›´æ¥è¿”å›ï¼Œæ— éœ€æ“ä½œ
                    logger.info(f"âœ… Target service {target_container} is already running.")
                    return
            except Exception as e:
                # å¦‚æœæ‰¾ä¸åˆ°å®¹å™¨ï¼Œè¯´æ˜æ²¡åˆ›å»ºï¼Œæç¤ºç”¨æˆ·
                logger.error(f"âŒ Container {target_container} not found. Please ensure it is created (e.g. docker compose up --no-start).")
                raise e

            # 2. åœæ­¢å†²çªå®¹å™¨ (é‡Šæ”¾æ˜¾å­˜)
            try:
                conflict = client.containers.get(conflict_container)
                if conflict.status == 'running':
                    logger.info(f"ğŸ›‘ Stopping conflicting service {conflict_container} to free VRAM...")
                    conflict.stop()
                    logger.info(f"âœ… Service {conflict_container} stopped.")
            except Exception:
                # å†²çªå®¹å™¨å¯èƒ½ä¸å­˜åœ¨æˆ–å·²åœæ­¢ï¼Œå¿½ç•¥
                pass

            # 3. å¯åŠ¨ç›®æ ‡å®¹å™¨
            logger.info(f"ğŸš€ Starting service {target_container} (Cold Start)...")
            target.start()

            # 4. ç­‰å¾…å¥åº·æ£€æŸ¥
            self._wait_for_health(health_url, timeout)
            
        finally:
            try:
                client.close()
            except:
                pass

    def _wait_for_health(self, url: str, timeout: int):
        """è½®è¯¢å¥åº·æ£€æŸ¥æ¥å£"""
        start_time = time.time()
        logger.info(f"â³ Waiting for service at {url} (timeout: {timeout}s)...")
        
        while time.time() - start_time < timeout:
            try:
                # æ˜¾å¼ä½¿ç”¨ host.docker.internal æˆ–è€…æ˜¯ Docker DNS å
                response = requests.get(url, timeout=2)
                if response.status_code == 200:
                    logger.info(f"âœ… Service is ready: {url}")
                    return
            except Exception:
                pass
            
            time.sleep(2) # æ¯2ç§’é‡è¯•ä¸€æ¬¡
        
        raise TimeoutError(f"Service at {url} did not become ready in {timeout} seconds")


class MinerUWorkerAPI(ls.LitAPI):
    def __init__(
        self,
        paddleocr_vl_vllm_api_list=None,
        mineru_vllm_api_list=None,
        output_dir=None,
        poll_interval=0.5,
        enable_worker_loop=True,
        paddleocr_vl_vllm_engine_enabled=False,
    ):
        """
        åˆå§‹åŒ– APIï¼šç›´æ¥åœ¨è¿™é‡Œæ¥æ”¶æ‰€æœ‰éœ€è¦çš„å‚æ•°
        """
        super().__init__()
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent
        default_output = project_root / "data" / "output"
        self.output_dir = output_dir or os.getenv("OUTPUT_PATH", str(default_output))
        self.poll_interval = poll_interval
        self.enable_worker_loop = enable_worker_loop
        self.paddleocr_vl_vllm_engine_enabled = paddleocr_vl_vllm_engine_enabled
        self.paddleocr_vl_vllm_api_list = paddleocr_vl_vllm_api_list or []
        self.mineru_vllm_api_list = mineru_vllm_api_list or []  # ä¿å­˜ MinerU API åˆ—è¡¨
        
        ctx = multiprocessing.get_context("spawn")
        self._global_worker_counter = ctx.Value("i", 0)

        # ã€å…³é”®ä¿®æ”¹ã€‘åœ¨ __init__ ä¸­ç›´æ¥åˆå§‹åŒ– VLLMController
        # å› ä¸ºç°åœ¨çš„ VLLMController ä¸æŒæœ‰ä¸å¯åºåˆ—åŒ–çš„ client å¯¹è±¡ï¼Œæ‰€ä»¥æ˜¯å®‰å…¨çš„
        self.vllm_controller = VLLMController()

    def setup(self, device):
        """
        åˆå§‹åŒ– Worker (æ¯ä¸ª GPU ä¸Šè°ƒç”¨ä¸€æ¬¡)

        Args:
            device: è®¾å¤‡ ID (cuda:0, cuda:1, cpu ç­‰)
        """
        ## é…ç½®æ¯ä¸ª Worker çš„å…¨å±€ç´¢å¼•å¹¶å°è¯•æ€§åˆ†é… API
        with self._global_worker_counter.get_lock():
            my_global_index = self._global_worker_counter.value
            self._global_worker_counter.value += 1
        logger.info(f"ğŸ”¢ [Init] I am Global Worker #{my_global_index} (on {device})")
        
        # 1. åˆ†é… PaddleOCR VLLM API
        if self.paddleocr_vl_vllm_engine_enabled and len(self.paddleocr_vl_vllm_api_list) > 0:
            assigned_api = self.paddleocr_vl_vllm_api_list[my_global_index % len(self.paddleocr_vl_vllm_api_list)]
            self.paddleocr_vl_vllm_api = assigned_api
            logger.info(f"ğŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: {assigned_api}")
        else:
            self.paddleocr_vl_vllm_api = None
            logger.info(f"ğŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: None")

        # 2. åˆ†é… MinerU VLLM API (æ–°å¢)
        if len(self.mineru_vllm_api_list) > 0:
            assigned_mineru_api = self.mineru_vllm_api_list[my_global_index % len(self.mineru_vllm_api_list)]
            self.mineru_vllm_api = assigned_mineru_api
            logger.info(f"ğŸ”§ Worker #{my_global_index} assigned MinerU VLLM API: {assigned_mineru_api}")
        else:
            self.mineru_vllm_api = None
            logger.info(f"ğŸ”§ Worker #{my_global_index} assigned MinerU VLLM API: None")

        # ============================================================================
        # ã€å…³é”®ã€‘ç¬¬ä¸€æ­¥ï¼šç«‹å³è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆå¿…é¡»åœ¨ä»»ä½•å¯¼å…¥ä¹‹å‰ï¼‰
        # ============================================================================
        # LitServe ä¸ºæ¯ä¸ª worker è¿›ç¨‹åˆ†é…ä¸åŒçš„ device (cuda:0, cuda:1, ...)
        # æˆ‘ä»¬éœ€è¦åœ¨å¯¼å…¥ä»»ä½• CUDA åº“ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå®ç°è¿›ç¨‹çº§ GPU éš”ç¦»
        if "cuda:" in str(device):
            gpu_id = str(device).split(":")[-1]
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            # ã€å…³é”®ã€‘è®¾ç½® MinerU çš„è®¾å¤‡æ¨¡å¼ä¸º cuda:0
            # å› ä¸ºè®¾ç½®äº† CUDA_VISIBLE_DEVICES åï¼Œè¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€å¼ å¡ï¼ˆé€»è¾‘ ID å˜ä¸º 0ï¼‰
            os.environ["MINERU_DEVICE_MODE"] = "cuda:0"
            logger.info(f"ğŸ¯ [GPU Isolation] Set CUDA_VISIBLE_DEVICES={gpu_id} (Physical GPU {gpu_id} â†’ Logical GPU 0)")
            logger.info("ğŸ¯ [GPU Isolation] Set MINERU_DEVICE_MODE=cuda:0")

        import socket

        # é…ç½®æ¨¡å‹ä¸‹è½½æºï¼ˆå¿…é¡»åœ¨ MinerU åˆå§‹åŒ–ä¹‹å‰ï¼‰
        # ä»ç¯å¢ƒå˜é‡ MODEL_DOWNLOAD_SOURCE è¯»å–é…ç½®
        # æ”¯æŒ: modelscope, huggingface, auto (é»˜è®¤)
        model_source = os.getenv("MODEL_DOWNLOAD_SOURCE", "auto").lower()

        if model_source in ["modelscope", "auto"]:
            # å°è¯•ä½¿ç”¨ ModelScopeï¼ˆä¼˜å…ˆï¼‰
            try:
                import importlib.util

                if importlib.util.find_spec("modelscope") is not None:
                    logger.info("ğŸ“¦ Model download source: ModelScope (å›½å†…æ¨è)")
                    logger.info("   Note: ModelScope automatically uses China mirror for faster downloads")
                else:
                    raise ImportError("modelscope not found")
            except ImportError:
                if model_source == "modelscope":
                    logger.warning("âš ï¸  ModelScope not available, falling back to HuggingFace")
                model_source = "huggingface"

        if model_source == "huggingface":
            # é…ç½® HuggingFace é•œåƒï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ä½¿ç”¨å›½å†…é•œåƒï¼‰
            hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
            os.environ.setdefault("HF_ENDPOINT", hf_endpoint)
            logger.info(f"ğŸ“¦ Model download source: HuggingFace (via: {hf_endpoint})")
        elif model_source == "modelscope":
            ## é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®,æ¥è®©æ¨¡å‹ä»modelscopeå¹³å°ä¸‹è½½, æˆ–è€…ä»modelscopeçš„ç¼“å­˜ç›®å½•åŠ è½½
            os.environ["MINERU_MODEL_SOURCE"] = "modelscope"
            logger.info("ğŸ“¦ Model download source: ModelScope")
        else:
            logger.warning(f"âš ï¸  Unknown model download source: {model_source}")

        self.device = device
        # ä¿å­˜ accelerator ç±»å‹ï¼ˆä» device å­—ç¬¦ä¸²æ¨æ–­ï¼‰
        # device å¯èƒ½æ˜¯ "cuda:0", "cuda:1", "cpu" ç­‰
        if "cuda" in str(device):
            self.accelerator = "cuda"
            self.engine_device = "cuda:0"  # å¼•æ“ç»Ÿä¸€ä½¿ç”¨ cuda:0ï¼ˆå› ä¸ºå·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼‰
        else:
            self.accelerator = "cpu"
            self.engine_device = "cpu"  # CPU æ¨¡å¼

        logger.info(f"ğŸ¯ [Device] Accelerator: {self.accelerator}, Engine Device: {self.engine_device}")

        # ä»ç±»å±æ€§è·å–é…ç½®ï¼ˆç”± start_litserve_workers è®¾ç½®ï¼‰
        # é»˜è®¤ä½¿ç”¨å…±äº«è¾“å‡ºç›®å½•ï¼ˆDocker ç¯å¢ƒï¼‰
        project_root = Path(__file__).parent.parent
        default_output_path = project_root / "data" / "output"
        default_output = os.getenv("OUTPUT_PATH", str(default_output_path))
        self.output_dir = getattr(self.__class__, "_output_dir", default_output)
        self.poll_interval = getattr(self.__class__, "_poll_interval", 0.5)
        self.enable_worker_loop = getattr(self.__class__, "_enable_worker_loop", True)

        # ============================================================================
        # ç¬¬äºŒæ­¥ï¼šç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥ MinerU äº†ï¼ˆCUDA_VISIBLE_DEVICES å·²è®¾ç½®ï¼‰
        # ============================================================================
        global get_vram, clean_memory
        from mineru.utils.model_utils import get_vram, clean_memory

        # é…ç½® MinerU çš„ VRAM è®¾ç½®
        if os.getenv("MINERU_VIRTUAL_VRAM_SIZE", None) is None:
            device_mode = os.environ.get("MINERU_DEVICE_MODE", str(device))
            if device_mode.startswith("cuda") or device_mode.startswith("npu"):
                try:
                    # æ³¨æ„ï¼šget_vram éœ€è¦ä¼ å…¥è®¾å¤‡å­—ç¬¦ä¸²ï¼ˆå¦‚ "cuda:0"ï¼‰
                    vram = round(get_vram(device_mode))
                    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = str(vram)
                    logger.info(f"ğŸ® [MinerU VRAM] Detected: {vram}GB")
                except Exception as e:
                    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "8"  # é»˜è®¤å€¼
                    logger.warning(f"âš ï¸  Failed to detect VRAM, using default: 8GB ({e})")
            else:
                os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "1"
                logger.info("ğŸ® [MinerU VRAM] CPU mode, set to 1GB")

        # éªŒè¯ PyTorch CUDA è®¾ç½®
        try:
            import torch

            if torch.cuda.is_available():
                visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "all")
                device_count = torch.cuda.device_count()
                logger.info("âœ… PyTorch CUDA verified:")
                logger.info(f"   CUDA_VISIBLE_DEVICES = {visible_devices}")
                logger.info(f"   torch.cuda.device_count() = {device_count}")
                if device_count == 1:
                    logger.info(f"   âœ… SUCCESS: Process isolated to 1 GPU (physical GPU {visible_devices})")
                else:
                    logger.warning(f"   âš ï¸  WARNING: Expected 1 GPU but found {device_count}")
            else:
                logger.warning("âš ï¸  CUDA not available")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to verify PyTorch CUDA: {e}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä»»åŠ¡æ•°æ®åº“ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå…¼å®¹ Docker å’Œæœ¬åœ°ï¼‰
        db_path_env = os.getenv("DATABASE_PATH")
        if db_path_env:
            db_path = Path(db_path_env).resolve()  # ä½¿ç”¨ resolve() è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            logger.info(f"ğŸ“Š Using DATABASE_PATH from environment: {db_path_env} -> {db_path}")
        else:
            # é»˜è®¤è·¯å¾„ï¼ˆä¸ TaskDB å’Œ AuthDB ä¿æŒä¸€è‡´ï¼‰
            project_root = Path(__file__).parent.parent
            default_db = project_root / "data" / "db" / "mineru_tianshu.db"
            db_path = default_db.resolve()
            logger.warning(f"âš ï¸  DATABASE_PATH not set, using default: {db_path}")

        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        db_path.parent.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²ä¼ é€’ç»™ TaskDB
        db_path_str = str(db_path.absolute())
        logger.info(f"ğŸ“Š Database path (absolute): {db_path_str}")

        self.task_db = TaskDB(db_path_str)

        # éªŒè¯æ•°æ®åº“è¿æ¥å¹¶è¾“å‡ºåˆå§‹ç»Ÿè®¡
        try:
            stats = self.task_db.get_queue_stats()
            logger.info(f"ğŸ“Š Database initialized: {db_path} (exists: {db_path.exists()})")
            logger.info(f"ğŸ“Š TaskDB.db_path: {self.task_db.db_path}")
            logger.info(f"ğŸ“Š Initial queue stats: {stats}")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize database or get stats: {e}")
            logger.exception(e)

        # Worker çŠ¶æ€
        self.running = True
        self.current_task_id = None

        # ç”Ÿæˆå”¯ä¸€çš„ worker_id: tianshu-{hostname}-{device}-{pid}
        hostname = socket.gethostname()
        pid = os.getpid()
        self.worker_id = f"tianshu-{hostname}-{device}-{pid}"
        # å­è¿›ç¨‹ï¼ˆsetup ä¸­ï¼‰ï¼š

        # åˆå§‹åŒ–å¯é€‰çš„å¤„ç†å¼•æ“
        self.markitdown = MarkItDown() if MARKITDOWN_AVAILABLE else None
        self.mineru_pipeline_engine = None  # å»¶è¿ŸåŠ è½½
        self.paddleocr_vl_engine = None  # å»¶è¿ŸåŠ è½½
        self.paddleocr_vl_vllm_engine = None  # å»¶è¿ŸåŠ è½½
        self.sensevoice_engine = None  # å»¶è¿ŸåŠ è½½
        self.video_engine = None  # å»¶è¿ŸåŠ è½½
        self.watermark_handler = None  # å»¶è¿ŸåŠ è½½

        logger.info("=" * 60)
        logger.info(f"ğŸš€ Worker Setup: {self.worker_id}")
        logger.info("=" * 60)
        logger.info(f"ğŸ“ Device: {device}")
        logger.info(f"ğŸ“‚ Output Dir: {self.output_dir}")
        logger.info(f"ğŸ—ƒï¸  Database: {db_path}")
        logger.info(f"ğŸ”„ Worker Loop: {'Enabled' if self.enable_worker_loop else 'Disabled'}")
        if self.enable_worker_loop:
            logger.info(f"â±ï¸  Poll Interval: {self.poll_interval}s")
        logger.info("")

        # æ‰“å°å¯ç”¨çš„å¼•æ“
        logger.info("ğŸ“¦ Available Engines:")
        logger.info(f"   â€¢ MarkItDown: {'âœ…' if MARKITDOWN_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ MinerU Pipeline: {'âœ…' if MINERU_PIPELINE_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ PaddleOCR-VL: {'âœ…' if PADDLEOCR_VL_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ SenseVoice: {'âœ…' if SENSEVOICE_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ Video Engine: {'âœ…' if VIDEO_ENGINE_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ Watermark Removal: {'âœ…' if WATERMARK_REMOVAL_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ Format Engines: {'âœ…' if FORMAT_ENGINES_AVAILABLE else 'âŒ'}")
        logger.info("")

        # æ£€æµ‹å’Œåˆå§‹åŒ–æ°´å°å»é™¤å¼•æ“ï¼ˆä»… CUDAï¼‰
        if WATERMARK_REMOVAL_AVAILABLE and "cuda" in str(device).lower():
            try:
                logger.info("ğŸ¨ Initializing watermark removal engine...")
                # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿åœ¨ CUDA_VISIBLE_DEVICES è®¾ç½®ä¹‹å
                from remove_watermark.pdf_watermark_handler import PDFWatermarkHandler

                # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
                # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
                self.watermark_handler = PDFWatermarkHandler(device="cuda:0", use_lama=True)
                gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
                logger.info(f"âœ… Watermark removal engine initialized on cuda:0 (physical GPU {gpu_id})")
            except Exception as e:
                logger.error(f"âŒ Failed to initialize watermark removal engine: {e}")
                self.watermark_handler = None

        logger.info("âœ… Worker ready")
        logger.info(f"   LitServe Device: {device}")
        logger.info(f"   MinerU Device Mode: {os.environ.get('MINERU_DEVICE_MODE', 'auto')}")
        logger.info(f"   MinerU VRAM: {os.environ.get('MINERU_VIRTUAL_VRAM_SIZE', 'unknown')}GB")
        if "cuda" in str(device).lower():
            physical_gpu = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"   Physical GPU: {physical_gpu}")

        # å¦‚æœå¯ç”¨äº† worker å¾ªç¯ï¼Œå¯åŠ¨åå°çº¿ç¨‹æ‹‰å–ä»»åŠ¡
        if self.enable_worker_loop:
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            logger.info(f"ğŸ”„ Worker loop started (poll_interval={self.poll_interval}s)")
        else:
            logger.info("â¸ï¸  Worker loop disabled, waiting for manual triggers")

    def _worker_loop(self):
        """
        Worker åå°å¾ªç¯ï¼šæŒç»­æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†

        è¿™ä¸ªå¾ªç¯åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä¸æ–­æ£€æŸ¥æ˜¯å¦æœ‰æ–°ä»»åŠ¡
        ä¸€æ—¦æœ‰ä»»åŠ¡ï¼Œç«‹å³å¤„ç†ï¼Œå¤„ç†å®Œæˆåç»§ç»­å¾ªç¯
        """
        logger.info(f"ğŸ” {self.worker_id} started task polling loop")

        # è®°å½•åˆå§‹è¯Šæ–­ä¿¡æ¯
        try:
            stats = self.task_db.get_queue_stats()
            logger.info(f"ğŸ“Š Initial queue stats: {stats}")
            logger.info(f"ğŸ—ƒï¸  Database path: {self.task_db.db_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to get initial queue stats: {e}")

        loop_count = 0
        last_stats_log = 0
        stats_log_interval = 20  # æ¯20æ¬¡å¾ªç¯è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆçº¦10ç§’ï¼‰

        while self.running:
            try:
                loop_count += 1

                # æ‹‰å–ä»»åŠ¡ï¼ˆåŸå­æ“ä½œï¼Œé˜²æ­¢é‡å¤å¤„ç†ï¼‰
                task = self.task_db.get_next_task(worker_id=self.worker_id)

                if task:
                    task_id = task["task_id"]
                    self.current_task_id = task_id
                    logger.info(
                        f"ğŸ“¥ {self.worker_id} pulled task: {task_id} (file: {task.get('file_name', 'unknown')})"
                    )

                    try:
                        # å¤„ç†ä»»åŠ¡
                        self._process_task(task)
                        logger.info(f"âœ… {self.worker_id} completed task: {task_id}")
                    except Exception as e:
                        logger.error(f"âŒ {self.worker_id} failed task {task_id}: {e}")
                        logger.exception(e)
                    finally:
                        self.current_task_id = None
                else:
                    # æ²¡æœ‰ä»»åŠ¡ï¼Œç©ºé—²ç­‰å¾…
                    # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ä»¥ä¾¿è¯Šæ–­
                    if loop_count - last_stats_log >= stats_log_interval:
                        try:
                            stats = self.task_db.get_queue_stats()
                            pending = stats.get("pending", 0)
                            processing = stats.get("processing", 0)

                            if pending > 0:
                                logger.warning(
                                    f"âš ï¸  {self.worker_id} polling (loop #{loop_count}): "
                                    f"{pending} pending tasks found but not pulled! "
                                    f"Processing: {processing}, Completed: {stats.get('completed', 0)}, "
                                    f"Failed: {stats.get('failed', 0)}"
                                )
                            elif loop_count % 100 == 0:  # æ¯50ç§’ï¼ˆ100æ¬¡å¾ªç¯ï¼‰è¾“å‡ºä¸€æ¬¡
                                logger.info(
                                    f"ğŸ’¤ {self.worker_id} idle (loop #{loop_count}): "
                                    f"No pending tasks. Queue stats: {stats}"
                                )
                        except Exception as e:
                            logger.error(f"âŒ Failed to get queue stats: {e}")

                        last_stats_log = loop_count

                    time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"âŒ Worker loop error (loop #{loop_count}): {e}")
                logger.exception(e)
                time.sleep(self.poll_interval)

    def _process_task(self, task: dict):
        """
        å¤„ç†å•ä¸ªä»»åŠ¡ (é›†æˆäº’æ–¥å¯åŠ¨é€»è¾‘)

        Args:
            task: ä»»åŠ¡å­—å…¸ï¼ˆä»æ•°æ®åº“æ‹‰å–ï¼‰
        """
        task_id = task["task_id"]
        file_path = task["file_path"]
        options = json.loads(task.get("options", "{}"))
        parent_task_id = task.get("parent_task_id")
        backend = task.get("backend", "auto")
        
        try:
            # 1. æ™ºèƒ½æœåŠ¡åˆ‡æ¢é€»è¾‘
            paddle_container = "tianshu-vllm-paddleocr"
            mineru_container = "tianshu-vllm-mineru"
            
            # å¦‚æœæ˜¯ PaddleOCR ä»»åŠ¡
            if backend == "paddleocr-vl-vllm" and self.paddleocr_vl_vllm_api:
                base = self.paddleocr_vl_vllm_api.replace("/v1", "")
                health = f"{base}/health"
                # ç¡®ä¿ Paddle è¿è¡Œï¼Œå…³é—­ MinerU
                self.vllm_controller.ensure_service(paddle_container, mineru_container, health)
                
            # å¦‚æœæ˜¯ MinerU ä»»åŠ¡ (vlm/hybrid local æ¨¡å¼)
            # æ³¨æ„: remote client æ¨¡å¼ä¸éœ€è¦å¯åŠ¨æœ¬åœ°å®¹å™¨
            elif backend in ["vlm-auto-engine", "hybrid-auto-engine"] and self.mineru_vllm_api:
                base = self.mineru_vllm_api.replace("/v1", "")
                health = f"{base}/health"
                # ç¡®ä¿ MinerU è¿è¡Œï¼Œå…³é—­ Paddle
                self.vllm_controller.ensure_service(mineru_container, paddle_container, health)

            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            file_ext = Path(file_path).suffix.lower()

            # ã€æ–°å¢ã€‘Office è½¬ PDF é¢„å¤„ç†
            office_extensions = [".docx", ".xlsx", ".pptx", ".doc", ".xls", ".ppt"]
            if file_ext in office_extensions and options.get("convert_office_to_pdf", False):
                logger.info(f"ğŸ“„ [Preprocessing] Converting Office to PDF: {file_path}")
                try:
                    pdf_path = self._convert_office_to_pdf(file_path)

                    # æ›´æ–°æ–‡ä»¶è·¯å¾„å’Œæ‰©å±•å
                    original_file_path = file_path
                    file_path = pdf_path
                    file_ext = ".pdf"

                    logger.info(f"âœ… [Preprocessing] Office converted, continuing with PDF: {pdf_path}")
                    logger.info(f"   Original: {Path(original_file_path).name}")
                    logger.info(f"   Converted: {Path(pdf_path).name}")

                except Exception as e:
                    logger.warning(f"âš ï¸ [Preprocessing] Office to PDF conversion failed: {e}")
                    logger.warning(f"   Falling back to MarkItDown for: {file_path}")
                    # è½¬æ¢å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ–‡ä»¶ï¼ˆMarkItDown å¤„ç†ï¼‰

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†åˆ† PDFï¼ˆä»…å¯¹éå­ä»»åŠ¡çš„ PDF è¿›è¡Œåˆ¤æ–­ï¼‰
            if file_ext == ".pdf" and not parent_task_id:
                if self._should_split_pdf(task_id, file_path, task, options):
                    # PDF å·²è¢«æ‹†åˆ†ï¼Œå½“å‰ä»»åŠ¡å·²è½¬ä¸ºçˆ¶ä»»åŠ¡ï¼Œç›´æ¥è¿”å›
                    return

            # 0. å¯é€‰ï¼šé¢„å¤„ç† - å»é™¤æ°´å°ï¼ˆä»… PDFï¼Œä½œä¸ºé¢„å¤„ç†æ­¥éª¤ï¼‰
            if file_ext == ".pdf" and options.get("remove_watermark", False) and self.watermark_handler:
                logger.info(f"ğŸ¨ [Preprocessing] Removing watermark from PDF: {file_path}")
                try:
                    cleaned_pdf_path = self._preprocess_remove_watermark(file_path, options)
                    file_path = str(cleaned_pdf_path)  # ä½¿ç”¨å»æ°´å°åçš„æ–‡ä»¶ç»§ç»­å¤„ç†
                    logger.info(f"âœ… [Preprocessing] Watermark removed, continuing with: {file_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ [Preprocessing] Watermark removal failed: {e}, continuing with original file")
                    # ç»§ç»­ä½¿ç”¨åŸæ–‡ä»¶å¤„ç†

            # ç»Ÿä¸€çš„å¼•æ“è·¯ç”±é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ backendï¼Œå¦åˆ™è‡ªåŠ¨é€‰æ‹©
            result = None  # åˆå§‹åŒ– result

            # 1. ç”¨æˆ·æŒ‡å®šäº†éŸ³é¢‘å¼•æ“
            if backend == "sensevoice":
                if not SENSEVOICE_AVAILABLE:
                    raise ValueError("SenseVoice engine is not available")
                logger.info(f"ğŸ¤ Processing with SenseVoice: {file_path}")
                result = self._process_audio(file_path, options)

            # 3. ç”¨æˆ·æŒ‡å®šäº†è§†é¢‘å¼•æ“
            elif backend == "video":
                if not VIDEO_ENGINE_AVAILABLE:
                    raise ValueError("Video processing engine is not available")
                logger.info(f"ğŸ¬ Processing with video engine: {file_path}")
                result = self._process_video(file_path, options)

            # 4. ç”¨æˆ·æŒ‡å®šäº† PaddleOCR-VL
            elif backend == "paddleocr-vl":
                if not PADDLEOCR_VL_AVAILABLE:
                    raise ValueError("PaddleOCR-VL engine is not available")
                logger.info(f"ğŸ” Processing with PaddleOCR-VL: {file_path}")
                result = self._process_with_paddleocr_vl(file_path, options)

            # 5. ç”¨æˆ·æŒ‡å®šäº† PaddleOCR-VL-VLLM
            elif backend == "paddleocr-vl-vllm":
                if (
                    not PADDLEOCR_VL_VLLM_AVAILABLE
                    or not self.paddleocr_vl_vllm_engine_enabled
                    or len(self.paddleocr_vl_vllm_api_list) == 0
                ):
                    raise ValueError("PaddleOCR-VL-VLLM engine is not available")
                logger.info(f"ğŸ” Processing with PaddleOCR-VL-VLLM: {file_path}")
                result = self._process_with_paddleocr_vl_vllm(file_path, options)
            
            # 6. ç”¨æˆ·æŒ‡å®šäº† MinerU çš„æŸç§æ¨¡å¼ (pipeline, vlm-*, hybrid-*)
            elif "pipeline" in backend or "vlm-" in backend or "hybrid-" in backend:
                if not MINERU_PIPELINE_AVAILABLE:
                    raise ValueError(f"MinerU Pipeline engine is not available, cannot run {backend}")
                
                logger.info(f"ğŸ”§ Processing with MinerU ({backend}): {file_path}")
                
                # å°† backend æ¨¡å¼å†™å…¥ optionsï¼Œä¼ é€’ç»™ Engine
                options["parse_mode"] = backend  # ã€å…³é”®ã€‘ç¡®ä¿ parse_mode æ­£ç¡®ä¼ é€’
                result = self._process_with_mineru(file_path, options)

            # 7. auto æ¨¡å¼ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©å¼•æ“
            elif backend == "auto":
                # 7.1 æ£€æŸ¥æ˜¯å¦æ˜¯ä¸“ä¸šæ ¼å¼ï¼ˆFASTA, GenBank ç­‰ï¼‰
                if FORMAT_ENGINES_AVAILABLE and FormatEngineRegistry.is_supported(file_path):
                    logger.info(f"ğŸ§¬ [Auto] Processing with format engine: {file_path}")
                    result = self._process_with_format_engine(file_path, options)

                # 7.2 æ£€æŸ¥æ˜¯å¦æ˜¯éŸ³é¢‘æ–‡ä»¶
                elif file_ext in [".wav", ".mp3", ".flac", ".m4a", ".ogg"] and SENSEVOICE_AVAILABLE:
                    logger.info(f"ğŸ¤ [Auto] Processing audio file: {file_path}")
                    result = self._process_audio(file_path, options)

                # 7.3 æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘æ–‡ä»¶
                elif file_ext in [".mp4", ".avi", ".mkv", ".mov", ".flv", ".wmv"] and VIDEO_ENGINE_AVAILABLE:
                    logger.info(f"ğŸ¬ [Auto] Processing video file: {file_path}")
                    result = self._process_video(file_path, options)

                # 7.4 é»˜è®¤ä½¿ç”¨ MinerU Pipeline å¤„ç† PDF/å›¾ç‰‡
                elif file_ext in [".pdf", ".png", ".jpg", ".jpeg"] and MINERU_PIPELINE_AVAILABLE:
                    logger.info(f"ğŸ”§ [Auto] Processing with MinerU Pipeline (Default): {file_path}")
                    # é»˜è®¤ä½¿ç”¨ pipeline æ¨¡å¼
                    options["parse_mode"] = "pipeline" 
                    result = self._process_with_mineru(file_path, options)

                # 7.5 å…œåº•ï¼šOffice æ–‡æ¡£/æ–‡æœ¬/HTML ä½¿ç”¨ MarkItDownï¼ˆå¦‚æœå¯ç”¨ï¼‰
                elif (
                    file_ext in [".docx", ".xlsx", ".pptx", ".doc", ".xls", ".ppt", ".html", ".txt", ".csv"]
                    and self.markitdown
                ):
                    logger.info(f"ğŸ“„ [Auto] Processing Office/Text file with MarkItDown: {file_path}")
                    result = self._process_with_markitdown(file_path)

                else:
                    # æ²¡æœ‰åˆé€‚çš„å¤„ç†å™¨
                    supported_formats = "PDF, PNG, JPG (MinerU/PaddleOCR), Audio (SenseVoice), Video, FASTA, GenBank"
                    if self.markitdown:
                        supported_formats += ", Office/Text (MarkItDown)"
                    raise ValueError(
                        f"Unsupported file type: file={file_path}, ext={file_ext}. "
                        f"Supported formats: {supported_formats}"
                    )

            else:
                # 8. å°è¯•ä½¿ç”¨æ ¼å¼å¼•æ“ï¼ˆç”¨æˆ·æ˜ç¡®æŒ‡å®šäº† fasta, genbank ç­‰ï¼‰
                if FORMAT_ENGINES_AVAILABLE:
                    engine = FormatEngineRegistry.get_engine(backend)
                    if engine is not None:
                        logger.info(f"ğŸ§¬ Processing with format engine: {backend}")
                        result = self._process_with_format_engine(file_path, options, engine_name=backend)
                    else:
                        # æœªçŸ¥çš„ backend
                        raise ValueError(
                            f"Unknown backend: {backend}. "
                            f"Supported backends: auto, pipeline, vlm-*, hybrid-*, paddleocr-vl, sensevoice, video, fasta, genbank"
                        )
                else:
                    # æ ¼å¼å¼•æ“ä¸å¯ç”¨
                    raise ValueError(
                        f"Unknown backend: {backend}. "
                        f"Supported backends: auto, pipeline, vlm-*, hybrid-*, paddleocr-vl, sensevoice, video"
                    )

            # æ£€æŸ¥ result æ˜¯å¦è¢«æ­£ç¡®èµ‹å€¼
            if result is None:
                raise ValueError(f"No result generated for backend: {backend}, file: {file_path}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            self.task_db.update_task_status(
                task_id=task_id,
                status="completed",
                result_path=result["result_path"],
                error_message=None,
            )

            # å¦‚æœæ˜¯å­ä»»åŠ¡,æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘åˆå¹¶
            if parent_task_id:
                parent_id_to_merge = self.task_db.on_child_task_completed(task_id)

                if parent_id_to_merge:
                    # æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ,æ‰§è¡Œåˆå¹¶
                    logger.info(f"ğŸ”€ All subtasks completed, merging results for parent task {parent_id_to_merge}")
                    try:
                        self._merge_parent_task_results(parent_id_to_merge)
                    except Exception as merge_error:
                        logger.error(f"âŒ Failed to merge parent task {parent_id_to_merge}: {merge_error}")
                        # æ ‡è®°çˆ¶ä»»åŠ¡ä¸ºå¤±è´¥
                        self.task_db.update_task_status(
                            parent_id_to_merge, "failed", error_message=f"Merge failed: {merge_error}"
                        )

            # æ¸…ç†æ˜¾å­˜ï¼ˆå¦‚æœæ˜¯ GPUï¼‰
            if "cuda" in str(self.device).lower():
                clean_memory()

        except Exception as e:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            error_msg = f"{type(e).__name__}: {str(e)}"
            self.task_db.update_task_status(task_id=task_id, status="failed", result_path=None, error_message=error_msg)

            # å¦‚æœæ˜¯å­ä»»åŠ¡å¤±è´¥,æ ‡è®°çˆ¶ä»»åŠ¡å¤±è´¥
            if parent_task_id:
                self.task_db.on_child_task_failed(task_id, error_msg)

            raise

    # ---------------- ENGINE WRAPPERS ----------------

    def _process_with_paddleocr_vl(self, file_path: str, options: dict) -> dict:
        if self.accelerator == "cpu": raise RuntimeError("PaddleOCR-VL requires GPU")
        if self.paddleocr_vl_engine is None:
            from paddleocr_vl import PaddleOCRVLEngine
            # âœ… ä¿®æ”¹ï¼šä½¿ç”¨æ˜ç¡®çš„å®˜æ–¹æ¨¡å‹åç§° "PaddleOCR-VL-1.5"
            # è¿™æ · PaddleX ä¼šè‡ªåŠ¨åœ¨ PADDLEX_HOME ä¸‹æŸ¥æ‰¾æˆ–ä¸‹è½½
            self.paddleocr_vl_engine = PaddleOCRVLEngine(device="cuda:0", model_name="PaddleOCR-VL-1.5")
            
        output_dir = Path(self.output_dir) / Path(file_path).stem
        result = self.paddleocr_vl_engine.parse(file_path, output_path=str(output_dir))
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_with_paddleocr_vl_vllm(self, file_path: str, options: dict) -> dict:
        if self.accelerator == "cpu": raise RuntimeError("PaddleOCR-VL-VLLM requires GPU")
        if self.paddleocr_vl_vllm_engine is None:
            from paddleocr_vl_vllm import PaddleOCRVLVLLMEngine
            # âœ… ä¿®æ”¹ï¼šä½¿ç”¨æ˜ç¡®çš„å®˜æ–¹æ¨¡å‹åç§°
            self.paddleocr_vl_vllm_engine = PaddleOCRVLVLLMEngine(
                device="cuda:0", 
                vllm_api_base=self.paddleocr_vl_vllm_api,
                model_name="PaddleOCR-VL-1.5-0.9B"
            )
            
        output_dir = Path(self.output_dir) / Path(file_path).stem
        result = self.paddleocr_vl_vllm_engine.parse(file_path, output_path=str(output_dir))
        normalize_output(output_dir, handle_method="paddleocr-vl")
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_with_mineru(self, file_path: str, options: dict) -> dict:
        if self.mineru_pipeline_engine is None:
            from mineru_pipeline import MinerUPipelineEngine
            self.mineru_pipeline_engine = MinerUPipelineEngine(
                device=self.engine_device,
                vlm_api_base=self.mineru_vllm_api
            )
            
        output_dir = Path(self.output_dir) / Path(file_path).stem
        # Check remote
        if "http-client" in options.get("parse_mode", "") and not options.get("server_url"):
            if self.mineru_vllm_api:
                options["server_url"] = self.mineru_vllm_api.replace("/v1", "")

        result = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)
        # Normalize inside engine output
        actual_output = Path(result["result_path"])
        normalize_output(actual_output)
        return {"result_path": str(actual_output), "content": result["markdown"]}

    def _process_with_markitdown(self, file_path: str) -> dict:
        if not self.markitdown: raise RuntimeError("MarkItDown unavailable")
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        result = self.markitdown.convert(file_path)
        (output_dir / "result.md").write_text(result.text_content, encoding="utf-8")
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": result.text_content}
    
    # ... (Video/Audio/PDF split helpers omitted for brevity but should be kept as in original)
    def _convert_office_to_pdf(self, file_path: str) -> str:
        import subprocess
        import shutil
        import tempfile
        from pathlib import Path

        input_file = Path(file_path)
        final_output_dir = input_file.parent

        # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å
        final_pdf_file = final_output_dir / f"{input_file.stem}.pdf"

        # å¦‚æœå·²å­˜åœ¨åŒå PDFï¼Œå…ˆåˆ é™¤
        if final_pdf_file.exists():
            final_pdf_file.unlink()

        logger.info(f"ğŸ”„ Converting Office to PDF: {input_file.name}")

        try:
            # ä½¿ç”¨ /tmp ä½œä¸ºä¸´æ—¶ç›®å½•ï¼ˆé¿å… Docker æŒ‚è½½å·å†™å…¥é—®é¢˜ï¼‰
            with tempfile.TemporaryDirectory(prefix="libreoffice_") as temp_dir:
                temp_dir_path = Path(temp_dir)

                # å¤åˆ¶è¾“å…¥æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                temp_input = temp_dir_path / input_file.name
                shutil.copy2(input_file, temp_input)

                # åœ¨ä¸´æ—¶ç›®å½•æ‰§è¡Œè½¬æ¢
                cmd = [
                    "libreoffice",
                    "--headless",  # æ— ç•Œé¢æ¨¡å¼
                    "--convert-to",
                    "pdf",  # è½¬æ¢ä¸º PDF
                    "--outdir",
                    str(temp_dir_path),  # è¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•
                    str(temp_input),  # è¾“å…¥æ–‡ä»¶
                ]

                # æ‰§è¡Œè½¬æ¢ï¼ˆè¶…æ—¶ 120 ç§’ï¼‰
                result = subprocess.run(cmd, check=True, timeout=120, capture_output=True, text=True)

                # ä¸´æ—¶è¾“å‡ºæ–‡ä»¶è·¯å¾„
                temp_pdf = temp_dir_path / f"{input_file.stem}.pdf"

                # éªŒè¯è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not temp_pdf.exists():
                    stderr_output = result.stderr if result.stderr else "No error output"
                    raise RuntimeError(
                        f"LibreOffice conversion failed: output file not found: {temp_pdf}\nstderr: {stderr_output}"
                    )

                # ç§»åŠ¨è½¬æ¢åçš„ PDF åˆ°æœ€ç»ˆç›®å½•
                shutil.move(str(temp_pdf), str(final_pdf_file))

                logger.info(
                    f"âœ… Office converted to PDF: {final_pdf_file.name} ({final_pdf_file.stat().st_size / 1024:.1f} KB)"
                )

                return str(final_pdf_file)

        except subprocess.TimeoutExpired:
            raise RuntimeError(f"LibreOffice conversion timeout (>120s): {input_file.name}")
        except subprocess.CalledProcessError as e:
            stderr_output = e.stderr if e.stderr else "No error output"
            raise RuntimeError(f"LibreOffice conversion failed: {stderr_output}")
        except Exception as e:
            raise RuntimeError(f"Office to PDF conversion error: {e}")
            
    def _preprocess_remove_watermark(self, file_path: str, options: dict) -> Path:
        if not self.watermark_handler:
            raise RuntimeError("Watermark removal is not available (CUDA required)")

        # è®¾ç½®è¾“å‡ºè·¯å¾„
        output_file = Path(self.output_dir) / f"{Path(file_path).stem}_no_watermark.pdf"

        # æ„å»ºå‚æ•°å­—å…¸ï¼ˆåªä¼ é€’å®é™…æä¾›çš„å‚æ•°ï¼‰
        kwargs = {}

        # é€šç”¨å‚æ•°
        if "auto_detect" in options:
            kwargs["auto_detect"] = options["auto_detect"]
        if "force_scanned" in options:
            kwargs["force_scanned"] = options["force_scanned"]

        # å¯ç¼–è¾‘ PDF å‚æ•°
        if "remove_text" in options:
            kwargs["remove_text"] = options["remove_text"]
        if "remove_images" in options:
            kwargs["remove_images"] = options["remove_images"]
        if "remove_annotations" in options:
            kwargs["remove_annotations"] = options["remove_annotations"]
        if "watermark_keywords" in options:
            kwargs["keywords"] = options["watermark_keywords"]

        # æ‰«æä»¶ PDF å‚æ•°
        if "watermark_dpi" in options:
            kwargs["dpi"] = options["watermark_dpi"]
        if "watermark_conf_threshold" in options:
            kwargs["conf_threshold"] = options["watermark_conf_threshold"]
        if "watermark_dilation" in options:
            kwargs["dilation"] = options["watermark_dilation"]

        # å»é™¤æ°´å°ï¼ˆè¿”å›è¾“å‡ºè·¯å¾„ï¼‰
        cleaned_pdf_path = self.watermark_handler.remove_watermark(
            input_path=file_path, output_path=str(output_file), **kwargs
        )

        return cleaned_pdf_path
        
    def _should_split_pdf(self, task_id: str, file_path: str, task: dict, options: dict) -> bool:
        from utils.pdf_utils import get_pdf_page_count, split_pdf_file

        # è¯»å–é…ç½®
        pdf_split_enabled = os.getenv("PDF_SPLIT_ENABLED", "true").lower() == "true"
        if not pdf_split_enabled:
            return False

        pdf_split_threshold = int(os.getenv("PDF_SPLIT_THRESHOLD_PAGES", "500"))
        pdf_split_chunk_size = int(os.getenv("PDF_SPLIT_CHUNK_SIZE", "500"))

        try:
            # å¿«é€Ÿè¯»å– PDF é¡µæ•°ï¼ˆåªè¯»å…ƒæ•°æ®ï¼‰
            page_count = get_pdf_page_count(Path(file_path))
            logger.info(f"ğŸ“„ PDF has {page_count} pages (threshold: {pdf_split_threshold})")

            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‹†åˆ†
            if page_count <= pdf_split_threshold:
                return False

            logger.info(
                f"ğŸ”€ Large PDF detected ({page_count} pages), splitting into chunks of {pdf_split_chunk_size} pages"
            )

            # å°†å½“å‰ä»»åŠ¡è½¬ä¸ºçˆ¶ä»»åŠ¡
            self.task_db.convert_to_parent_task(task_id, child_count=0)

            # æ‹†åˆ† PDF æ–‡ä»¶
            split_dir = Path(self.output_dir) / "splits" / task_id
            split_dir.mkdir(parents=True, exist_ok=True)

            chunks = split_pdf_file(
                pdf_path=Path(file_path),
                output_dir=split_dir,
                chunk_size=pdf_split_chunk_size,
                parent_task_id=task_id,
            )

            logger.info(f"âœ‚ï¸  PDF split into {len(chunks)} chunks")

            # ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»ºå­ä»»åŠ¡
            backend = task.get("backend", "auto")
            priority = task.get("priority", 0)
            user_id = task.get("user_id")

            for chunk_info in chunks:
                # å¤åˆ¶é€‰é¡¹å¹¶æ·»åŠ åˆ†å—ä¿¡æ¯
                chunk_options = options.copy()
                chunk_options["chunk_info"] = {
                    "start_page": chunk_info["start_page"],
                    "end_page": chunk_info["end_page"],
                    "page_count": chunk_info["page_count"],
                }

                # åˆ›å»ºå­ä»»åŠ¡
                child_task_id = self.task_db.create_child_task(
                    parent_task_id=task_id,
                    file_name=f"{Path(file_path).stem}_pages_{chunk_info['start_page']}-{chunk_info['end_page']}.pdf",
                    file_path=chunk_info["path"],
                    backend=backend,
                    options=chunk_options,
                    priority=priority,
                    user_id=user_id,
                )

                logger.info(
                    f"  âœ… Created subtask {child_task_id}: pages {chunk_info['start_page']}-{chunk_info['end_page']}"
                )

            # æ›´æ–°çˆ¶ä»»åŠ¡çš„å­ä»»åŠ¡æ•°é‡
            self.task_db.convert_to_parent_task(task_id, child_count=len(chunks))

            logger.info(f"ğŸ‰ Large PDF split complete: {len(chunks)} subtasks created for parent task {task_id}")

            return True

        except Exception as e:
            logger.error(f"âŒ Failed to split PDF: {e}")
            logger.warning("âš ï¸  Falling back to processing as single task")
            return False

    def decode_request(self, request): return request.get("action", "health")
    def predict(self, action): return {"status": "healthy"}
    def encode_response(self, response): return response

# ... start_litserve_workers and main block (same as original) ...
if __name__ == "__main__":
    import argparse
    # ... args parsing ...
    parser = argparse.ArgumentParser()
    parser.add_argument("--output-dir", type=str)
    parser.add_argument("--port", type=int, default=8001)
    parser.add_argument("--accelerator", type=str, default="auto")
    parser.add_argument("--devices", type=str, default="auto")
    parser.add_argument("--workers-per-device", type=int, default=1, help="Number of workers per device (default: 1)")
    parser.add_argument("--poll-interval", type=float, default=0.5, help="Worker poll interval in seconds (default: 0.5)")
    parser.add_argument("--disable-worker-loop", action="store_true", help="Disable automatic worker loop")
    parser.add_argument("--paddleocr-vl-vllm-engine-enabled", action="store_true")
    parser.add_argument("--paddleocr-vl-vllm-api-list", type=parse_list_arg, default=[])
    parser.add_argument("--mineru-vllm-api-list", type=parse_list_arg, default=[])
    args = parser.parse_args()

    start_litserve_workers(
        output_dir=args.output_dir,
        accelerator=args.accelerator,
        devices=args.devices,
        workers_per_device=args.workers_per_device,
        port=args.port,
        poll_interval=args.poll_interval,
        enable_worker_loop=not args.disable_worker_loop,
        paddleocr_vl_vllm_engine_enabled=args.paddleocr_vl_vllm_engine_enabled,
        paddleocr_vl_vllm_api_list=args.paddleocr_vl_vllm_api_list,
        mineru_vllm_api_list=args.mineru_vllm_api_list
    )
