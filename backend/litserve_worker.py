"""
MinerU Tianshu - LitServe Worker
å¤©æ¢ LitServe Worker

ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å° - GPU Worker
æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†
ä½¿ç”¨ LitServe å®ç° GPU èµ„æºçš„è‡ªåŠ¨è´Ÿè½½å‡è¡¡
Worker ä¸»åŠ¨å¾ªç¯æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
from pathlib import Path
from typing import Optional
import multiprocessing
import requests

# Fix litserve MCP compatibility with mcp>=1.1.0
# Completely disable LitServe's internal MCP to avoid conflicts with our standalone MCP Server
import litserve as ls
from litserve.connector import check_cuda_with_nvidia_smi
from utils import parse_list_arg

try:
Â  Â  # Patch LitServe's MCP module to disable it completely
Â  Â  import litserve.mcp as ls_mcp
Â  Â  import sys
Â  Â  from contextlib import asynccontextmanager

Â  Â  # Inject MCPServer (mcp.server.lowlevel.Server) as dummy
Â  Â  if not hasattr(ls_mcp, "MCPServer"):

Â  Â  Â  Â  class DummyMCPServer:
Â  Â  Â  Â  Â  Â  def __init__(self, *args, **kwargs):
Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  ls_mcp.MCPServer = DummyMCPServer
Â  Â  Â  Â  if "litserve.mcp" in sys.modules:
Â  Â  Â  Â  Â  Â  sys.modules["litserve.mcp"].MCPServer = DummyMCPServer

Â  Â  # Inject StreamableHTTPSessionManager as dummy
Â  Â  if not hasattr(ls_mcp, "StreamableHTTPSessionManager"):

Â  Â  Â  Â  class DummyStreamableHTTPSessionManager:
Â  Â  Â  Â  Â  Â  def __init__(self, *args, **kwargs):
Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  ls_mcp.StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager
Â  Â  Â  Â  if "litserve.mcp" in sys.modules:
Â  Â  Â  Â  Â  Â  sys.modules["litserve.mcp"].StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager

Â  Â  # Replace _LitMCPServerConnector with a complete dummy implementation
Â  Â  class DummyMCPConnector:
Â  Â  Â  Â  """å®Œå…¨ç¦ç”¨ LitServe å†…ç½® MCP çš„ Dummy å®ç°"""

Â  Â  Â  Â  def __init__(self, *args, **kwargs):
Â  Â  Â  Â  Â  Â  self.mcp_server = None
Â  Â  Â  Â  Â  Â  self.session_manager = None
Â  Â  Â  Â  Â  Â  self.request_handler = None

Â  Â  Â  Â  @asynccontextmanager
Â  Â  Â  Â  async def lifespan(self, app):
Â  Â  Â  Â  Â  Â  """ç©ºçš„ lifespan context managerï¼Œä¸åšä»»ä½•äº‹æƒ…"""
Â  Â  Â  Â  Â  Â  yieldÂ  # ä»€ä¹ˆéƒ½ä¸åšï¼Œç›´æ¥è®©æœåŠ¡å™¨å¯åŠ¨

Â  Â  Â  Â  def connect_mcp_server(self, *args, **kwargs):
Â  Â  Â  Â  Â  Â  """ç©ºçš„ connect_mcp_server æ–¹æ³•ï¼Œä¸åšä»»ä½•äº‹æƒ…"""
Â  Â  Â  Â  Â  Â  passÂ  # ä»€ä¹ˆéƒ½ä¸åšï¼Œè·³è¿‡ MCP åˆå§‹åŒ–

Â  Â  # æ›¿æ¢ _LitMCPServerConnector ç±»
Â  Â  ls_mcp._LitMCPServerConnector = DummyMCPConnector

Â  Â  # åŒæ—¶æ›´æ–° sys.modules ä¸­çš„å¼•ç”¨
Â  Â  if "litserve.mcp" in sys.modules:
Â  Â  Â  Â  sys.modules["litserve.mcp"]._LitMCPServerConnector = DummyMCPConnector

except Exception as e:
Â  Â  # If patching fails, log warning and continue
Â  Â  # The server might still work or fail with a clearer error message
Â  Â  import warnings

Â  Â  warnings.warn(f"Failed to patch litserve.mcp (MCP will be disabled): {e}")

from loguru import logger

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ MinerU
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from task_db import TaskDB
from output_normalizer import normalize_output

# å»¶è¿Ÿå¯¼å…¥ MinerUï¼Œé¿å…è¿‡æ—©åˆå§‹åŒ– CUDA
# MinerU ä¼šåœ¨ setup() è®¾ç½® CUDA_VISIBLE_DEVICES åå†å¯¼å…¥
# from mineru.cli.common import do_parse
# from mineru.utils.model_utils import get_vram, clean_memory

# å¯¼å…¥ importlib ç”¨äºæ£€æŸ¥æ¨¡å—å¯ç”¨æ€§
import importlib.util

# å°è¯•å¯¼å…¥ markitdown
try:
Â  Â  from markitdown import MarkItDown

Â  Â  MARKITDOWN_AVAILABLE = True
except ImportError:
Â  Â  MARKITDOWN_AVAILABLE = False
Â  Â  logger.warning("âš ï¸Â  markitdown not available, Office format parsing will be disabled")

# æ£€æŸ¥ PaddleOCR-VL æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
PADDLEOCR_VL_AVAILABLE = importlib.util.find_spec("paddleocr_vl") is not None
if PADDLEOCR_VL_AVAILABLE:
Â  Â  logger.info("âœ… PaddleOCR-VL engine available")
else:
Â  Â  logger.info("â„¹ï¸Â  PaddleOCR-VL not available (optional)")

# æ£€æŸ¥ PaddleOCR-VL-VLLM æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
PADDLEOCR_VL_VLLM_AVAILABLE = importlib.util.find_spec("paddleocr_vl_vllm") is not None
if PADDLEOCR_VL_VLLM_AVAILABLE:
Â  Â  logger.info("âœ… PaddleOCR-VL-VLLM engine available")
else:
Â  Â  logger.info("â„¹ï¸Â  PaddleOCR-VL-VLLM not available (optional)")

# æ£€æŸ¥ MinerU Pipeline æ˜¯å¦å¯ç”¨
MINERU_PIPELINE_AVAILABLE = importlib.util.find_spec("mineru_pipeline") is not None
if MINERU_PIPELINE_AVAILABLE:
Â  Â  logger.info("âœ… MinerU Pipeline engine available")
else:
Â  Â  logger.info("â„¹ï¸Â  MinerU Pipeline not available (optional)")

# å°è¯•å¯¼å…¥ SenseVoice éŸ³é¢‘å¤„ç†
SENSEVOICE_AVAILABLE = importlib.util.find_spec("audio_engines") is not None
if SENSEVOICE_AVAILABLE:
Â  Â  logger.info("âœ… SenseVoice audio engine available")
else:
Â  Â  logger.info("â„¹ï¸Â  SenseVoice not available (optional)")

# å°è¯•å¯¼å…¥è§†é¢‘å¤„ç†å¼•æ“
VIDEO_ENGINE_AVAILABLE = importlib.util.find_spec("video_engines") is not None
if VIDEO_ENGINE_AVAILABLE:
Â  Â  logger.info("âœ… Video processing engine available")
else:
Â  Â  logger.info("â„¹ï¸Â  Video processing engine not available (optional)")

# æ£€æŸ¥æ°´å°å»é™¤å¼•æ“æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
WATERMARK_REMOVAL_AVAILABLE = importlib.util.find_spec("remove_watermark") is not None
if WATERMARK_REMOVAL_AVAILABLE:
Â  Â  logger.info("âœ… Watermark removal engine available")
else:
Â  Â  logger.info("â„¹ï¸Â  Watermark removal engine not available (optional)")

# å°è¯•å¯¼å…¥æ ¼å¼å¼•æ“ï¼ˆä¸“ä¸šé¢†åŸŸæ ¼å¼æ”¯æŒï¼‰
try:
Â  Â  from format_engines import FormatEngineRegistry, FASTAEngine, GenBankEngine

Â  Â  # æ³¨å†Œæ‰€æœ‰å¼•æ“
Â  Â  FormatEngineRegistry.register(FASTAEngine())
Â  Â  FormatEngineRegistry.register(GenBankEngine())

Â  Â  FORMAT_ENGINES_AVAILABLE = True
Â  Â  logger.info("âœ… Format engines available")
Â  Â  logger.info(f"Â  Â Supported extensions: {', '.join(FormatEngineRegistry.get_supported_extensions())}")
except ImportError as e:
Â  Â  FORMAT_ENGINES_AVAILABLE = False
Â  Â  logger.info(f"â„¹ï¸Â  Format engines not available (optional): {e}")


# ==============================================================================
# VLLM Container Controller (äº’æ–¥åˆ‡æ¢ç‰ˆ + Pickle ä¿®å¤)
# ==============================================================================
class VLLMController:
Â  Â  """ç®¡ç† vLLM Docker å®¹å™¨çš„äº’æ–¥å¯åŠ¨"""
Â  Â Â 
Â  Â  def __init__(self):
Â  Â  Â  Â  # ä¸åœ¨ __init__ ä¸­åˆ›å»º clientï¼Œç¡®ä¿å¯¹è±¡æ˜¯å¯åºåˆ—åŒ–çš„ (Pickle Safe)
Â  Â  Â  Â  pass

Â  Â  def _get_client(self):
Â  Â  Â  Â  """æŒ‰éœ€è·å– Docker å®¢æˆ·ç«¯"""
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  import docker
Â  Â  Â  Â  Â  Â  # è¿æ¥åˆ°æŒ‚è½½çš„ /var/run/docker.sock
Â  Â  Â  Â  Â  Â  return docker.from_env()
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Docker client init failed: {e}")
Â  Â  Â  Â  Â  Â  return None

Â  Â  def ensure_service(self, target_container: str, conflict_container: str, health_url: str, timeout: int = 300):
Â  Â  Â  Â  """
Â  Â  Â  Â  ç¡®ä¿ç›®æ ‡å®¹å™¨è¿è¡Œï¼Œå¹¶å…³é—­å†²çªå®¹å™¨ (äº’æ–¥é€»è¾‘)
Â  Â  Â  Â Â 
Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  target_container: éœ€è¦è¿è¡Œçš„å®¹å™¨å
Â  Â  Â  Â  Â  Â  conflict_container: éœ€è¦å…³é—­çš„äº’æ–¥å®¹å™¨å
Â  Â  Â  Â  Â  Â  health_url: ç›®æ ‡å®¹å™¨çš„å¥åº·æ£€æŸ¥åœ°å€
Â  Â  Â  Â  Â  Â  timeout: è¶…æ—¶æ—¶é—´
Â  Â  Â  Â  """
Â  Â  Â  Â  client = self._get_client()
Â  Â  Â  Â  if not client:
Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # 1. æ£€æŸ¥ç›®æ ‡å®¹å™¨æ˜¯å¦å·²ç»åœ¨è¿è¡Œ
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  target = client.containers.get(target_container)
Â  Â  Â  Â  Â  Â  Â  Â  if target.status == 'running':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # å¦‚æœå·²ç»åœ¨è¿è¡Œï¼Œç›´æ¥è¿”å›ï¼Œæ— éœ€æ“ä½œ
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Target service {target_container} is already running.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  # å¦‚æœæ‰¾ä¸åˆ°å®¹å™¨ï¼Œè¯´æ˜æ²¡åˆ›å»ºï¼Œæç¤ºç”¨æˆ·
Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Container {target_container} not found. Please ensure it is created (e.g. docker compose up --no-start).")
Â  Â  Â  Â  Â  Â  Â  Â  raise e

Â  Â  Â  Â  Â  Â  # 2. åœæ­¢å†²çªå®¹å™¨ (é‡Šæ”¾æ˜¾å­˜)
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  conflict = client.containers.get(conflict_container)
Â  Â  Â  Â  Â  Â  Â  Â  if conflict.status == 'running':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ›‘ Stopping conflicting service {conflict_container} to free VRAM...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conflict.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Service {conflict_container} stopped.")
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  # å†²çªå®¹å™¨å¯èƒ½ä¸å­˜åœ¨æˆ–å·²åœæ­¢ï¼Œå¿½ç•¥
Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  Â  Â  # 3. å¯åŠ¨ç›®æ ‡å®¹å™¨
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸš€ Starting service {target_container} (Cold Start)...")
Â  Â  Â  Â  Â  Â  target.start()

Â  Â  Â  Â  Â  Â  # 4. ç­‰å¾…å¥åº·æ£€æŸ¥
Â  Â  Â  Â  Â  Â  self._wait_for_health(health_url, timeout)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  finally:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  client.close()
Â  Â  Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  def _wait_for_health(self, url: str, timeout: int):
Â  Â  Â  Â  """è½®è¯¢å¥åº·æ£€æŸ¥æ¥å£"""
Â  Â  Â  Â  start_time = time.time()
Â  Â  Â  Â  logger.info(f"â³ Waiting for service at {url} (timeout: {timeout}s)...")
Â  Â  Â  Â Â 
Â  Â  Â  Â  while time.time() - start_time < timeout:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # æ˜¾å¼ä½¿ç”¨ host.docker.internal æˆ–è€…æ˜¯ Docker DNS å
Â  Â  Â  Â  Â  Â  Â  Â  response = requests.get(url, timeout=2)
Â  Â  Â  Â  Â  Â  Â  Â  if response.status_code == 200:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Service is ready: {url}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  pass
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  time.sleep(2) # æ¯2ç§’é‡è¯•ä¸€æ¬¡
Â  Â  Â  Â Â 
Â  Â  Â  Â  raise TimeoutError(f"Service at {url} did not become ready in {timeout} seconds")


class MinerUWorkerAPI(ls.LitAPI):
Â  Â  def __init__(
Â  Â  Â  Â  self,
Â  Â  Â  Â  paddleocr_vl_vllm_api_list=None,
Â  Â  Â  Â  mineru_vllm_api_list=None,
Â  Â  Â  Â  output_dir=None,
Â  Â  Â  Â  poll_interval=0.5,
Â  Â  Â  Â  enable_worker_loop=True,
Â  Â  Â  Â  paddleocr_vl_vllm_engine_enabled=False,
Â  Â  ):
Â  Â  Â  Â  """
Â  Â  Â  Â  åˆå§‹åŒ– APIï¼šç›´æ¥åœ¨è¿™é‡Œæ¥æ”¶æ‰€æœ‰éœ€è¦çš„å‚æ•°
Â  Â  Â  Â  """
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  # è·å–é¡¹ç›®æ ¹ç›®å½•
Â  Â  Â  Â  project_root = Path(__file__).parent.parent
Â  Â  Â  Â  default_output = project_root / "data" / "output"
Â  Â  Â  Â  self.output_dir = output_dir or os.getenv("OUTPUT_PATH", str(default_output))
Â  Â  Â  Â  self.poll_interval = poll_interval
Â  Â  Â  Â  self.enable_worker_loop = enable_worker_loop
Â  Â  Â  Â  self.paddleocr_vl_vllm_engine_enabled = paddleocr_vl_vllm_engine_enabled
Â  Â  Â  Â  self.paddleocr_vl_vllm_api_list = paddleocr_vl_vllm_api_list or []
Â  Â  Â  Â  self.mineru_vllm_api_list = mineru_vllm_api_list or []Â  # ä¿å­˜ MinerU API åˆ—è¡¨
Â  Â  Â  Â Â 
Â  Â  Â  Â  ctx = multiprocessing.get_context("spawn")
Â  Â  Â  Â  self._global_worker_counter = ctx.Value("i", 0)

Â  Â  Â  Â  # ã€å…³é”®ä¿®æ”¹ã€‘åœ¨ __init__ ä¸­ç›´æ¥åˆå§‹åŒ– VLLMController
Â  Â  Â  Â  # å› ä¸ºç°åœ¨çš„ VLLMController ä¸æŒæœ‰ä¸å¯åºåˆ—åŒ–çš„ client å¯¹è±¡ï¼Œæ‰€ä»¥æ˜¯å®‰å…¨çš„
Â  Â  Â  Â  self.vllm_controller = VLLMController()

Â  Â  def setup(self, device):
Â  Â  Â  Â  """
Â  Â  Â  Â  åˆå§‹åŒ– Worker (æ¯ä¸ª GPU ä¸Šè°ƒç”¨ä¸€æ¬¡)

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  device: è®¾å¤‡ ID (cuda:0, cuda:1, cpu ç­‰)
Â  Â  Â  Â  """
Â  Â  Â  Â  ## é…ç½®æ¯ä¸ª Worker çš„å…¨å±€ç´¢å¼•å¹¶å°è¯•æ€§åˆ†é… API
Â  Â  Â  Â  with self._global_worker_counter.get_lock():
Â  Â  Â  Â  Â  Â  my_global_index = self._global_worker_counter.value
Â  Â  Â  Â  Â  Â  self._global_worker_counter.value += 1
Â  Â  Â  Â  logger.info(f"ğŸ”¢ [Init] I am Global Worker #{my_global_index} (on {device})")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. åˆ†é… PaddleOCR VLLM API
Â  Â  Â  Â  if self.paddleocr_vl_vllm_engine_enabled and len(self.paddleocr_vl_vllm_api_list) > 0:
Â  Â  Â  Â  Â  Â  assigned_api = self.paddleocr_vl_vllm_api_list[my_global_index % len(self.paddleocr_vl_vllm_api_list)]
Â  Â  Â  Â  Â  Â  self.paddleocr_vl_vllm_api = assigned_api
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: {assigned_api}")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  self.paddleocr_vl_vllm_api = None
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: None")

Â  Â  Â  Â  # 2. åˆ†é… MinerU VLLM API (æ–°å¢)
Â  Â  Â  Â  if len(self.mineru_vllm_api_list) > 0:
Â  Â  Â  Â  Â  Â  assigned_mineru_api = self.mineru_vllm_api_list[my_global_index % len(self.mineru_vllm_api_list)]
Â  Â  Â  Â  Â  Â  self.mineru_vllm_api = assigned_mineru_api
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”§ Worker #{my_global_index} assigned MinerU VLLM API: {assigned_mineru_api}")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  self.mineru_vllm_api = None
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”§ Worker #{my_global_index} assigned MinerU VLLM API: None")

Â  Â  Â  Â  # ============================================================================
Â  Â  Â  Â  # ã€å…³é”®ã€‘ç¬¬ä¸€æ­¥ï¼šç«‹å³è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆå¿…é¡»åœ¨ä»»ä½•å¯¼å…¥ä¹‹å‰ï¼‰
Â  Â  Â  Â  # ============================================================================
Â  Â  Â  Â  # LitServe ä¸ºæ¯ä¸ª worker è¿›ç¨‹åˆ†é…ä¸åŒçš„ device (cuda:0, cuda:1, ...)
Â  Â  Â  Â  # æˆ‘ä»¬éœ€è¦åœ¨å¯¼å…¥ä»»ä½• CUDA åº“ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå®ç°è¿›ç¨‹çº§ GPU éš”ç¦»
Â  Â  Â  Â  if "cuda:" in str(device):
Â  Â  Â  Â  Â  Â  gpu_id = str(device).split(":")[-1]
Â  Â  Â  Â  Â  Â  os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
Â  Â  Â  Â  Â  Â  # ã€å…³é”®ã€‘è®¾ç½® MinerU çš„è®¾å¤‡æ¨¡å¼ä¸º cuda:0
Â  Â  Â  Â  Â  Â  # å› ä¸ºè®¾ç½®äº† CUDA_VISIBLE_DEVICES åï¼Œè¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€å¼ å¡ï¼ˆé€»è¾‘ ID å˜ä¸º 0ï¼‰
Â  Â  Â  Â  Â  Â  os.environ["MINERU_DEVICE_MODE"] = "cuda:0"
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ¯ [GPU Isolation] Set CUDA_VISIBLE_DEVICES={gpu_id} (Physical GPU {gpu_id} â†’ Logical GPU 0)")
Â  Â  Â  Â  Â  Â  logger.info("ğŸ¯ [GPU Isolation] Set MINERU_DEVICE_MODE=cuda:0")

Â  Â  Â  Â  import socket

Â  Â  Â  Â  # é…ç½®æ¨¡å‹ä¸‹è½½æºï¼ˆå¿…é¡»åœ¨ MinerU åˆå§‹åŒ–ä¹‹å‰ï¼‰
Â  Â  Â  Â  # ä»ç¯å¢ƒå˜é‡ MODEL_DOWNLOAD_SOURCE è¯»å–é…ç½®
Â  Â  Â  Â  # æ”¯æŒ: modelscope, huggingface, auto (é»˜è®¤)
Â  Â  Â  Â  model_source = os.getenv("MODEL_DOWNLOAD_SOURCE", "auto").lower()

Â  Â  Â  Â  if model_source in ["modelscope", "auto"]:
Â  Â  Â  Â  Â  Â  # å°è¯•ä½¿ç”¨ ModelScopeï¼ˆä¼˜å…ˆï¼‰
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  import importlib.util

Â  Â  Â  Â  Â  Â  Â  Â  if importlib.util.find_spec("modelscope") is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("ğŸ“¦ Model download source: ModelScope (å›½å†…æ¨è)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("Â  Â Note: ModelScope automatically uses China mirror for faster downloads")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ImportError("modelscope not found")
Â  Â  Â  Â  Â  Â  except ImportError:
Â  Â  Â  Â  Â  Â  Â  Â  if model_source == "modelscope":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸Â  ModelScope not available, falling back to HuggingFace")
Â  Â  Â  Â  Â  Â  Â  Â  model_source = "huggingface"

Â  Â  Â  Â  if model_source == "huggingface":
Â  Â  Â  Â  Â  Â  # é…ç½® HuggingFace é•œåƒï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ä½¿ç”¨å›½å†…é•œåƒï¼‰
Â  Â  Â  Â  Â  Â  hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
Â  Â  Â  Â  Â  Â  os.environ.setdefault("HF_ENDPOINT", hf_endpoint)
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“¦ Model download source: HuggingFace (via: {hf_endpoint})")
Â  Â  Â  Â  elif model_source == "modelscope":
Â  Â  Â  Â  Â  Â  ## é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®,æ¥è®©æ¨¡å‹ä»modelscopeå¹³å°ä¸‹è½½, æˆ–è€…ä»modelscopeçš„ç¼“å­˜ç›®å½•åŠ è½½
Â  Â  Â  Â  Â  Â  os.environ["MINERU_MODEL_SOURCE"] = "modelscope"
Â  Â  Â  Â  Â  Â  logger.info("ğŸ“¦ Model download source: ModelScope")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Unknown model download source: {model_source}")

Â  Â  Â  Â  self.device = device
Â  Â  Â  Â  # ä¿å­˜ accelerator ç±»å‹ï¼ˆä» device å­—ç¬¦ä¸²æ¨æ–­ï¼‰
Â  Â  Â  Â  # device å¯èƒ½æ˜¯ "cuda:0", "cuda:1", "cpu" ç­‰
Â  Â  Â  Â  if "cuda" in str(device):
Â  Â  Â  Â  Â  Â  self.accelerator = "cuda"
Â  Â  Â  Â  Â  Â  self.engine_device = "cuda:0"Â  # å¼•æ“ç»Ÿä¸€ä½¿ç”¨ cuda:0ï¼ˆå› ä¸ºå·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼‰
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  self.accelerator = "cpu"
Â  Â  Â  Â  Â  Â  self.engine_device = "cpu"Â  # CPU æ¨¡å¼

Â  Â  Â  Â  logger.info(f"ğŸ¯ [Device] Accelerator: {self.accelerator}, Engine Device: {self.engine_device}")

Â  Â  Â  Â  # ä»ç±»å±æ€§è·å–é…ç½®ï¼ˆç”± start_litserve_workers è®¾ç½®ï¼‰
Â  Â  Â  Â  # é»˜è®¤ä½¿ç”¨å…±äº«è¾“å‡ºç›®å½•ï¼ˆDocker ç¯å¢ƒï¼‰
Â  Â  Â  Â  project_root = Path(__file__).parent.parent
Â  Â  Â  Â  default_output_path = project_root / "data" / "output"
Â  Â  Â  Â  default_output = os.getenv("OUTPUT_PATH", str(default_output_path))
Â  Â  Â  Â  self.output_dir = getattr(self.__class__, "_output_dir", default_output)
Â  Â  Â  Â  self.poll_interval = getattr(self.__class__, "_poll_interval", 0.5)
Â  Â  Â  Â  self.enable_worker_loop = getattr(self.__class__, "_enable_worker_loop", True)

Â  Â  Â  Â  # ============================================================================
Â  Â  Â  Â  # ç¬¬äºŒæ­¥ï¼šç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥ MinerU äº†ï¼ˆCUDA_VISIBLE_DEVICES å·²è®¾ç½®ï¼‰
Â  Â  Â  Â  # ============================================================================
Â  Â  Â  Â  global get_vram, clean_memory
Â  Â  Â  Â  from mineru.utils.model_utils import get_vram, clean_memory

Â  Â  Â  Â  # é…ç½® MinerU çš„ VRAM è®¾ç½®
Â  Â  Â  Â  if os.getenv("MINERU_VIRTUAL_VRAM_SIZE", None) is None:
Â  Â  Â  Â  Â  Â  device_mode = os.environ.get("MINERU_DEVICE_MODE", str(device))
Â  Â  Â  Â  Â  Â  if device_mode.startswith("cuda") or device_mode.startswith("npu"):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šget_vram éœ€è¦ä¼ å…¥è®¾å¤‡å­—ç¬¦ä¸²ï¼ˆå¦‚ "cuda:0"ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  vram = round(get_vram(device_mode))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = str(vram)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ® [MinerU VRAM] Detected: {vram}GB")
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "8"Â  # é»˜è®¤å€¼
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to detect VRAM, using default: 8GB ({e})")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "1"
Â  Â  Â  Â  Â  Â  Â  Â  logger.info("ğŸ® [MinerU VRAM] CPU mode, set to 1GB")

Â  Â  Â  Â  # éªŒè¯ PyTorch CUDA è®¾ç½®
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  import torch

Â  Â  Â  Â  Â  Â  if torch.cuda.is_available():
Â  Â  Â  Â  Â  Â  Â  Â  visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "all")
Â  Â  Â  Â  Â  Â  Â  Â  device_count = torch.cuda.device_count()
Â  Â  Â  Â  Â  Â  Â  Â  logger.info("âœ… PyTorch CUDA verified:")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Â  Â CUDA_VISIBLE_DEVICES = {visible_devices}")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Â  Â torch.cuda.device_count() = {device_count}")
Â  Â  Â  Â  Â  Â  Â  Â  if device_count == 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Â  Â âœ… SUCCESS: Process isolated to 1 GPU (physical GPU {visible_devices})")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Â  Â âš ï¸Â  WARNING: Expected 1 GPU but found {device_count}")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸Â  CUDA not available")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to verify PyTorch CUDA: {e}")

Â  Â  Â  Â  # åˆ›å»ºè¾“å‡ºç›®å½•
Â  Â  Â  Â  Path(self.output_dir).mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # åˆå§‹åŒ–ä»»åŠ¡æ•°æ®åº“ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå…¼å®¹ Docker å’Œæœ¬åœ°ï¼‰
Â  Â  Â  Â  db_path_env = os.getenv("DATABASE_PATH")
Â  Â  Â  Â  if db_path_env:
Â  Â  Â  Â  Â  Â  db_path = Path(db_path_env).resolve()Â  # ä½¿ç”¨ resolve() è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Using DATABASE_PATH from environment: {db_path_env} -> {db_path}")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # é»˜è®¤è·¯å¾„ï¼ˆä¸ TaskDB å’Œ AuthDB ä¿æŒä¸€è‡´ï¼‰
Â  Â  Â  Â  Â  Â  project_root = Path(__file__).parent.parent
Â  Â  Â  Â  Â  Â  default_db = project_root / "data" / "db" / "mineru_tianshu.db"
Â  Â  Â  Â  Â  Â  db_path = default_db.resolve()
Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  DATABASE_PATH not set, using default: {db_path}")

Â  Â  Â  Â  # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
Â  Â  Â  Â  db_path.parent.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # ä½¿ç”¨ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²ä¼ é€’ç»™ TaskDB
Â  Â  Â  Â  db_path_str = str(db_path.absolute())
Â  Â  Â  Â  logger.info(f"ğŸ“Š Database path (absolute): {db_path_str}")

Â  Â  Â  Â  self.task_db = TaskDB(db_path_str)

Â  Â  Â  Â  # éªŒè¯æ•°æ®åº“è¿æ¥å¹¶è¾“å‡ºåˆå§‹ç»Ÿè®¡
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  stats = self.task_db.get_queue_stats()
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Database initialized: {db_path} (exists: {db_path.exists()})")
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š TaskDB.db_path: {self.task_db.db_path}")
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Initial queue stats: {stats}")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to initialize database or get stats: {e}")
Â  Â  Â  Â  Â  Â  logger.exception(e)

Â  Â  Â  Â  # Worker çŠ¶æ€
Â  Â  Â  Â  self.running = True
Â  Â  Â  Â  self.current_task_id = None

Â  Â  Â  Â  # ç”Ÿæˆå”¯ä¸€çš„ worker_id: tianshu-{hostname}-{device}-{pid}
Â  Â  Â  Â  hostname = socket.gethostname()
Â  Â  Â  Â  pid = os.getpid()
Â  Â  Â  Â  self.worker_id = f"tianshu-{hostname}-{device}-{pid}"
Â  Â  Â  Â  # å­è¿›ç¨‹ï¼ˆsetup ä¸­ï¼‰ï¼š

Â  Â  Â  Â  # åˆå§‹åŒ–å¯é€‰çš„å¤„ç†å¼•æ“
Â  Â  Â  Â  self.markitdown = MarkItDown() if MARKITDOWN_AVAILABLE else None
Â  Â  Â  Â  self.mineru_pipeline_engine = NoneÂ  # å»¶è¿ŸåŠ è½½
Â  Â  Â  Â  self.paddleocr_vl_engine = NoneÂ  # å»¶è¿ŸåŠ è½½
Â  Â  Â  Â  self.paddleocr_vl_vllm_engine = NoneÂ  # å»¶è¿ŸåŠ è½½
Â  Â  Â  Â  self.sensevoice_engine = NoneÂ  # å»¶è¿ŸåŠ è½½
Â  Â  Â  Â  self.video_engine = NoneÂ  # å»¶è¿ŸåŠ è½½
Â  Â  Â  Â  self.watermark_handler = NoneÂ  # å»¶è¿ŸåŠ è½½

Â  Â  Â  Â  logger.info("=" * 60)
Â  Â  Â  Â  logger.info(f"ğŸš€ Worker Setup: {self.worker_id}")
Â  Â  Â  Â  logger.info("=" * 60)
Â  Â  Â  Â  logger.info(f"ğŸ“ Device: {device}")
Â  Â  Â  Â  logger.info(f"ğŸ“‚ Output Dir: {self.output_dir}")
Â  Â  Â  Â  logger.info(f"ğŸ—ƒï¸Â  Database: {db_path}")
Â  Â  Â  Â  logger.info(f"ğŸ”„ Worker Loop: {'Enabled' if self.enable_worker_loop else 'Disabled'}")
Â  Â  Â  Â  if self.enable_worker_loop:
Â  Â  Â  Â  Â  Â  logger.info(f"â±ï¸Â  Poll Interval: {self.poll_interval}s")
Â  Â  Â  Â  logger.info("")

Â  Â  Â  Â  # æ‰“å°å¯ç”¨çš„å¼•æ“
Â  Â  Â  Â  logger.info("ğŸ“¦ Available Engines:")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ MarkItDown: {'âœ…' if MARKITDOWN_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ MinerU Pipeline: {'âœ…' if MINERU_PIPELINE_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ PaddleOCR-VL: {'âœ…' if PADDLEOCR_VL_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ SenseVoice: {'âœ…' if SENSEVOICE_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ Video Engine: {'âœ…' if VIDEO_ENGINE_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ Watermark Removal: {'âœ…' if WATERMARK_REMOVAL_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info(f"Â  Â â€¢ Format Engines: {'âœ…' if FORMAT_ENGINES_AVAILABLE else 'âŒ'}")
Â  Â  Â  Â  logger.info("")

Â  Â  Â  Â  # æ£€æµ‹å’Œåˆå§‹åŒ–æ°´å°å»é™¤å¼•æ“ï¼ˆä»… CUDAï¼‰
Â  Â  Â  Â  if WATERMARK_REMOVAL_AVAILABLE and "cuda" in str(device).lower():
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  logger.info("ğŸ¨ Initializing watermark removal engine...")
Â  Â  Â  Â  Â  Â  Â  Â  # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿åœ¨ CUDA_VISIBLE_DEVICES è®¾ç½®ä¹‹å
Â  Â  Â  Â  Â  Â  Â  Â  from remove_watermark.pdf_watermark_handler import PDFWatermarkHandler

Â  Â  Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
Â  Â  Â  Â  Â  Â  Â  Â  # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  self.watermark_handler = PDFWatermarkHandler(device="cuda:0", use_lama=True)
Â  Â  Â  Â  Â  Â  Â  Â  gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Watermark removal engine initialized on cuda:0 (physical GPU {gpu_id})")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to initialize watermark removal engine: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  self.watermark_handler = None

Â  Â  Â  Â  logger.info("âœ… Worker ready")
Â  Â  Â  Â  logger.info(f"Â  Â LitServe Device: {device}")
Â  Â  Â  Â  logger.info(f"Â  Â MinerU Device Mode: {os.environ.get('MINERU_DEVICE_MODE', 'auto')}")
Â  Â  Â  Â  logger.info(f"Â  Â MinerU VRAM: {os.environ.get('MINERU_VIRTUAL_VRAM_SIZE', 'unknown')}GB")
Â  Â  Â  Â  if "cuda" in str(device).lower():
Â  Â  Â  Â  Â  Â  physical_gpu = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  logger.info(f"Â  Â Physical GPU: {physical_gpu}")

Â  Â  Â  Â  # å¦‚æœå¯ç”¨äº† worker å¾ªç¯ï¼Œå¯åŠ¨åå°çº¿ç¨‹æ‹‰å–ä»»åŠ¡
Â  Â  Â  Â  if self.enable_worker_loop:
Â  Â  Â  Â  Â  Â  self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
Â  Â  Â  Â  Â  Â  self.worker_thread.start()
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”„ Worker loop started (poll_interval={self.poll_interval}s)")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  logger.info("â¸ï¸Â  Worker loop disabled, waiting for manual triggers")

Â  Â  def _worker_loop(self):
Â  Â  Â  Â  """
Â  Â  Â  Â  Worker åå°å¾ªç¯ï¼šæŒç»­æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†

Â  Â  Â  Â  è¿™ä¸ªå¾ªç¯åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä¸æ–­æ£€æŸ¥æ˜¯å¦æœ‰æ–°ä»»åŠ¡
Â  Â  Â  Â  ä¸€æ—¦æœ‰ä»»åŠ¡ï¼Œç«‹å³å¤„ç†ï¼Œå¤„ç†å®Œæˆåç»§ç»­å¾ªç¯
Â  Â  Â  Â  """
Â  Â  Â  Â  logger.info(f"ğŸ” {self.worker_id} started task polling loop")

Â  Â  Â  Â  # è®°å½•åˆå§‹è¯Šæ–­ä¿¡æ¯
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  stats = self.task_db.get_queue_stats()
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Initial queue stats: {stats}")
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ—ƒï¸Â  Database path: {self.task_db.db_path}")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to get initial queue stats: {e}")

Â  Â  Â  Â  loop_count = 0
Â  Â  Â  Â  last_stats_log = 0
Â  Â  Â  Â  stats_log_interval = 20Â  # æ¯20æ¬¡å¾ªç¯è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆçº¦10ç§’ï¼‰

Â  Â  Â  Â  while self.running:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  loop_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  # æ‹‰å–ä»»åŠ¡ï¼ˆåŸå­æ“ä½œï¼Œé˜²æ­¢é‡å¤å¤„ç†ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  task = self.task_db.get_next_task(worker_id=self.worker_id)

Â  Â  Â  Â  Â  Â  Â  Â  if task:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  task_id = task["task_id"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.current_task_id = task_id
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"ğŸ“¥ {self.worker_id} pulled task: {task_id} (file: {task.get('file_name', 'unknown')})"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # å¤„ç†ä»»åŠ¡
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self._process_task(task)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… {self.worker_id} completed task: {task_id}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ {self.worker_id} failed task {task_id}: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.exception(e)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  finally:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.current_task_id = None
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ²¡æœ‰ä»»åŠ¡ï¼Œç©ºé—²ç­‰å¾…
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ä»¥ä¾¿è¯Šæ–­
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if loop_count - last_stats_log >= stats_log_interval:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stats = self.task_db.get_queue_stats()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pending = stats.get("pending", 0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing = stats.get("processing", 0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pending > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"âš ï¸Â  {self.worker_id} polling (loop #{loop_count}): "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{pending} pending tasks found but not pulled! "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Processing: {processing}, Completed: {stats.get('completed', 0)}, "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Failed: {stats.get('failed', 0)}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif loop_count % 100 == 0:Â  # æ¯50ç§’ï¼ˆ100æ¬¡å¾ªç¯ï¼‰è¾“å‡ºä¸€æ¬¡
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"ğŸ’¤ {self.worker_id} idle (loop #{loop_count}): "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"No pending tasks. Queue stats: {stats}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to get queue stats: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  last_stats_log = loop_count

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(self.poll_interval)

Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Worker loop error (loop #{loop_count}): {e}")
Â  Â  Â  Â  Â  Â  Â  Â  logger.exception(e)
Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(self.poll_interval)

Â  Â  def _process_task(self, task: dict):
Â  Â  Â  Â  """
Â  Â  Â  Â  å¤„ç†å•ä¸ªä»»åŠ¡ (é›†æˆäº’æ–¥å¯åŠ¨é€»è¾‘)

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  task: ä»»åŠ¡å­—å…¸ï¼ˆä»æ•°æ®åº“æ‹‰å–ï¼‰
Â  Â  Â  Â  """
Â  Â  Â  Â  task_id = task["task_id"]
Â  Â  Â  Â  file_path = task["file_path"]
Â  Â  Â  Â  options = json.loads(task.get("options", "{}"))
Â  Â  Â  Â  parent_task_id = task.get("parent_task_id")
Â  Â  Â  Â  backend = task.get("backend", "auto")
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # 1. æ™ºèƒ½æœåŠ¡åˆ‡æ¢é€»è¾‘
Â  Â  Â  Â  Â  Â  paddle_container = "tianshu-vllm-paddleocr"
Â  Â  Â  Â  Â  Â  mineru_container = "tianshu-vllm-mineru"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # å¦‚æœæ˜¯ PaddleOCR ä»»åŠ¡
Â  Â  Â  Â  Â  Â  if backend == "paddleocr-vl-vllm" and self.paddleocr_vl_vllm_api:
Â  Â  Â  Â  Â  Â  Â  Â  base = self.paddleocr_vl_vllm_api.replace("/v1", "")
Â  Â  Â  Â  Â  Â  Â  Â  health = f"{base}/health"
Â  Â  Â  Â  Â  Â  Â  Â  # ç¡®ä¿ Paddle è¿è¡Œï¼Œå…³é—­ MinerU
Â  Â  Â  Â  Â  Â  Â  Â  self.vllm_controller.ensure_service(paddle_container, mineru_container, health)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # å¦‚æœæ˜¯ MinerU ä»»åŠ¡ (vlm/hybrid local æ¨¡å¼)
Â  Â  Â  Â  Â  Â  # æ³¨æ„: remote client æ¨¡å¼ä¸éœ€è¦å¯åŠ¨æœ¬åœ°å®¹å™¨
Â  Â  Â  Â  Â  Â  elif backend in ["vlm-auto-engine", "hybrid-auto-engine"] and self.mineru_vllm_api:
Â  Â  Â  Â  Â  Â  Â  Â  base = self.mineru_vllm_api.replace("/v1", "")
Â  Â  Â  Â  Â  Â  Â  Â  health = f"{base}/health"
Â  Â  Â  Â  Â  Â  Â  Â  # ç¡®ä¿ MinerU è¿è¡Œï¼Œå…³é—­ Paddle
Â  Â  Â  Â  Â  Â  Â  Â  self.vllm_controller.ensure_service(mineru_container, paddle_container, health)

Â  Â  Â  Â  Â  Â  # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
Â  Â  Â  Â  Â  Â  file_ext = Path(file_path).suffix.lower()

Â  Â  Â  Â  Â  Â  # ã€æ–°å¢ã€‘Office è½¬ PDF é¢„å¤„ç†
Â  Â  Â  Â  Â  Â  office_extensions = [".docx", ".xlsx", ".pptx", ".doc", ".xls", ".ppt"]
Â  Â  Â  Â  Â  Â  if file_ext in office_extensions and options.get("convert_office_to_pdf", False):
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“„ [Preprocessing] Converting Office to PDF: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pdf_path = self._convert_office_to_pdf(file_path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ›´æ–°æ–‡ä»¶è·¯å¾„å’Œæ‰©å±•å
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_file_path = file_path
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_path = pdf_path
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_ext = ".pdf"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… [Preprocessing] Office converted, continuing with PDF: {pdf_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Â  Â Original: {Path(original_file_path).name}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Â  Â Converted: {Path(pdf_path).name}")

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸ [Preprocessing] Office to PDF conversion failed: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Â  Â Falling back to MarkItDown for: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # è½¬æ¢å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ–‡ä»¶ï¼ˆMarkItDown å¤„ç†ï¼‰

Â  Â  Â  Â  Â  Â  # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†åˆ† PDFï¼ˆä»…å¯¹éå­ä»»åŠ¡çš„ PDF è¿›è¡Œåˆ¤æ–­ï¼‰
Â  Â  Â  Â  Â  Â  if file_ext == ".pdf" and not parent_task_id:
Â  Â  Â  Â  Â  Â  Â  Â  if self._should_split_pdf(task_id, file_path, task, options):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # PDF å·²è¢«æ‹†åˆ†ï¼Œå½“å‰ä»»åŠ¡å·²è½¬ä¸ºçˆ¶ä»»åŠ¡ï¼Œç›´æ¥è¿”å›
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  Â  Â  # 0. å¯é€‰ï¼šé¢„å¤„ç† - å»é™¤æ°´å°ï¼ˆä»… PDFï¼Œä½œä¸ºé¢„å¤„ç†æ­¥éª¤ï¼‰
Â  Â  Â  Â  Â  Â  if file_ext == ".pdf" and options.get("remove_watermark", False) and self.watermark_handler:
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ¨ [Preprocessing] Removing watermark from PDF: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cleaned_pdf_path = self._preprocess_remove_watermark(file_path, options)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_path = str(cleaned_pdf_path)Â  # ä½¿ç”¨å»æ°´å°åçš„æ–‡ä»¶ç»§ç»­å¤„ç†
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… [Preprocessing] Watermark removed, continuing with: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸ [Preprocessing] Watermark removal failed: {e}, continuing with original file")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ç»§ç»­ä½¿ç”¨åŸæ–‡ä»¶å¤„ç†

Â  Â  Â  Â  Â  Â  # ç»Ÿä¸€çš„å¼•æ“è·¯ç”±é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ backendï¼Œå¦åˆ™è‡ªåŠ¨é€‰æ‹©
Â  Â  Â  Â  Â  Â  result = NoneÂ  # åˆå§‹åŒ– result

Â  Â  Â  Â  Â  Â  # 1. ç”¨æˆ·æŒ‡å®šäº†éŸ³é¢‘å¼•æ“
Â  Â  Â  Â  Â  Â  if backend == "sensevoice":
Â  Â  Â  Â  Â  Â  Â  Â  if not SENSEVOICE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError("SenseVoice engine is not available")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ¤ Processing with SenseVoice: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_audio(file_path, options)

Â  Â  Â  Â  Â  Â  # 3. ç”¨æˆ·æŒ‡å®šäº†è§†é¢‘å¼•æ“
Â  Â  Â  Â  Â  Â  elif backend == "video":
Â  Â  Â  Â  Â  Â  Â  Â  if not VIDEO_ENGINE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError("Video processing engine is not available")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ¬ Processing with video engine: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_video(file_path, options)

Â  Â  Â  Â  Â  Â  # 4. ç”¨æˆ·æŒ‡å®šäº† PaddleOCR-VL
Â  Â  Â  Â  Â  Â  elif backend == "paddleocr-vl":
Â  Â  Â  Â  Â  Â  Â  Â  if not PADDLEOCR_VL_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError("PaddleOCR-VL engine is not available")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ” Processing with PaddleOCR-VL: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_paddleocr_vl(file_path, options)

Â  Â  Â  Â  Â  Â  # 5. ç”¨æˆ·æŒ‡å®šäº† PaddleOCR-VL-VLLM
Â  Â  Â  Â  Â  Â  elif backend == "paddleocr-vl-vllm":
Â  Â  Â  Â  Â  Â  Â  Â  if (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  not PADDLEOCR_VL_VLLM_AVAILABLE
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  or not self.paddleocr_vl_vllm_engine_enabled
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  or len(self.paddleocr_vl_vllm_api_list) == 0
Â  Â  Â  Â  Â  Â  Â  Â  ):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError("PaddleOCR-VL-VLLM engine is not available")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ” Processing with PaddleOCR-VL-VLLM: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_paddleocr_vl_vllm(file_path, options)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 6. ç”¨æˆ·æŒ‡å®šäº† MinerU çš„æŸç§æ¨¡å¼ (pipeline, vlm-*, hybrid-*)
Â  Â  Â  Â  Â  Â  elif "pipeline" in backend or "vlm-" in backend or "hybrid-" in backend:
Â  Â  Â  Â  Â  Â  Â  Â  if not MINERU_PIPELINE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"MinerU Pipeline engine is not available, cannot run {backend}")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”§ Processing with MinerU ({backend}): {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # å°† backend æ¨¡å¼å†™å…¥ optionsï¼Œä¼ é€’ç»™ Engine
Â  Â  Â  Â  Â  Â  Â  Â  options["parse_mode"] = backendÂ  # ã€å…³é”®ã€‘ç¡®ä¿ parse_mode æ­£ç¡®ä¼ é€’
Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_mineru(file_path, options)

Â  Â  Â  Â  Â  Â  # 7. auto æ¨¡å¼ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©å¼•æ“
Â  Â  Â  Â  Â  Â  elif backend == "auto":
Â  Â  Â  Â  Â  Â  Â  Â  # 7.1 æ£€æŸ¥æ˜¯å¦æ˜¯ä¸“ä¸šæ ¼å¼ï¼ˆFASTA, GenBank ç­‰ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  if FORMAT_ENGINES_AVAILABLE and FormatEngineRegistry.is_supported(file_path):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ§¬ [Auto] Processing with format engine: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_format_engine(file_path, options)

Â  Â  Â  Â  Â  Â  Â  Â  # 7.2 æ£€æŸ¥æ˜¯å¦æ˜¯éŸ³é¢‘æ–‡ä»¶
Â  Â  Â  Â  Â  Â  Â  Â  elif file_ext in [".wav", ".mp3", ".flac", ".m4a", ".ogg"] and SENSEVOICE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ¤ [Auto] Processing audio file: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_audio(file_path, options)

Â  Â  Â  Â  Â  Â  Â  Â  # 7.3 æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘æ–‡ä»¶
Â  Â  Â  Â  Â  Â  Â  Â  elif file_ext in [".mp4", ".avi", ".mkv", ".mov", ".flv", ".wmv"] and VIDEO_ENGINE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ¬ [Auto] Processing video file: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_video(file_path, options)

Â  Â  Â  Â  Â  Â  Â  Â  # 7.4 é»˜è®¤ä½¿ç”¨ MinerU Pipeline å¤„ç† PDF/å›¾ç‰‡
Â  Â  Â  Â  Â  Â  Â  Â  elif file_ext in [".pdf", ".png", ".jpg", ".jpeg"] and MINERU_PIPELINE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”§ [Auto] Processing with MinerU Pipeline (Default): {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # é»˜è®¤ä½¿ç”¨ pipeline æ¨¡å¼
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  options["parse_mode"] = "pipeline"Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_mineru(file_path, options)

Â  Â  Â  Â  Â  Â  Â  Â  # 7.5 å…œåº•ï¼šOffice æ–‡æ¡£/æ–‡æœ¬/HTML ä½¿ç”¨ MarkItDownï¼ˆå¦‚æœå¯ç”¨ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  elif (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_ext in [".docx", ".xlsx", ".pptx", ".doc", ".xls", ".ppt", ".html", ".txt", ".csv"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  and self.markitdown
Â  Â  Â  Â  Â  Â  Â  Â  ):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“„ [Auto] Processing Office/Text file with MarkItDown: {file_path}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_markitdown(file_path)

Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ²¡æœ‰åˆé€‚çš„å¤„ç†å™¨
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  supported_formats = "PDF, PNG, JPG (MinerU/PaddleOCR), Audio (SenseVoice), Video, FASTA, GenBank"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self.markitdown:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  supported_formats += ", Office/Text (MarkItDown)"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Unsupported file type: file={file_path}, ext={file_ext}. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Supported formats: {supported_formats}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # 8. å°è¯•ä½¿ç”¨æ ¼å¼å¼•æ“ï¼ˆç”¨æˆ·æ˜ç¡®æŒ‡å®šäº† fasta, genbank ç­‰ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  if FORMAT_ENGINES_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  engine = FormatEngineRegistry.get_engine(backend)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if engine is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ§¬ Processing with format engine: {backend}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = self._process_with_format_engine(file_path, options, engine_name=backend)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æœªçŸ¥çš„ backend
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Unknown backend: {backend}. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Supported backends: auto, pipeline, vlm-*, hybrid-*, paddleocr-vl, sensevoice, video, fasta, genbank"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ ¼å¼å¼•æ“ä¸å¯ç”¨
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Unknown backend: {backend}. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Supported backends: auto, pipeline, vlm-*, hybrid-*, paddleocr-vl, sensevoice, video"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # æ£€æŸ¥ result æ˜¯å¦è¢«æ­£ç¡®èµ‹å€¼
Â  Â  Â  Â  Â  Â  if result is None:
Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"No result generated for backend: {backend}, file: {file_path}")

Â  Â  Â  Â  Â  Â  # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
Â  Â  Â  Â  Â  Â  self.task_db.update_task_status(
Â  Â  Â  Â  Â  Â  Â  Â  task_id=task_id,
Â  Â  Â  Â  Â  Â  Â  Â  status="completed",
Â  Â  Â  Â  Â  Â  Â  Â  result_path=result["result_path"],
Â  Â  Â  Â  Â  Â  Â  Â  error_message=None,
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # å¦‚æœæ˜¯å­ä»»åŠ¡,æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘åˆå¹¶
Â  Â  Â  Â  Â  Â  if parent_task_id:
Â  Â  Â  Â  Â  Â  Â  Â  parent_id_to_merge = self.task_db.on_child_task_completed(task_id)

Â  Â  Â  Â  Â  Â  Â  Â  if parent_id_to_merge:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ,æ‰§è¡Œåˆå¹¶
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”€ All subtasks completed, merging results for parent task {parent_id_to_merge}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self._merge_parent_task_results(parent_id_to_merge)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as merge_error:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to merge parent task {parent_id_to_merge}: {merge_error}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ ‡è®°çˆ¶ä»»åŠ¡ä¸ºå¤±è´¥
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.task_db.update_task_status(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  parent_id_to_merge, "failed", error_message=f"Merge failed: {merge_error}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # æ¸…ç†æ˜¾å­˜ï¼ˆå¦‚æœæ˜¯ GPUï¼‰
Â  Â  Â  Â  Â  Â  if "cuda" in str(self.device).lower():
Â  Â  Â  Â  Â  Â  Â  Â  clean_memory()

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
Â  Â  Â  Â  Â  Â  error_msg = f"{type(e).__name__}: {str(e)}"
Â  Â  Â  Â  Â  Â  self.task_db.update_task_status(task_id=task_id, status="failed", result_path=None, error_message=error_msg)

Â  Â  Â  Â  Â  Â  # å¦‚æœæ˜¯å­ä»»åŠ¡å¤±è´¥,æ ‡è®°çˆ¶ä»»åŠ¡å¤±è´¥
Â  Â  Â  Â  Â  Â  if parent_task_id:
Â  Â  Â  Â  Â  Â  Â  Â  self.task_db.on_child_task_failed(task_id, error_msg)

Â  Â  Â  Â  Â  Â  raise

Â  Â  def _process_with_mineru(self, file_path: str, options: dict) -> dict:
Â  Â  Â  Â  """
Â  Â  Â  Â  ä½¿ç”¨ MinerU å¤„ç†æ–‡æ¡£

Â  Â  Â  Â  æ³¨æ„ï¼š
Â  Â  Â  Â  - MinerU çš„ do_parse åªæ¥å— PDF æ ¼å¼ï¼Œå›¾ç‰‡éœ€è¦å…ˆè½¬æ¢ä¸º PDF
Â  Â  Â  Â  - CUDA_VISIBLE_DEVICES å·²åœ¨ setup() é˜¶æ®µè®¾ç½®ï¼ŒMinerU ä¼šè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„ GPU
Â  Â  Â  Â  """
Â  Â  Â  Â  # å»¶è¿ŸåŠ è½½ MinerU Pipelineï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
Â  Â  Â  Â  if self.mineru_pipeline_engine is None:
Â  Â  Â  Â  Â  Â  from mineru_pipeline import MinerUPipelineEngine

Â  Â  Â  Â  Â  Â  # ä½¿ç”¨åŠ¨æ€è®¾å¤‡é€‰æ‹©ï¼ˆæ”¯æŒ CPU/CUDAï¼‰
Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šCUDA æ¨¡å¼ä¸‹å·²åœ¨ setup() ä¸­è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
Â  Â  Â  Â  Â  Â  # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
Â  Â  Â  Â  Â  Â  self.mineru_pipeline_engine = MinerUPipelineEngine(
Â  Â  Â  Â  Â  Â  Â  Â  device=self.engine_device,
Â  Â  Â  Â  Â  Â  Â  Â  vlm_api_base=self.mineru_vllm_apiÂ  # ä¼ å…¥ MinerU API åœ°å€ (æœ¬åœ°æœåŠ¡)
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  if self.accelerator == "cuda":
Â  Â  Â  Â  Â  Â  Â  Â  gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… MinerU Pipeline engine loaded on cuda:0 (physical GPU {gpu_id})")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  logger.info("âœ… MinerU Pipeline engine loaded on CPU")

Â  Â  Â  Â  # è®¾ç½®è¾“å‡ºç›®å½•
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # æ£€æŸ¥æ˜¯å¦æ˜¯è¿œç¨‹æ¨¡å¼
Â  Â  Â  Â  backend = options.get("parse_mode", "pipeline")
Â  Â  Â  Â  if "http-client" in backend:
Â  Â  Â  Â  Â  Â  # å¦‚æœæ˜¯å®¢æˆ·ç«¯æ¨¡å¼ï¼Œå¿…é¡»æœ‰ server_url
Â  Â  Â  Â  Â  Â  if not options.get("server_url"):
Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Remote backend {backend} selected but no server_url provided in options.")
Â  Â  Â  Â  Â  Â  Â  Â  # å°è¯•å›é€€åˆ°é»˜è®¤æœ¬åœ°æœåŠ¡ï¼ˆå¦‚æœæœ‰ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  if self.mineru_vllm_api:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â options["server_url"] = self.mineru_vllm_api.replace("/v1", "") # å»æ‰ /v1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â logger.info(f"Â  Â Using default local server: {options['server_url']}")

Â  Â  Â  Â  # å¤„ç†æ–‡ä»¶
Â  Â  Â  Â  result = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  # æ³¨æ„ï¼šresult["result_path"] æ˜¯å®é™…åŒ…å« md æ–‡ä»¶çš„ç›®å½•ï¼ˆä¾‹å¦‚ {output_dir}/{file_name}/auto/ï¼‰
Â  Â  Â  Â  # æˆ‘ä»¬éœ€è¦åœ¨è¿™ä¸ªresult["result_path"] ä¸Šè¿è¡Œ normalize_output
Â  Â  Â  Â  actual_output_dir = Path(result["result_path"])
Â  Â  Â  Â  normalize_output(actual_output_dir)

Â  Â  Â  Â  # MinerU Pipeline è¿”å›ç»“æ„ï¼š
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "result_path": result["result_path"],
Â  Â  Â  Â  Â  Â  "content": result["markdown"],
Â  Â  Â  Â  Â  Â  "json_path": result.get("json_path"),
Â  Â  Â  Â  Â  Â  "json_content": result.get("json_content"),
Â  Â  Â  Â  }

Â  Â  def _process_with_markitdown(self, file_path: str) -> dict:
Â  Â  Â  Â  """ä½¿ç”¨ MarkItDown å¤„ç† Office æ–‡æ¡£ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒ DOCX å›¾ç‰‡æå–ï¼‰"""
Â  Â  Â  Â  if not self.markitdown:
Â  Â  Â  Â  Â  Â  raise RuntimeError("MarkItDown is not available")

Â  Â  Â  Â  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # å¤„ç†æ–‡ä»¶ï¼šæå–æ–‡æœ¬
Â  Â  Â  Â  result = self.markitdown.convert(file_path)
Â  Â  Â  Â  markdown_content = result.text_content

Â  Â  Â  Â  # å¦‚æœæ˜¯ DOCX æ–‡ä»¶ï¼Œæå–åµŒå…¥çš„å›¾ç‰‡
Â  Â  Â  Â  file_ext = Path(file_path).suffix.lower()
Â  Â  Â  Â  if file_ext == ".docx":
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  from utils.docx_image_extractor import extract_images_from_docx, append_images_to_markdown

Â  Â  Â  Â  Â  Â  Â  Â  # æå–å›¾ç‰‡åˆ° images ç›®å½•
Â  Â  Â  Â  Â  Â  Â  Â  images_dir = output_dir / "images"
Â  Â  Â  Â  Â  Â  Â  Â  images = extract_images_from_docx(file_path, str(images_dir))

Â  Â  Â  Â  Â  Â  Â  Â  # å¦‚æœæœ‰å›¾ç‰‡ï¼Œå°†å›¾ç‰‡å¼•ç”¨æ·»åŠ åˆ° Markdown
Â  Â  Â  Â  Â  Â  Â  Â  if images:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  markdown_content = append_images_to_markdown(markdown_content, images)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ–¼ï¸Â  Extracted {len(images)} images from DOCX")

Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to extract images from DOCX: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  # ç»§ç»­å¤„ç†ï¼Œä¸å½±å“æ–‡æœ¬æå–

Â  Â  Â  Â  # ä¿å­˜ç»“æœåˆ°ç›®å½•ä¸­
Â  Â  Â  Â  output_file = output_dir / f"{Path(file_path).stem}_markitdown.md"
Â  Â  Â  Â  output_file.write_text(markdown_content, encoding="utf-8")

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  normalize_output(output_dir)

Â  Â  Â  Â  # è¿”å›ç›®å½•è·¯å¾„ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
Â  Â  Â  Â  return {"result_path": str(output_dir), "content": markdown_content}

Â  Â  def _convert_office_to_pdf(self, file_path: str) -> str:
Â  Â  Â  Â  """
Â  Â  Â  Â  ä½¿ç”¨ LibreOffice å°† Office æ–‡ä»¶è½¬æ¢ä¸º PDF

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  file_path: Office æ–‡ä»¶è·¯å¾„

Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  è½¬æ¢åçš„ PDF æ–‡ä»¶è·¯å¾„

Â  Â  Â  Â  Raises:
Â  Â  Â  Â  Â  Â  RuntimeError: è½¬æ¢å¤±è´¥æ—¶æŠ›å‡º
Â  Â  Â  Â  """
Â  Â  Â  Â  import subprocess
Â  Â  Â  Â  import shutil
Â  Â  Â  Â  import tempfile
Â  Â  Â  Â  from pathlib import Path

Â  Â  Â  Â  input_file = Path(file_path)
Â  Â  Â  Â  final_output_dir = input_file.parent

Â  Â  Â  Â  # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å
Â  Â  Â  Â  final_pdf_file = final_output_dir / f"{input_file.stem}.pdf"

Â  Â  Â  Â  # å¦‚æœå·²å­˜åœ¨åŒå PDFï¼Œå…ˆåˆ é™¤
Â  Â  Â  Â  if final_pdf_file.exists():
Â  Â  Â  Â  Â  Â  final_pdf_file.unlink()

Â  Â  Â  Â  logger.info(f"ğŸ”„ Converting Office to PDF: {input_file.name}")

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # ä½¿ç”¨ /tmp ä½œä¸ºä¸´æ—¶ç›®å½•ï¼ˆé¿å… Docker æŒ‚è½½å·å†™å…¥é—®é¢˜ï¼‰
Â  Â  Â  Â  Â  Â  with tempfile.TemporaryDirectory(prefix="libreoffice_") as temp_dir:
Â  Â  Â  Â  Â  Â  Â  Â  temp_dir_path = Path(temp_dir)

Â  Â  Â  Â  Â  Â  Â  Â  # å¤åˆ¶è¾“å…¥æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
Â  Â  Â  Â  Â  Â  Â  Â  temp_input = temp_dir_path / input_file.name
Â  Â  Â  Â  Â  Â  Â  Â  shutil.copy2(input_file, temp_input)

Â  Â  Â  Â  Â  Â  Â  Â  # åœ¨ä¸´æ—¶ç›®å½•æ‰§è¡Œè½¬æ¢
Â  Â  Â  Â  Â  Â  Â  Â  cmd = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "libreoffice",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "--headless",Â  # æ— ç•Œé¢æ¨¡å¼
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "--convert-to",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "pdf",Â  # è½¬æ¢ä¸º PDF
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "--outdir",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  str(temp_dir_path),Â  # è¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  str(temp_input),Â  # è¾“å…¥æ–‡ä»¶
Â  Â  Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â  Â  Â  # æ‰§è¡Œè½¬æ¢ï¼ˆè¶…æ—¶ 120 ç§’ï¼‰
Â  Â  Â  Â  Â  Â  Â  Â  result = subprocess.run(cmd, check=True, timeout=120, capture_output=True, text=True)

Â  Â  Â  Â  Â  Â  Â  Â  # ä¸´æ—¶è¾“å‡ºæ–‡ä»¶è·¯å¾„
Â  Â  Â  Â  Â  Â  Â  Â  temp_pdf = temp_dir_path / f"{input_file.stem}.pdf"

Â  Â  Â  Â  Â  Â  Â  Â  # éªŒè¯è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
Â  Â  Â  Â  Â  Â  Â  Â  if not temp_pdf.exists():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stderr_output = result.stderr if result.stderr else "No error output"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise RuntimeError(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"LibreOffice conversion failed: output file not found: {temp_pdf}\nstderr: {stderr_output}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  # ç§»åŠ¨è½¬æ¢åçš„ PDF åˆ°æœ€ç»ˆç›®å½•
Â  Â  Â  Â  Â  Â  Â  Â  shutil.move(str(temp_pdf), str(final_pdf_file))

Â  Â  Â  Â  Â  Â  Â  Â  logger.info(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"âœ… Office converted to PDF: {final_pdf_file.name} ({final_pdf_file.stat().st_size / 1024:.1f} KB)"
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  return str(final_pdf_file)

Â  Â  Â  Â  except subprocess.TimeoutExpired:
Â  Â  Â  Â  Â  Â  raise RuntimeError(f"LibreOffice conversion timeout (>120s): {input_file.name}")
Â  Â  Â  Â  except subprocess.CalledProcessError as e:
Â  Â  Â  Â  Â  Â  stderr_output = e.stderr if e.stderr else "No error output"
Â  Â  Â  Â  Â  Â  raise RuntimeError(f"LibreOffice conversion failed: {stderr_output}")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  raise RuntimeError(f"Office to PDF conversion error: {e}")

Â  Â  def _process_with_paddleocr_vl(self, file_path: str, options: dict) -> dict:
Â  Â  Â  Â  """ä½¿ç”¨ PaddleOCR-VL å¤„ç†å›¾ç‰‡æˆ– PDF"""
Â  Â  Â  Â  # æ£€æŸ¥åŠ é€Ÿå™¨ç±»å‹ï¼ˆPaddleOCR-VL ä»…æ”¯æŒ GPUï¼‰
Â  Â  Â  Â  if self.accelerator == "cpu":
Â  Â  Â  Â  Â  Â  raise RuntimeError(
Â  Â  Â  Â  Â  Â  Â  Â  "PaddleOCR-VL requires GPU and is not supported in CPU mode. "
Â  Â  Â  Â  Â  Â  Â  Â  "Please use 'mineru' or 'markitdown' backend instead."
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  # å»¶è¿ŸåŠ è½½ PaddleOCR-VLï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
Â  Â  Â  Â  if self.paddleocr_vl_engine is None:
Â  Â  Â  Â  Â  Â  from paddleocr_vl import PaddleOCRVLEngine

Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
Â  Â  Â  Â  Â  Â  # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
Â  Â  Â  Â  Â  Â  # ã€å…³é”®ä¿®æ”¹ã€‘æŒ‡å®šæ¨¡å‹åç§°ï¼ŒåŒ¹é…æœ¬åœ°ä¸‹è½½çš„ç›®å½•
Â  Â  Â  Â  Â  Â  self.paddleocr_vl_engine = PaddleOCRVLEngine(device="cuda:0", model_name="PaddleOCR-VL-1.5-0.9B")
Â  Â  Â  Â  Â  Â  gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  logger.info(f"âœ… PaddleOCR-VL engine loaded on cuda:0 (physical GPU {gpu_id})")

Â  Â  Â  Â  # è®¾ç½®è¾“å‡ºç›®å½•
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # å¤„ç†æ–‡ä»¶ï¼ˆparse æ–¹æ³•éœ€è¦ output_pathï¼‰
Â  Â  Â  Â  result = self.paddleocr_vl_engine.parse(file_path, output_path=str(output_dir))

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  normalize_output(output_dir)

Â  Â  Â  Â  # è¿”å›ç»“æœ
Â  Â  Â  Â  return {"result_path": str(output_dir), "content": result.get("markdown", "")}

Â  Â  def _process_with_paddleocr_vl_vllm(self, file_path: str, options: dict) -> dict:
Â  Â  Â  Â  """ä½¿ç”¨ PaddleOCR-VL VLLM å¤„ç†å›¾ç‰‡æˆ– PDF"""
Â  Â  Â  Â  # æ£€æŸ¥åŠ é€Ÿå™¨ç±»å‹ï¼ˆPaddleOCR-VL VLLM ä»…æ”¯æŒ GPUï¼‰
Â  Â  Â  Â  if self.accelerator == "cpu":
Â  Â  Â  Â  Â  Â  raise RuntimeError(
Â  Â  Â  Â  Â  Â  Â  Â  "PaddleOCR-VL VLLM requires GPU and is not supported in CPU mode. "
Â  Â  Â  Â  Â  Â  Â  Â  "Please use 'mineru' or 'markitdown' backend instead."
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  # å»¶è¿ŸåŠ è½½ PaddleOCR-VLï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
Â  Â  Â  Â  if self.paddleocr_vl_vllm_engine is None:
Â  Â  Â  Â  Â  Â  from paddleocr_vl_vllm import PaddleOCRVLVLLMEngine

Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
Â  Â  Â  Â  Â  Â  # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
Â  Â  Â  Â  Â  Â  # ã€å…³é”®ä¿®æ”¹ã€‘æŒ‡å®šæ¨¡å‹åç§°
Â  Â  Â  Â  Â  Â  self.paddleocr_vl_vllm_engine = PaddleOCRVLVLLMEngine(
Â  Â  Â  Â  Â  Â  Â  Â  device="cuda:0",Â 
Â  Â  Â  Â  Â  Â  Â  Â  vllm_api_base=self.paddleocr_vl_vllm_api,
Â  Â  Â  Â  Â  Â  Â  Â  model_name="PaddleOCR-VL-1.5-0.9B"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  logger.info(f"âœ… PaddleOCR-VL VLLM engine loaded on cuda:0 (physical GPU {gpu_id})")

Â  Â  Â  Â  # è®¾ç½®è¾“å‡ºç›®å½•
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # å¤„ç†æ–‡ä»¶ï¼ˆparse æ–¹æ³•éœ€è¦ output_pathï¼‰
Â  Â  Â  Â  result = self.paddleocr_vl_vllm_engine.parse(file_path, output_path=str(output_dir))

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  normalize_output(output_dir, handle_method="paddleocr-vl")

Â  Â  Â  Â  # è¿”å›ç»“æœ
Â  Â  Â  Â  return {"result_path": str(output_dir), "content": result.get("markdown", "")}

Â  Â  def _process_audio(self, file_path: str, options: dict) -> dict:
Â  Â  Â  Â  """ä½¿ç”¨ SenseVoice å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
Â  Â  Â  Â  # å»¶è¿ŸåŠ è½½ SenseVoiceï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
Â  Â  Â  Â  if self.sensevoice_engine is None:
Â  Â  Â  Â  Â  Â  from audio_engines import SenseVoiceEngine

Â  Â  Â  Â  Â  Â  # ä½¿ç”¨åŠ¨æ€è®¾å¤‡é€‰æ‹©ï¼ˆæ”¯æŒ CPU/CUDAï¼‰
Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šCUDA æ¨¡å¼ä¸‹å·²åœ¨ setup() ä¸­è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
Â  Â  Â  Â  Â  Â  # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
Â  Â  Â  Â  Â  Â  self.sensevoice_engine = SenseVoiceEngine(device=self.engine_device)
Â  Â  Â  Â  Â  Â  if self.accelerator == "cuda":
Â  Â  Â  Â  Â  Â  Â  Â  gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… SenseVoice engine loaded on cuda:0 (physical GPU {gpu_id})")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  logger.info("âœ… SenseVoice engine loaded on CPU")

Â  Â  Â  Â  # è®¾ç½®è¾“å‡ºç›®å½•
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # å¤„ç†éŸ³é¢‘ï¼ˆparse æ–¹æ³•éœ€è¦ output_path å‚æ•°ï¼‰
Â  Â  Â  Â  result = self.sensevoice_engine.parse(
Â  Â  Â  Â  Â  Â  audio_path=file_path,
Â  Â  Â  Â  Â  Â  output_path=str(output_dir),
Â  Â  Â  Â  Â  Â  language=options.get("lang", "auto"),
Â  Â  Â  Â  Â  Â  use_itn=options.get("use_itn", True),
Â  Â  Â  Â  Â  Â  enable_speaker_diarization=options.get("enable_speaker_diarization", False),Â  # ä» API å‚æ•°æ§åˆ¶
Â  Â  Â  Â  )

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  normalize_output(output_dir)

Â  Â  Â  Â  # SenseVoice è¿”å›ç»“æ„ï¼š
Â  Â  Â  Â  # {
Â  Â  Â  Â  #Â  Â "success": True,
Â  Â  Â  Â  #Â  Â "output_path": str,
Â  Â  Â  Â  #Â  Â "markdown": str,
Â  Â  Â  Â  #Â  Â "markdown_file": str,
Â  Â  Â  Â  #Â  Â "json_file": str,
Â  Â  Â  Â  #Â  Â "json_data": dict,
Â  Â  Â  Â  #Â  Â "result": dict
Â  Â  Â  Â  # }
Â  Â  Â  Â  return {"result_path": str(output_dir), "content": result.get("markdown", "")}

Â  Â  def _process_video(self, file_path: str, options: dict) -> dict:
Â  Â  Â  Â  """ä½¿ç”¨è§†é¢‘å¤„ç†å¼•æ“å¤„ç†è§†é¢‘æ–‡ä»¶"""
Â  Â  Â  Â  # å»¶è¿ŸåŠ è½½è§†é¢‘å¼•æ“ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
Â  Â  Â  Â  if self.video_engine is None:
Â  Â  Â  Â  Â  Â  from video_engines import VideoProcessingEngine

Â  Â  Â  Â  Â  Â  # ä½¿ç”¨åŠ¨æ€è®¾å¤‡é€‰æ‹©ï¼ˆæ”¯æŒ CPU/CUDAï¼‰
Â  Â  Â  Â  Â  Â  # æ³¨æ„ï¼šCUDA æ¨¡å¼ä¸‹å·²åœ¨ setup() ä¸­è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
Â  Â  Â  Â  Â  Â  # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
Â  Â  Â  Â  Â  Â  self.video_engine = VideoProcessingEngine(device=self.engine_device)
Â  Â  Â  Â  Â  Â  if self.accelerator == "cuda":
Â  Â  Â  Â  Â  Â  Â  Â  gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Video processing engine loaded on cuda:0 (physical GPU {gpu_id})")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  logger.info("âœ… Video processing engine loaded on CPU")

Â  Â  Â  Â  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # å¤„ç†è§†é¢‘
Â  Â  Â  Â  result = self.video_engine.parse(
Â  Â  Â  Â  Â  Â  video_path=file_path,
Â  Â  Â  Â  Â  Â  output_path=str(output_dir),
Â  Â  Â  Â  Â  Â  language=options.get("lang", "auto"),
Â  Â  Â  Â  Â  Â  use_itn=options.get("use_itn", True),
Â  Â  Â  Â  Â  Â  keep_audio=options.get("keep_audio", False),
Â  Â  Â  Â  Â  Â  enable_keyframe_ocr=options.get("enable_keyframe_ocr", False),
Â  Â  Â  Â  Â  Â  ocr_backend=options.get("ocr_backend", "paddleocr-vl"),
Â  Â  Â  Â  Â  Â  keep_keyframes=options.get("keep_keyframes", False),
Â  Â  Â  Â  )

Â  Â  Â  Â  # ä¿å­˜ç»“æœï¼ˆMarkdown æ ¼å¼ï¼‰
Â  Â  Â  Â  output_file = output_dir / f"{Path(file_path).stem}_video_analysis.md"
Â  Â  Â  Â  output_file.write_text(result["markdown"], encoding="utf-8")

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  normalize_output(output_dir)

Â  Â  Â  Â  return {"result_path": str(output_dir), "content": result["markdown"]}

Â  Â  def _preprocess_remove_watermark(self, file_path: str, options: dict) -> Path:
Â  Â  Â  Â  """
Â  Â  Â  Â  é¢„å¤„ç†ï¼šå»é™¤ PDF æ°´å°

Â  Â  Â  Â  è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå»é™¤æ°´å°åçš„æ–‡ä»¶ä¼šè¢«åç»­çš„è§£æå¼•æ“å¤„ç†

Â  Â  Â  Â  è¿”å›ï¼š
Â  Â  Â  Â  Â  Â  å»é™¤æ°´å°åçš„ PDF è·¯å¾„

Â  Â  Â  Â  æ”¯æŒçš„ options å‚æ•°ï¼š
Â  Â  Â  Â  Â  Â  - auto_detect: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹ PDF ç±»å‹ï¼ˆé»˜è®¤ Trueï¼‰
Â  Â  Â  Â  Â  Â  - force_scanned: å¼ºåˆ¶ä½¿ç”¨æ‰«æä»¶æ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼‰
Â  Â  Â  Â  Â  Â  - remove_text: æ˜¯å¦åˆ é™¤æ–‡æœ¬å¯¹è±¡ï¼ˆå¯ç¼–è¾‘ PDFï¼Œé»˜è®¤ Trueï¼‰
Â  Â  Â  Â  Â  Â  - remove_images: æ˜¯å¦åˆ é™¤å›¾ç‰‡å¯¹è±¡ï¼ˆå¯ç¼–è¾‘ PDFï¼Œé»˜è®¤ Trueï¼‰
Â  Â  Â  Â  Â  Â  - remove_annotations: æ˜¯å¦åˆ é™¤æ³¨é‡Šï¼ˆå¯ç¼–è¾‘ PDFï¼Œé»˜è®¤ Trueï¼‰
Â  Â  Â  Â  Â  Â  - keywords: æ–‡æœ¬å…³é”®è¯åˆ—è¡¨ï¼ˆå¯ç¼–è¾‘ PDFï¼Œåªåˆ é™¤åŒ…å«è¿™äº›å…³é”®è¯çš„æ–‡æœ¬ï¼‰
Â  Â  Â  Â  Â  Â  - dpi: è½¬æ¢åˆ†è¾¨ç‡ï¼ˆæ‰«æä»¶ PDFï¼Œé»˜è®¤ 200ï¼‰
Â  Â  Â  Â  Â  Â  - conf_threshold: YOLO ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆæ‰«æä»¶ PDFï¼Œé»˜è®¤ 0.35ï¼‰
Â  Â  Â  Â  Â  Â  - dilation: æ©ç è†¨èƒ€ï¼ˆæ‰«æä»¶ PDFï¼Œé»˜è®¤ 10ï¼‰
Â  Â  Â  Â  """
Â  Â  Â  Â  if not self.watermark_handler:
Â  Â  Â  Â  Â  Â  raise RuntimeError("Watermark removal is not available (CUDA required)")

Â  Â  Â  Â  # è®¾ç½®è¾“å‡ºè·¯å¾„
Â  Â  Â  Â  output_file = Path(self.output_dir) / f"{Path(file_path).stem}_no_watermark.pdf"

Â  Â  Â  Â  # æ„å»ºå‚æ•°å­—å…¸ï¼ˆåªä¼ é€’å®é™…æä¾›çš„å‚æ•°ï¼‰
Â  Â  Â  Â  kwargs = {}

Â  Â  Â  Â  # é€šç”¨å‚æ•°
Â  Â  Â  Â  if "auto_detect" in options:
Â  Â  Â  Â  Â  Â  kwargs["auto_detect"] = options["auto_detect"]
Â  Â  Â  Â  if "force_scanned" in options:
Â  Â  Â  Â  Â  Â  kwargs["force_scanned"] = options["force_scanned"]

Â  Â  Â  Â  # å¯ç¼–è¾‘ PDF å‚æ•°
Â  Â  Â  Â  if "remove_text" in options:
Â  Â  Â  Â  Â  Â  kwargs["remove_text"] = options["remove_text"]
Â  Â  Â  Â  if "remove_images" in options:
Â  Â  Â  Â  Â  Â  kwargs["remove_images"] = options["remove_images"]
Â  Â  Â  Â  if "remove_annotations" in options:
Â  Â  Â  Â  Â  Â  kwargs["remove_annotations"] = options["remove_annotations"]
Â  Â  Â  Â  if "watermark_keywords" in options:
Â  Â  Â  Â  Â  Â  kwargs["keywords"] = options["watermark_keywords"]

Â  Â  Â  Â  # æ‰«æä»¶ PDF å‚æ•°
Â  Â  Â  Â  if "watermark_dpi" in options:
Â  Â  Â  Â  Â  Â  kwargs["dpi"] = options["watermark_dpi"]
Â  Â  Â  Â  if "watermark_conf_threshold" in options:
Â  Â  Â  Â  Â  Â  kwargs["conf_threshold"] = options["watermark_conf_threshold"]
Â  Â  Â  Â  if "watermark_dilation" in options:
Â  Â  Â  Â  Â  Â  kwargs["dilation"] = options["watermark_dilation"]

Â  Â  Â  Â  # å»é™¤æ°´å°ï¼ˆè¿”å›è¾“å‡ºè·¯å¾„ï¼‰
Â  Â  Â  Â  cleaned_pdf_path = self.watermark_handler.remove_watermark(
Â  Â  Â  Â  Â  Â  input_path=file_path, output_path=str(output_file), **kwargs
Â  Â  Â  Â  )

Â  Â  Â  Â  return cleaned_pdf_path

Â  Â  def _should_split_pdf(self, task_id: str, file_path: str, task: dict, options: dict) -> bool:
Â  Â  Â  Â  """
Â  Â  Â  Â  åˆ¤æ–­ PDF æ˜¯å¦éœ€è¦æ‹†åˆ†ï¼Œå¦‚æœéœ€è¦åˆ™æ‰§è¡Œæ‹†åˆ†

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  task_id: ä»»åŠ¡ID
Â  Â  Â  Â  Â  Â  file_path: PDF æ–‡ä»¶è·¯å¾„
Â  Â  Â  Â  Â  Â  task: ä»»åŠ¡å­—å…¸
Â  Â  Â  Â  Â  Â  options: å¤„ç†é€‰é¡¹

Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  bool: True è¡¨ç¤ºå·²æ‹†åˆ†ï¼ŒFalse è¡¨ç¤ºä¸éœ€è¦æ‹†åˆ†
Â  Â  Â  Â  """
Â  Â  Â  Â  from utils.pdf_utils import get_pdf_page_count, split_pdf_file

Â  Â  Â  Â  # è¯»å–é…ç½®
Â  Â  Â  Â  pdf_split_enabled = os.getenv("PDF_SPLIT_ENABLED", "true").lower() == "true"
Â  Â  Â  Â  if not pdf_split_enabled:
Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  pdf_split_threshold = int(os.getenv("PDF_SPLIT_THRESHOLD_PAGES", "500"))
Â  Â  Â  Â  pdf_split_chunk_size = int(os.getenv("PDF_SPLIT_CHUNK_SIZE", "500"))

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # å¿«é€Ÿè¯»å– PDF é¡µæ•°ï¼ˆåªè¯»å…ƒæ•°æ®ï¼‰
Â  Â  Â  Â  Â  Â  page_count = get_pdf_page_count(Path(file_path))
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“„ PDF has {page_count} pages (threshold: {pdf_split_threshold})")

Â  Â  Â  Â  Â  Â  # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‹†åˆ†
Â  Â  Â  Â  Â  Â  if page_count <= pdf_split_threshold:
Â  Â  Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  Â  Â  logger.info(
Â  Â  Â  Â  Â  Â  Â  Â  f"ğŸ”€ Large PDF detected ({page_count} pages), splitting into chunks of {pdf_split_chunk_size} pages"
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # å°†å½“å‰ä»»åŠ¡è½¬ä¸ºçˆ¶ä»»åŠ¡
Â  Â  Â  Â  Â  Â  self.task_db.convert_to_parent_task(task_id, child_count=0)

Â  Â  Â  Â  Â  Â  # æ‹†åˆ† PDF æ–‡ä»¶
Â  Â  Â  Â  Â  Â  split_dir = Path(self.output_dir) / "splits" / task_id
Â  Â  Â  Â  Â  Â  split_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  Â  Â  chunks = split_pdf_file(
Â  Â  Â  Â  Â  Â  Â  Â  pdf_path=Path(file_path),
Â  Â  Â  Â  Â  Â  Â  Â  output_dir=split_dir,
Â  Â  Â  Â  Â  Â  Â  Â  chunk_size=pdf_split_chunk_size,
Â  Â  Â  Â  Â  Â  Â  Â  parent_task_id=task_id,
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  logger.info(f"âœ‚ï¸Â  PDF split into {len(chunks)} chunks")

Â  Â  Â  Â  Â  Â  # ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»ºå­ä»»åŠ¡
Â  Â  Â  Â  Â  Â  backend = task.get("backend", "auto")
Â  Â  Â  Â  Â  Â  priority = task.get("priority", 0)
Â  Â  Â  Â  Â  Â  user_id = task.get("user_id")

Â  Â  Â  Â  Â  Â  for chunk_info in chunks:
Â  Â  Â  Â  Â  Â  Â  Â  # å¤åˆ¶é€‰é¡¹å¹¶æ·»åŠ åˆ†å—ä¿¡æ¯
Â  Â  Â  Â  Â  Â  Â  Â  chunk_options = options.copy()
Â  Â  Â  Â  Â  Â  Â  Â  chunk_options["chunk_info"] = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "start_page": chunk_info["start_page"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "end_page": chunk_info["end_page"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "page_count": chunk_info["page_count"],
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  # åˆ›å»ºå­ä»»åŠ¡
Â  Â  Â  Â  Â  Â  Â  Â  child_task_id = self.task_db.create_child_task(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  parent_task_id=task_id,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"{Path(file_path).stem}_pages_{chunk_info['start_page']}-{chunk_info['end_page']}.pdf",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_path=chunk_info["path"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  backend=backend,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  options=chunk_options,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  priority=priority,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  user_id=user_id,
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  logger.info(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Â  âœ… Created subtask {child_task_id}: pages {chunk_info['start_page']}-{chunk_info['end_page']}"
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # æ›´æ–°çˆ¶ä»»åŠ¡çš„å­ä»»åŠ¡æ•°é‡
Â  Â  Â  Â  Â  Â  self.task_db.convert_to_parent_task(task_id, child_count=len(chunks))

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ‰ Large PDF split complete: {len(chunks)} subtasks created for parent task {task_id}")

Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to split PDF: {e}")
Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸Â  Falling back to processing as single task")
Â  Â  Â  Â  Â  Â  return False

Â  Â  def _merge_parent_task_results(self, parent_task_id: str):
Â  Â  Â  Â  """
Â  Â  Â  Â  åˆå¹¶çˆ¶ä»»åŠ¡çš„æ‰€æœ‰å­ä»»åŠ¡ç»“æœ

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  parent_task_id: çˆ¶ä»»åŠ¡ID
Â  Â  Â  Â  """
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # è·å–çˆ¶ä»»åŠ¡å’Œæ‰€æœ‰å­ä»»åŠ¡
Â  Â  Â  Â  Â  Â  parent_task = self.task_db.get_task_with_children(parent_task_id)

Â  Â  Â  Â  Â  Â  if not parent_task:
Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"Parent task {parent_task_id} not found")

Â  Â  Â  Â  Â  Â  children = parent_task.get("children", [])

Â  Â  Â  Â  Â  Â  if not children:
Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"No child tasks found for parent {parent_task_id}")

Â  Â  Â  Â  Â  Â  # æŒ‰é¡µç æ’åºå­ä»»åŠ¡
Â  Â  Â  Â  Â  Â  children.sort(key=lambda x: json.loads(x.get("options", "{}")).get("chunk_info", {}).get("start_page", 0))

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”€ Merging {len(children)} subtask results for parent task {parent_task_id}")

Â  Â  Â  Â  Â  Â  # åˆ›å»ºçˆ¶ä»»åŠ¡è¾“å‡ºç›®å½•
Â  Â  Â  Â  Â  Â  parent_output_dir = Path(self.output_dir) / Path(parent_task["file_path"]).stem
Â  Â  Â  Â  Â  Â  parent_output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  Â  Â  # åˆå¹¶ Markdown
Â  Â  Â  Â  Â  Â  markdown_parts = []
Â  Â  Â  Â  Â  Â  json_pages = []
Â  Â  Â  Â  Â  Â  has_json = False

Â  Â  Â  Â  Â  Â  for idx, child in enumerate(children):
Â  Â  Â  Â  Â  Â  Â  Â  if child["status"] != "completed":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Child task {child['task_id']} not completed (status: {child['status']})")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  result_dir = Path(child["result_path"])
Â  Â  Â  Â  Â  Â  Â  Â  chunk_info = json.loads(child.get("options", "{}")).get("chunk_info", {})

Â  Â  Â  Â  Â  Â  Â  Â  # è¯»å– Markdown
Â  Â  Â  Â  Â  Â  Â  Â  md_files = list(result_dir.rglob("*.md"))
Â  Â  Â  Â  Â  Â  Â  Â  if md_files:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  md_file = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for f in md_files:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if f.name == "result.md":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  md_file = f
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not md_file:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  md_file = md_files[0]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = md_file.read_text(encoding="utf-8")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ·»åŠ åˆ†é¡µæ ‡è®°
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if chunk_info:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  markdown_parts.append(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"\n\n\n\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  markdown_parts.append(content)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Â  Â âœ… Merged chunk {idx + 1}/{len(children)}: "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"pages {chunk_info.get('start_page', '?')}-{chunk_info.get('end_page', '?')}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  # è¯»å– JSON (å¦‚æœæœ‰)
Â  Â  Â  Â  Â  Â  Â  Â  json_files = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for f in result_dir.rglob("*.json")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if f.name in ["content.json", "result.json"] or "_content_list.json" in f.name
Â  Â  Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â  Â  Â  if json_files:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json_file = json_files[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json_content = json.loads(json_file.read_text(encoding="utf-8"))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # åˆå¹¶ JSON é¡µé¢æ•°æ®
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "pages" in json_content:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  has_json = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page_offset = chunk_info.get("start_page", 1) - 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for page in json_content["pages"]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # è°ƒæ•´é¡µç 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "page_number" in page:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page["page_number"] += page_offset
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json_pages.append(page)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as json_e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to merge JSON for chunk {idx + 1}: {json_e}")

Â  Â  Â  Â  Â  Â  # ä¿å­˜åˆå¹¶åçš„ Markdown
Â  Â  Â  Â  Â  Â  merged_md = "".join(markdown_parts)
Â  Â  Â  Â  Â  Â  md_output = parent_output_dir / "result.md"
Â  Â  Â  Â  Â  Â  md_output.write_text(merged_md, encoding="utf-8")
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“„ Merged Markdown saved: {md_output}")

Â  Â  Â  Â  Â  Â  # ä¿å­˜åˆå¹¶åçš„ JSON (å¦‚æœæœ‰)
Â  Â  Â  Â  Â  Â  if has_json and json_pages:
Â  Â  Â  Â  Â  Â  Â  Â  merged_json = {"pages": json_pages}
Â  Â  Â  Â  Â  Â  Â  Â  json_output = parent_output_dir / "result.json"
Â  Â  Â  Â  Â  Â  Â  Â  json_output.write_text(json.dumps(merged_json, indent=2, ensure_ascii=False), encoding="utf-8")
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“„ Merged JSON saved: {json_output}")

Â  Â  Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡º
Â  Â  Â  Â  Â  Â  normalize_output(parent_output_dir)

Â  Â  Â  Â  Â  Â  # æ›´æ–°çˆ¶ä»»åŠ¡çŠ¶æ€
Â  Â  Â  Â  Â  Â  self.task_db.update_task_status(
Â  Â  Â  Â  Â  Â  Â  Â  task_id=parent_task_id, status="completed", result_path=str(parent_output_dir)
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Parent task {parent_task_id} merged successfully")

Â  Â  Â  Â  Â  Â  # æ¸…ç†å­ä»»åŠ¡çš„ä¸´æ—¶æ–‡ä»¶
Â  Â  Â  Â  Â  Â  self._cleanup_child_task_files(children)

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Failed to merge parent task {parent_task_id}: {e}")
Â  Â  Â  Â  Â  Â  logger.exception(e)
Â  Â  Â  Â  Â  Â  raise

Â  Â  def _cleanup_child_task_files(self, children: list):
Â  Â  Â  Â  """
Â  Â  Â  Â  æ¸…ç†å­ä»»åŠ¡çš„ä¸´æ—¶æ–‡ä»¶

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  children: å­ä»»åŠ¡åˆ—è¡¨
Â  Â  Â  Â  """
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  for child in children:
Â  Â  Â  Â  Â  Â  Â  Â  # åˆ é™¤å­ä»»åŠ¡çš„åˆ†ç‰‡ PDF æ–‡ä»¶
Â  Â  Â  Â  Â  Â  Â  Â  if child.get("file_path"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chunk_file = Path(child["file_path"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if chunk_file.exists() and chunk_file.is_file():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chunk_file.unlink()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.debug(f"ğŸ—‘ï¸Â  Deleted chunk file: {chunk_file.name}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to delete chunk file {chunk_file.name}: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  # å¯é€‰: åˆ é™¤å­ä»»åŠ¡çš„ç»“æœç›®å½• (å¦‚æœéœ€è¦èŠ‚çœç©ºé—´)
Â  Â  Â  Â  Â  Â  Â  Â  # æ³¨æ„: è¿™ä¼šåˆ é™¤ä¸­é—´ç»“æœ,å¯èƒ½å½±å“è°ƒè¯•
Â  Â  Â  Â  Â  Â  Â  Â  # if child.get("result_path"):
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â result_dir = Path(child["result_path"])
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â if result_dir.exists() and result_dir.is_dir():
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â  Â  Â try:
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â  Â  Â  Â  Â shutil.rmtree(result_dir)
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â  Â  Â  Â  Â logger.debug(f"ğŸ—‘ï¸Â  Deleted result dir: {result_dir.name}")
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â  Â  Â except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â  Â  Â  Â  Â  Â logger.warning(f"âš ï¸Â  Failed to delete result dir {result_dir.name}: {e}")

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to cleanup child task files: {e}")

Â  Â  def _process_with_format_engine(self, file_path: str, options: dict, engine_name: Optional[str] = None) -> dict:
Â  Â  Â  Â  """
Â  Â  Â  Â  ä½¿ç”¨æ ¼å¼å¼•æ“å¤„ç†ä¸“ä¸šé¢†åŸŸæ ¼å¼æ–‡ä»¶

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  file_path: æ–‡ä»¶è·¯å¾„
Â  Â  Â  Â  Â  Â  options: å¤„ç†é€‰é¡¹
Â  Â  Â  Â  Â  Â  engine_name: æŒ‡å®šçš„å¼•æ“åç§°ï¼ˆå¦‚ fasta, genbankï¼‰ï¼Œä¸º None æ—¶è‡ªåŠ¨é€‰æ‹©
Â  Â  Â  Â  """
Â  Â  Â  Â  # è·å–è¯­è¨€è®¾ç½®
Â  Â  Â  Â  lang = options.get("language", "en")

Â  Â  Â  Â  # æ ¹æ®æŒ‡å®šçš„å¼•æ“åç§°æˆ–æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¼•æ“
Â  Â  Â  Â  if engine_name:
Â  Â  Â  Â  Â  Â  # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†å¼•æ“
Â  Â  Â  Â  Â  Â  engine = FormatEngineRegistry.get_engine(engine_name)
Â  Â  Â  Â  Â  Â  if engine is None:
Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"Format engine '{engine_name}' not found or not registered")

Â  Â  Â  Â  Â  Â  # éªŒè¯æ–‡ä»¶æ˜¯å¦é€‚åˆè¯¥å¼•æ“
Â  Â  Â  Â  Â  Â  if not engine.validate_file(file_path):
Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"File '{file_path}' is not supported by '{engine_name}' engine. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Supported extensions: {', '.join(engine.SUPPORTED_EXTENSIONS)}"
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # ä½¿ç”¨æŒ‡å®šå¼•æ“å¤„ç†
Â  Â  Â  Â  Â  Â  result = engine.parse(file_path, options={"language": lang})
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # è‡ªåŠ¨é€‰æ‹©å¼•æ“ï¼ˆæ ¹æ®æ–‡ä»¶æ‰©å±•åï¼‰
Â  Â  Â  Â  Â  Â  engine = FormatEngineRegistry.get_engine_by_extension(file_path)
Â  Â  Â  Â  Â  Â  if engine is None:
Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"No format engine available for file: {file_path}")

Â  Â  Â  Â  Â  Â  result = engine.parse(file_path, options={"language": lang})

Â  Â  Â  Â  # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸“å±è¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
Â  Â  Â  Â  output_dir = Path(self.output_dir) / Path(file_path).stem
Â  Â  Â  Â  output_dir.mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â  # ä¿å­˜ç»“æœï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´çš„å‘½åè§„èŒƒï¼‰
Â  Â  Â  Â  # ä¸»ç»“æœæ–‡ä»¶ï¼šresult.md å’Œ result.json
Â  Â  Â  Â  output_file = output_dir / "result.md"
Â  Â  Â  Â  output_file.write_text(result["markdown"], encoding="utf-8")
Â  Â  Â  Â  logger.info("ğŸ“„ Main result saved: result.md")

Â  Â  Â  Â  # å¤‡ä»½æ–‡ä»¶ï¼šä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆä¾¿äºè°ƒè¯•ï¼‰
Â  Â  Â  Â  backup_md_file = output_dir / f"{Path(file_path).stem}_{result['format']}.md"
Â  Â  Â  Â  backup_md_file.write_text(result["markdown"], encoding="utf-8")
Â  Â  Â  Â  logger.info(f"ğŸ“„ Backup saved: {backup_md_file.name}")

Â  Â  Â  Â  # ä¹Ÿä¿å­˜ JSON ç»“æ„åŒ–æ•°æ®
Â  Â  Â  Â  json_file = output_dir / "result.json"
Â  Â  Â  Â  json_file.write_text(json.dumps(result["json_content"], indent=2, ensure_ascii=False), encoding="utf-8")
Â  Â  Â  Â  logger.info("ğŸ“„ Main JSON saved: result.json")

Â  Â  Â  Â  # å¤‡ä»½ JSON æ–‡ä»¶
Â  Â  Â  Â  backup_json_file = output_dir / f"{Path(file_path).stem}_{result['format']}.json"
Â  Â  Â  Â  backup_json_file.write_text(json.dumps(result["json_content"], indent=2, ensure_ascii=False), encoding="utf-8")
Â  Â  Â  Â  logger.info(f"ğŸ“„ Backup JSON saved: {backup_json_file.name}")

Â  Â  Â  Â  # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
Â  Â  Â  Â  # Format Engine å·²ç»è¾“å‡ºæ ‡å‡†æ ¼å¼ï¼Œä½†ä»ç„¶è°ƒç”¨è§„èŒƒåŒ–å™¨ä»¥ç¡®ä¿ä¸€è‡´æ€§
Â  Â  Â  Â  normalize_output(output_dir)

Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "result_path": str(output_dir),Â  # è¿”å›ä»»åŠ¡ä¸“å±ç›®å½•
Â  Â  Â  Â  Â  Â  "content": result["content"],
Â  Â  Â  Â  Â  Â  "json_path": str(json_file),
Â  Â  Â  Â  Â  Â  "json_content": result["json_content"],
Â  Â  Â  Â  }

Â  Â  def decode_request(self, request):
Â  Â  Â  Â  """
Â  Â  Â  Â  è§£ç è¯·æ±‚

Â  Â  Â  Â  LitServe ä¼šè°ƒç”¨è¿™ä¸ªæ–¹æ³•æ¥è§£æè¯·æ±‚
Â  Â  Â  Â  æˆ‘ä»¬çš„è¯·æ±‚æ ¼å¼: {"action": "health" | "poll"}
Â  Â  Â  Â  """
Â  Â  Â  Â  return request.get("action", "health")

Â  Â  def predict(self, action):
Â  Â  Â  Â  """
Â  Â  Â  Â  å¤„ç†è¯·æ±‚

Â  Â  Â  Â  Args:
Â  Â  Â  Â  Â  Â  action: è¯·æ±‚åŠ¨ä½œ
Â  Â  Â  Â  Â  Â  Â  Â  - "health": å¥åº·æ£€æŸ¥
Â  Â  Â  Â  Â  Â  Â  Â  - "poll": æ‰‹åŠ¨æ‹‰å–ä»»åŠ¡ï¼ˆå½“ worker loop ç¦ç”¨æ—¶ï¼‰

Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  å“åº”å­—å…¸
Â  Â  Â  Â  """
Â  Â  Â  Â  if action == "health":
Â  Â  Â  Â  Â  Â  # å¥åº·æ£€æŸ¥
Â  Â  Â  Â  Â  Â  vram_gb = None
Â  Â  Â  Â  Â  Â  if "cuda" in str(self.device).lower():
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  vram_gb = get_vram(self.device.split(":")[-1])
Â  Â  Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  Â  Â  "status": "healthy",
Â  Â  Â  Â  Â  Â  Â  Â  "worker_id": self.worker_id,
Â  Â  Â  Â  Â  Â  Â  Â  "device": str(self.device),
Â  Â  Â  Â  Â  Â  Â  Â  "vram_gb": vram_gb,
Â  Â  Â  Â  Â  Â  Â  Â  "running": self.running,
Â  Â  Â  Â  Â  Â  Â  Â  "current_task": self.current_task_id,
Â  Â  Â  Â  Â  Â  Â  Â  "worker_loop_enabled": self.enable_worker_loop,
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  elif action == "poll":
Â  Â  Â  Â  Â  Â  # æ‰‹åŠ¨æ‹‰å–ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•æˆ–ç¦ç”¨ worker loop æ—¶ï¼‰
Â  Â  Â  Â  Â  Â  if self.enable_worker_loop:
Â  Â  Â  Â  Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "status": "skipped",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "message": "Worker is in auto-loop mode, manual polling is disabled",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "worker_id": self.worker_id,
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  task = self.task_db.pull_task()
Â  Â  Â  Â  Â  Â  if task:
Â  Â  Â  Â  Â  Â  Â  Â  task_id = task["task_id"]
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“¥ {self.worker_id} manually pulled task: {task_id}")

Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self._process_task(task)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… {self.worker_id} completed task: {task_id}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return {"status": "completed", "task_id": task["task_id"], "worker_id": self.worker_id}
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "status": "failed",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "task_id": task["task_id"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "error": str(e),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "worker_id": self.worker_id,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # Worker å¾ªç¯æ¨¡å¼ï¼šè¿”å›çŠ¶æ€ä¿¡æ¯
Â  Â  Â  Â  Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "status": "auto_mode",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "message": "Worker is running in auto-loop mode, tasks are processed automatically",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "worker_id": self.worker_id,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "worker_running": self.running,
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  Â  Â  "status": "error",
Â  Â  Â  Â  Â  Â  Â  Â  "message": f'Invalid action: {action}. Use "health" or "poll".',
Â  Â  Â  Â  Â  Â  Â  Â  "worker_id": self.worker_id,
Â  Â  Â  Â  Â  Â  }

Â  Â  def encode_response(self, response):
Â  Â  Â  Â  """ç¼–ç å“åº”"""
Â  Â  Â  Â  return response

Â  Â  def teardown(self):
Â  Â  Â  Â  """æ¸…ç†èµ„æºï¼ˆWorker å…³é—­æ—¶è°ƒç”¨ï¼‰"""
Â  Â  Â  Â  # è·å– worker_idï¼ˆå¯èƒ½åœ¨ setup å¤±è´¥æ—¶æœªåˆå§‹åŒ–ï¼‰
Â  Â  Â  Â  worker_id = getattr(self, "worker_id", "unknown")

Â  Â  Â  Â  logger.info(f"ğŸ›‘ Worker {worker_id} shutting down...")

Â  Â  Â  Â  # è®¾ç½® running æ ‡å¿—ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
Â  Â  Â  Â  if hasattr(self, "running"):
Â  Â  Â  Â  Â  Â  self.running = False

Â  Â  Â  Â  # ç­‰å¾… worker çº¿ç¨‹ç»“æŸ
Â  Â  Â  Â  if hasattr(self, "worker_thread") and self.worker_thread.is_alive():
Â  Â  Â  Â  Â  Â  self.worker_thread.join(timeout=5)

Â  Â  Â  Â  logger.info(f"âœ… Worker {worker_id} stopped")


def start_litserve_workers(
Â  Â  output_dir=None,Â  # é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
Â  Â  accelerator="auto",
Â  Â  devices="auto",
Â  Â  workers_per_device=1,
Â  Â  port=8001,
Â  Â  poll_interval=0.5,
Â  Â  enable_worker_loop=True,
Â  Â  paddleocr_vl_vllm_engine_enabled=False,
Â  Â  paddleocr_vl_vllm_api_list=[],
Â  Â  mineru_vllm_api_list=[],Â  # æ–°å¢å‚æ•°
):
Â  Â  """
Â  Â  å¯åŠ¨ LitServe Worker Pool

Â  Â  Args:
Â  Â  Â  Â  output_dir: è¾“å‡ºç›®å½•
Â  Â  Â  Â  accelerator: åŠ é€Ÿå™¨ç±»å‹ (auto/cuda/cpu/mps)
Â  Â  Â  Â  devices: ä½¿ç”¨çš„è®¾å¤‡ (auto/[0,1,2])
Â  Â  Â  Â  workers_per_device: æ¯ä¸ª GPU çš„ worker æ•°é‡
Â  Â  Â  Â  port: æœåŠ¡ç«¯å£
Â  Â  Â  Â  poll_interval: Worker æ‹‰å–ä»»åŠ¡çš„é—´éš”ï¼ˆç§’ï¼‰
Â  Â  Â  Â  enable_worker_loop: æ˜¯å¦å¯ç”¨ worker è‡ªåŠ¨å¾ªç¯æ‹‰å–ä»»åŠ¡
Â  Â  Â  Â  paddleocr_vl_vllm_engine_enabled: æ˜¯å¦å¯ç”¨ PaddleOCR VL VLLM å¼•æ“
Â  Â  Â  Â  paddleocr_vl_vllm_api_list: PaddleOCR VL VLLM API åˆ—è¡¨
Â  Â  Â  Â  mineru_vllm_api_list: MinerU VLLM API åˆ—è¡¨
Â  Â  """

Â  Â  def resolve_auto_accelerator():
Â  Â  Â  Â  """
Â  Â  Â  Â  å½“ accelerator è®¾ç½®ä¸º "auto" æ—¶ï¼Œä½¿ç”¨å…ƒæ•°æ®åŠç¯å¢ƒä¿¡æ¯è‡ªåŠ¨æ£€æµ‹æœ€åˆé€‚çš„åŠ é€Ÿå™¨ç±»å‹(ä¸ç›´æ¥å¯¼å…¥torch)

Â  Â  Â  Â  Returns:
Â  Â  Â  Â  Â  Â  str: æ£€æµ‹åˆ°çš„åŠ é€Ÿå™¨ç±»å‹ ("cuda" æˆ– "cpu")
Â  Â  Â  Â  """
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  from importlib.metadata import distribution

Â  Â  Â  Â  Â  Â  distribution("torch")
Â  Â  Â  Â  Â  Â  torch_is_installed = True
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  torch_is_installed = False
Â  Â  Â  Â  Â  Â  logger.warning(f"Torch is not installed or cannot be imported: {e}")

Â  Â  Â  Â  if torch_is_installed and check_cuda_with_nvidia_smi() > 0:
Â  Â  Â  Â  Â  Â  return "cuda"
Â  Â  Â  Â  return "cpu"

Â  Â  # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
Â  Â  if output_dir is None:
Â  Â  Â  Â  project_root = Path(__file__).parent.parent
Â  Â  Â  Â  default_output = project_root / "data" / "output"
Â  Â  Â  Â  output_dir = os.getenv("OUTPUT_PATH", str(default_output))

Â  Â  logger.info("=" * 60)
Â  Â  logger.info("ğŸš€ Starting MinerU Tianshu LitServe Worker Pool")
Â  Â  logger.info("=" * 60)
Â  Â  logger.info(f"ğŸ“‚ Output Directory: {output_dir}")
Â  Â  logger.info(f"ğŸ’¾ Devices: {devices}")
Â  Â  logger.info(f"ğŸ‘· Workers per Device: {workers_per_device}")
Â  Â  logger.info(f"ğŸ”Œ Port: {port}")
Â  Â  logger.info(f"ğŸ”„ Worker Loop: {'Enabled' if enable_worker_loop else 'Disabled'}")
Â  Â  if enable_worker_loop:
Â  Â  Â  Â  logger.info(f"â±ï¸Â  Poll Interval: {poll_interval}s")
Â  Â  logger.info(f"ğŸ® Initial Accelerator setting: {accelerator}")

Â  Â  if paddleocr_vl_vllm_engine_enabled:
Â  Â  Â  Â  if not paddleocr_vl_vllm_api_list:
Â  Â  Â  Â  Â  Â  logger.error(
Â  Â  Â  Â  Â  Â  Â  Â  "è¯·é…ç½® --paddleocr-vl-vllm-api-list å‚æ•°ï¼Œæˆ–ç§»é™¤ --paddleocr-vl-vllm-engine-enabled ä»¥ç¦ç”¨ PaddleOCR VL VLLM å¼•æ“"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  sys.exit(1)
Â  Â  Â  Â  logger.success(f"PaddleOCR VL VLLM å¼•æ“å·²å¯ç”¨ï¼ŒAPI åˆ—è¡¨ä¸º: {paddleocr_vl_vllm_api_list}")
Â  Â  else:
Â  Â  Â  Â  os.environ.pop("PADDLEOCR_VL_VLLM_ENABLED", None)
Â  Â  Â  Â  logger.info("PaddleOCR VL VLLM å¼•æ“å·²ç¦ç”¨")

Â  Â  logger.info("=" * 60)

Â  Â  # 1. å®ä¾‹åŒ– API æ—¶ä¼ å…¥æ•°æ®
Â  Â  api = MinerUWorkerAPI(
Â  Â  Â  Â  output_dir=output_dir,
Â  Â  Â  Â  poll_interval=poll_interval,
Â  Â  Â  Â  enable_worker_loop=enable_worker_loop,
Â  Â  Â  Â  paddleocr_vl_vllm_engine_enabled=paddleocr_vl_vllm_engine_enabled,
Â  Â  Â  Â  paddleocr_vl_vllm_api_list=paddleocr_vl_vllm_api_list,
Â  Â  Â  Â  mineru_vllm_api_list=mineru_vllm_api_list,Â  # âœ… ä¼ å…¥ MinerU API åˆ—è¡¨
Â  Â  )

Â  Â  if accelerator == "auto":
Â  Â  Â  Â  # æ‰‹åŠ¨è§£æacceleratorçš„å…·ä½“è®¾ç½®
Â  Â  Â  Â  accelerator = resolve_auto_accelerator()
Â  Â  Â  Â  logger.info(f"ğŸ’« Auto-resolved Accelerator: {accelerator}")

Â  Â  server = ls.LitServer(
Â  Â  Â  Â  api,
Â  Â  Â  Â  accelerator=accelerator,
Â  Â  Â  Â  devices=devices,
Â  Â  Â  Â  workers_per_device=workers_per_device,
Â  Â  Â  Â  timeout=False,Â  # ä¸è®¾ç½®è¶…æ—¶
Â  Â  )

Â  Â  # æ³¨å†Œä¼˜é›…å…³é—­å¤„ç†å™¨
Â  Â  def graceful_shutdown(signum=None, frame=None):
Â  Â  Â  Â  """å¤„ç†å…³é—­ä¿¡å·ï¼Œä¼˜é›…åœ°åœæ­¢ worker"""
Â  Â  Â  Â  logger.info("ğŸ›‘ Received shutdown signal, gracefully stopping workers...")
Â  Â  Â  Â  # æ³¨æ„ï¼šLitServe ä¼šä¸ºæ¯ä¸ªè®¾å¤‡åˆ›å»ºå¤šä¸ª worker å®ä¾‹
Â  Â  Â  Â  # è¿™é‡Œçš„ api åªæ˜¯æ¨¡æ¿ï¼Œå®é™…çš„ worker å®ä¾‹ç”± LitServe ç®¡ç†
Â  Â  Â  Â  # teardown ä¼šåœ¨æ¯ä¸ª worker è¿›ç¨‹ä¸­è¢«è°ƒç”¨
Â  Â  Â  Â  if hasattr(api, "teardown"):
Â  Â  Â  Â  Â  Â  api.teardown()
Â  Â  Â  Â  sys.exit(0)

Â  Â  # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆCtrl+C ç­‰ï¼‰
Â  Â  signal.signal(signal.SIGINT, graceful_shutdown)
Â  Â  signal.signal(signal.SIGTERM, graceful_shutdown)

Â  Â  # æ³¨å†Œ atexit å¤„ç†å™¨ï¼ˆæ­£å¸¸é€€å‡ºæ—¶è°ƒç”¨ï¼‰
Â  Â  atexit.register(lambda: api.teardown() if hasattr(api, "teardown") else None)

Â  Â  logger.info("âœ… LitServe worker pool initialized")
Â  Â  logger.info(f"ğŸ“¡ Listening on: http://0.0.0.0:{port}/predict")
Â  Â  if enable_worker_loop:
Â  Â  Â  Â  logger.info("ğŸ” Workers will continuously poll and process tasks")
Â  Â  else:
Â  Â  Â  Â  logger.info("ğŸ”„ Workers will wait for scheduler triggers")
Â  Â  logger.info("=" * 60)

Â  Â  # å¯åŠ¨æœåŠ¡å™¨
Â  Â  # æ³¨æ„ï¼šLitServe å†…ç½® MCP å·²é€šè¿‡ monkeypatch å®Œå…¨ç¦ç”¨ï¼ˆæˆ‘ä»¬æœ‰ç‹¬ç«‹çš„ MCP Serverï¼‰
Â  Â  server.run(port=port, generate_client_file=False)


if __name__ == "__main__":
Â  Â  import argparse

Â  Â  parser = argparse.ArgumentParser(description="MinerU Tianshu LitServe Worker Pool")
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--output-dir",
Â  Â  Â  Â  type=str,
Â  Â  Â  Â  default=None,
Â  Â  Â  Â  help="Output directory for processed files (default: from OUTPUT_PATH env or /app/output)",
Â  Â  )
Â  Â  parser.add_argument("--port", type=int, default=8001, help="Server port (default: 8001, or from WORKER_PORT env)")
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--accelerator",
Â  Â  Â  Â  type=str,
Â  Â  Â  Â  default="auto",
Â  Â  Â  Â  choices=["auto", "cuda", "cpu"],
Â  Â  Â  Â  help="Accelerator type (default: auto)",
Â  Â  )
Â  Â  parser.add_argument("--workers-per-device", type=int, default=1, help="Number of workers per device (default: 1)")
Â  Â  parser.add_argument("--devices", type=str, default="auto", help="Devices to use, comma-separated (default: auto)")
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--poll-interval", type=float, default=0.5, help="Worker poll interval in seconds (default: 0.5)"
Â  Â  )
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--disable-worker-loop",
Â  Â  Â  Â  action="store_true",
Â  Â  Â  Â  help="Disable automatic worker loop (workers will wait for manual triggers)",
Â  Â  )
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--paddleocr-vl-vllm-engine-enabled",
Â  Â  Â  Â  action="store_true",
Â  Â  Â  Â  default=False,
Â  Â  Â  Â  help="æ˜¯å¦å¯ç”¨ PaddleOCR VL VLLM å¼•æ“ (é»˜è®¤: False)",
Â  Â  )
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--paddleocr-vl-vllm-api-list",
Â  Â  Â  Â  type=parse_list_arg,
Â  Â  Â  Â  default=[],
Â  Â  Â  Â  help='PaddleOCR VL VLLM API åˆ—è¡¨ï¼ˆPython list å­—é¢é‡æ ¼å¼ï¼Œå¦‚: \'["http://127.0.0.1:8000/v1", "http://127.0.0.1:8001/v1"]\'ï¼‰',
Â  Â  )
Â  Â  # æ–°å¢å‚æ•°å®šä¹‰
Â  Â  parser.add_argument(
Â  Â  Â  Â  "--mineru-vllm-api-list",
Â  Â  Â  Â  type=parse_list_arg,
Â  Â  Â  Â  default=[],
Â  Â  Â  Â  help='MinerU VLLM API åˆ—è¡¨ï¼ˆPython list å­—é¢é‡æ ¼å¼ï¼Œå¦‚: \'["http://127.0.0.1:30024/v1"]\'ï¼‰',
Â  Â  )
Â  Â  args = parser.parse_args()

Â  Â  # ============================================================================
Â  Â  # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆå¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æŒ‡å®šï¼‰
Â  Â  # ============================================================================
Â  Â  # 1. å¦‚æœæ²¡æœ‰é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®š devicesï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹æˆ–ä»ç¯å¢ƒå˜é‡è¯»å–
Â  Â  devices = args.devices
Â  Â  if devices == "auto":
Â  Â  Â  Â  # é¦–å…ˆå°è¯•ä»ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES è¯»å–ï¼ˆå¦‚æœç”¨æˆ·æ˜ç¡®è®¾ç½®äº†ï¼‰
Â  Â  Â  Â  env_devices = os.getenv("CUDA_VISIBLE_DEVICES")
Â  Â  Â  Â  if env_devices and env_devices.strip():
Â  Â  Â  Â  Â  Â  devices = env_devices
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Using devices from CUDA_VISIBLE_DEVICES: {devices}")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ CUDA è®¾å¤‡
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  import torch

Â  Â  Â  Â  Â  Â  Â  Â  if torch.cuda.is_available():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  device_count = torch.cuda.device_count()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  devices = ",".join(str(i) for i in range(device_count))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Auto-detected {device_count} CUDA devices: {devices}")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("ğŸ“Š No CUDA devices available, using CPU mode")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  devices = "auto"Â  # ä¿æŒ autoï¼Œè®© LitServe ä½¿ç”¨ CPU
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Failed to detect CUDA devices: {e}, using CPU mode")
Â  Â  Â  Â  Â  Â  Â  Â  devices = "auto"

Â  Â  # 2. å¤„ç† devices å‚æ•°ï¼ˆæ”¯æŒé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰
Â  Â  if devices != "auto":
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  devices = [int(d.strip()) for d in devices.split(",")]
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Parsed devices: {devices}")
Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Invalid devices format: {devices}. Use comma-separated integers (e.g., '0,1,2')")
Â  Â  Â  Â  Â  Â  sys.exit(1)

Â  Â  # 3. å¦‚æœæ²¡æœ‰é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®š workers-per-deviceï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡ WORKER_GPUS è¯»å–
Â  Â  workers_per_device = args.workers_per_device
Â  Â  if args.workers_per_device == 1:Â  # é»˜è®¤å€¼
Â  Â  Â  Â  env_workers = os.getenv("WORKER_GPUS")
Â  Â  Â  Â  if env_workers:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  workers_per_device = int(env_workers)
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Using workers-per-device from WORKER_GPUS: {workers_per_device}")
Â  Â  Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Invalid WORKER_GPUS value: {env_workers}, using default: 1")

Â  Â  # 4. å¦‚æœæ²¡æœ‰é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®š portï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡ WORKER_PORT è¯»å–
Â  Â  port = args.port
Â  Â  if args.port == 8001:Â  # é»˜è®¤å€¼
Â  Â  Â  Â  env_port = os.getenv("WORKER_PORT", "8001")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  port = int(env_port)
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Using port from WORKER_PORT env: {port}")
Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸Â  Invalid WORKER_PORT value: {env_port}, using default: 8001")
Â  Â  Â  Â  Â  Â  port = 8001

Â  Â  start_litserve_workers(
Â  Â  Â  Â  output_dir=args.output_dir,
Â  Â  Â  Â  accelerator=args.accelerator,
Â  Â  Â  Â  devices=devices,
Â  Â  Â  Â  workers_per_device=workers_per_device,
Â  Â  Â  Â  port=port,
Â  Â  Â  Â  poll_interval=args.poll_interval,
Â  Â  Â  Â  enable_worker_loop=not args.disable_worker_loop,
Â  Â  Â  Â  paddleocr_vl_vllm_engine_enabled=args.paddleocr_vl_vllm_engine_enabled,
Â  Â  Â  Â  paddleocr_vl_vllm_api_list=args.paddleocr_vl_vllm_api_list,
Â  Â  Â  Â  mineru_vllm_api_list=args.mineru_vllm_api_list, # âœ… ä¼ é€’å‚æ•°
Â  Â  )
