"""
MinerU Pipeline Engine
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
ä½¿ç”¨ MinerU å¤„ç† PDF å’Œå›¾ç‰‡

æ›´æ–°æ—¥å¿— (2026-02-15):
- [æ–°å¢] æ™ºèƒ½æ˜¾å­˜ä¼‘çœ æœºåˆ¶ (Auto-Sleep): ç©ºé—² 5 åˆ†é’Ÿè‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜
- [æ–°å¢] è‡ªåŠ¨å”¤é†’æœºåˆ¶ (Auto-Wakeup): æ–°ä»»åŠ¡è‡ªåŠ¨é‡æ–°åŠ è½½æ¨¡å‹
- [ä¼˜åŒ–] ç§»é™¤æ¯æ¬¡ä»»åŠ¡åçš„å¼ºåˆ¶æ˜¾å­˜æ¸…ç†ï¼Œæå‡è¿ç»­å¤„ç†æ€§èƒ½
- [æ ¸å¿ƒä¿®å¤] æ·±åº¦æ–‡æœ¬æ¸…æ´— (åŒé‡åè½¬ä¹‰ã€å»é‡ã€æ¸…æ´— LaTeX ç¬¦å·)
- [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨ä¸´æ—¶çº¯è‹±æ–‡ç›®å½•å¤„ç†ï¼Œè§„é¿ä¸­æ–‡è·¯å¾„é—®é¢˜
"""

import json
import os
import shutil
import tempfile
import time
import urllib.request
import urllib.error
import re
import html
import gc
import threading
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger
import img2pdf

# å°è¯•å¯¼å…¥ torch ç”¨äºæ˜¾å­˜ç®¡ç†
try:
    import torch
except ImportError:
    torch = None

class MinerUPipelineEngine:
    """
    MinerU Pipeline å¼•æ“
    é›†æˆè‡ªåŠ¨æ˜¾å­˜ç®¡ç†ä¸æ·±åº¦æ¸…æ´—åŠŸèƒ½
    """

    _instance: Optional["MinerUPipelineEngine"] = None
    _lock = Lock()
    _pipeline = None  # è¿™é‡Œçš„ pipeline å®é™…ä¸Šæ˜¯ do_parse å‡½æ•°
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, device: str = "cuda:0", vlm_api_base: str = None):
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            self.vlm_api_base = vlm_api_base

            if "cuda:" in device:
                self.gpu_id = device.split(":")[-1]
            else:
                self.gpu_id = "0"

            # =========================================================
            # [æ–°å¢] æ™ºèƒ½æ˜¾å­˜ç®¡ç†çŠ¶æ€å˜é‡
            # =========================================================
            self.last_active_time = time.time()  # æœ€åæ´»åŠ¨æ—¶é—´
            self.is_processing = False           # æ˜¯å¦æ­£åœ¨å¤„ç†ä»»åŠ¡
            self.is_offloaded = True             # æ˜¯å¦å·²å¸è½½æ˜¾å­˜
            self.idle_timeout = 300              # ç©ºé—²è¶…æ—¶æ—¶é—´ (ç§’) - 5åˆ†é’Ÿ

            # å¯åŠ¨æ˜¾å­˜ç›‘æ§åå°çº¿ç¨‹
            self._monitor_thread = threading.Thread(target=self._auto_sleep_monitor, daemon=True)
            self._monitor_thread.start()
            
            self._initialized = True
            logger.info(f"ğŸ”§ MinerU Pipeline Engine initialized on {device}")
            logger.info(f"â³ Auto-sleep monitor enabled (Timeout: {self.idle_timeout}s)")
            if self.vlm_api_base:
                logger.info(f"   VLLM API Base: {self.vlm_api_base}")

    def _auto_sleep_monitor(self):
        """
        [åå°çº¿ç¨‹] ç›‘æ§ç³»ç»Ÿç©ºé—²çŠ¶æ€ï¼Œè¶…æ—¶è‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜
        """
        while True:
            time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
            try:
                # å¦‚æœ 1. æ­£åœ¨å¤„ç†ä»»åŠ¡ æˆ– 2. å·²ç»å¸è½½ï¼Œåˆ™è·³è¿‡
                if self.is_processing or self.is_offloaded:
                    continue
                
                idle_duration = time.time() - self.last_active_time
                if idle_duration > self.idle_timeout:
                    logger.info(f"ğŸ’¤ System idle for {idle_duration:.0f}s. Unloading models to save VRAM...")
                    self.cleanup() # æ‰§è¡Œæ¸…ç†
                    self.is_offloaded = True # æ ‡è®°ä¸ºå·²å¸è½½
            except Exception as e:
                logger.error(f"Error in auto-sleep monitor: {e}")

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ MinerU ç®¡é“ (do_parse)"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            logger.info("=" * 60)
            logger.info("ğŸ“¥ Loading MinerU Pipeline (do_parse)...")
            logger.info("=" * 60)

            try:
                from mineru.cli.common import do_parse
                self._pipeline = do_parse
                logger.info("âœ… MinerU Pipeline loaded successfully!")
                return self._pipeline
            except Exception as e:
                logger.error(f"âŒ Error loading MinerU pipeline: {e}")
                raise

    def _wait_for_server(self, server_url: str, timeout: int = 60) -> bool:
        """ç­‰å¾… VLLM æœåŠ¡å°±ç»ª"""
        base_url = server_url.rstrip("/")
        if base_url.endswith("/v1"):
            base_url = base_url[:-3]
            
        health_url = f"{base_url}/v1/models"
        
        logger.info(f"â³ Waiting for VLLM server at {base_url} (Timeout: {timeout}s)...")
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                with urllib.request.urlopen(health_url, timeout=2) as response:
                    if response.status == 200:
                        logger.info(f"âœ… VLLM server is ready: {base_url}")
                        return True
            except (urllib.error.URLError, ConnectionRefusedError):
                pass
            except Exception as e:
                logger.debug(f"Health check warning: {e}")
            
            time.sleep(1)
            
        logger.warning(f"âš ï¸  VLLM server wait timed out after {timeout}s. Process may fail.")
        return False

    def _clean_markdown(self, text: str) -> str:
        """
        [å…³é”®åŠŸèƒ½] æ·±åº¦æ¸…æ´— Markdown æ–‡æœ¬
        è§£å†³ HTML è½¬ä¹‰ã€LaTeX è¿‡åº¦åŒ…è£…ã€éæ¢è¡Œç©ºæ ¼å’Œé‡å¤å†…å®¹é—®é¢˜
        """
        if not text:
            return ""

        if "117" in text or "LVEDd" in text:
            logger.debug(f"ğŸ§¹ Executing _clean_markdown... (Length: {len(text)})")

        # 1. HTML åè½¬ä¹‰ (æ‰§è¡Œä¸¤æ¬¡ä»¥è§£å†³ &amp;gt; è¿™ç§åŒé‡è½¬ä¹‰é—®é¢˜)
        text = html.unescape(text)
        text = html.unescape(text)

        # 2. æš´åŠ›æ›¿æ¢å¸¸è§çš„æœªè½¬ä¹‰å­—ç¬¦
        text = text.replace('&gt;', '>').replace('&lt;', '<').replace('&amp;', '&')

        # 3. å»é™¤ LaTeX çš„ \mathrm{} åŒ…è£…
        text = re.sub(r'\\mathrm\{(.*?)\}', r'\1', text, flags=re.DOTALL)

        # 4. æ¸…æ´— LaTeX ç‰¹æ®Šå­—ç¬¦
        text = text.replace('~', ' ')
        
        # 5. å»é™¤æ¨¡å‹å¹»è§‰äº§ç”Ÿçš„ <del> æ ‡ç­¾
        text = text.replace('<del>', '').replace('</del>', '')
        
        # 6. [åŠ å¼ºç‰ˆ] æš´åŠ›å»é‡é€»è¾‘ 
        text = re.sub(r'(\S+)([\s\r\n]+)\1', r'\1', text)

        # 7. å»é™¤è¿ç»­çš„å¤šä½™ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text

    def cleanup(self):
        """
        [å¢å¼ºç‰ˆ] æ¸…ç†æ˜¾å­˜ä¸æ¨¡å‹
        ç”¨äº Auto-Sleep æˆ–ç¨‹åºé€€å‡ºæ—¶
        """
        with self._lock:
            logger.info("ğŸ§¹ Starting memory cleanup...")
            try:
                from mineru.utils.model_utils import clean_memory
                clean_memory()
            except Exception:
                pass
            
            # å¼ºåˆ¶ GC ä¸ CUDA ç¼“å­˜æ¸…ç†
            try:
                self._pipeline = None # é‡Šæ”¾å‡½æ•°å¼•ç”¨ï¼Œä¿ƒä½¿ä¸‹æ¬¡é‡æ–°åŠ è½½
                gc.collect()
                if torch and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                logger.info("âœ… GPU Memory released completely.")
            except Exception as e:
                logger.warning(f"Hard cleanup warning: {e}")

    def parse(self, file_path: str, output_path: str, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶ (å¢å¼ºç‰ˆï¼šè‡ªåŠ¨å”¤é†’ + ä¸´æ—¶ç›®å½• + æ·±åº¦æ¸…æ´—)
        """
        # =========================================================
        # 1. çŠ¶æ€æ›´æ–°ä¸è‡ªåŠ¨å”¤é†’ (Auto-Wakeup)
        # =========================================================
        self.is_processing = True
        self.last_active_time = time.time()
        
        if self.is_offloaded:
            logger.info("ğŸš€ New task received. Waking up models (Auto-Wakeup)...")
            self.is_offloaded = False
            # æ³¨æ„ï¼šä¸‹æ–¹çš„ _load_pipeline() ä¼šè‡ªåŠ¨å¤„ç†é‡æ–°åŠ è½½é€»è¾‘
        
        try:
            options = options or {}
            
            final_output_dir = Path(output_path)
            final_output_dir.mkdir(parents=True, exist_ok=True)

            file_path_obj = Path(file_path)
            file_ext = file_path_obj.suffix.lower()

            # 1. ç¡®å®š Backend
            user_backend = options.get("parse_mode", "pipeline")
            if user_backend == "auto":
                user_backend = "pipeline"

            backend = user_backend
            server_url = options.get("server_url")

            # æ™ºèƒ½åˆ‡æ¢ VLLM
            if not server_url and self.vlm_api_base:
                if user_backend == "vlm-auto-engine":
                    backend = "vlm-http-client"
                    server_url = self.vlm_api_base.replace("/v1", "")
                    logger.info(f"ğŸ”„ [Accelerate] Switching to {backend} using local vLLM")
                elif user_backend == "hybrid-auto-engine":
                    backend = "hybrid-http-client"
                    server_url = self.vlm_api_base.replace("/v1", "")
                    logger.info(f"ğŸ”„ [Accelerate] Switching to {backend} using local vLLM")

            # æœåŠ¡å¥åº·æ£€æŸ¥
            if "http-client" in backend and server_url:
                self._wait_for_server(server_url)

            # 2. å‡†å¤‡å‚æ•°
            parse_method = options.get("method", "auto")
            if options.get("force_ocr"):
                parse_method = "ocr"

            formula_enable = options.get("formula_enable", True)
            table_enable = options.get("table_enable", True)
            
            f_draw_layout_bbox = options.get("draw_layout_bbox", True)      
            f_draw_span_bbox = options.get("draw_span_bbox", True)          
            f_dump_md = options.get("dump_markdown", True)                  
            f_dump_middle_json = options.get("dump_middle_json", True)      
            f_dump_model_output = options.get("dump_model_output", True)    
            f_dump_content_list = options.get("dump_content_list", True)    
            f_dump_orig_pdf = options.get("dump_orig_pdf", True)            

            start_page_id = options.get("start_page_id", 0)
            end_page_id = options.get("end_page_id", None)
            
            try: start_page_id = int(start_page_id)
            except: start_page_id = 0
            
            try: 
                if end_page_id is not None and str(end_page_id).strip() != "": 
                    end_page_id = int(end_page_id)
                    if end_page_id == -1: end_page_id = None
                else: end_page_id = None
            except: end_page_id = None

            do_parse_func = self._load_pipeline()

            with open(file_path, "rb") as f:
                file_bytes = f.read()

            if file_ext in [".png", ".jpg", ".jpeg"]:
                logger.info("ğŸ–¼ï¸  Converting image to PDF...")
                try:
                    pdf_bytes = img2pdf.convert(file_bytes)
                except Exception as e:
                    raise ValueError(f"Image conversion failed: {e}")
            else:
                pdf_bytes = file_bytes

            lang = options.get("lang", "auto")
            if lang == "auto": lang = "ch"

            # ä½¿ç”¨ä¸´æ—¶çº¯è‹±æ–‡ç›®å½•å¤„ç†
            with tempfile.TemporaryDirectory(prefix="mineru_proc_") as temp_dir:
                temp_work_dir = Path(temp_dir)
                logger.info(f"ğŸ› ï¸  Working in temp directory: {temp_work_dir}")
                
                safe_file_name = "result.pdf"
                
                do_parse_func(
                    output_dir=str(temp_work_dir),
                    pdf_file_names=[safe_file_name],
                    pdf_bytes_list=[pdf_bytes],
                    p_lang_list=[lang],
                    
                    backend=backend,
                    parse_method=parse_method,
                    server_url=server_url,
                    
                    start_page_id=start_page_id,
                    end_page_id=end_page_id,
                    formula_enable=formula_enable,
                    table_enable=table_enable,
                    
                    f_draw_layout_bbox=f_draw_layout_bbox,
                    f_draw_span_bbox=f_draw_span_bbox,
                    f_dump_md=f_dump_md,
                    f_dump_middle_json=f_dump_middle_json,
                    f_dump_model_output=f_dump_model_output,
                    f_dump_orig_pdf=f_dump_orig_pdf,
                    f_dump_content_list=f_dump_content_list
                )

                # ç»“æœæå–ä¸æ¬è¿
                generated_result_dir = temp_work_dir / "result"
                
                if not generated_result_dir.exists():
                    temp_md_files = list(temp_work_dir.rglob("*.md"))
                    if temp_md_files:
                        generated_result_dir = temp_md_files[0].parent.parent
                    else:
                        temp_json_files = list(temp_work_dir.rglob("*_content_list.json"))
                        if temp_json_files:
                             generated_result_dir = temp_json_files[0].parent.parent
                        else:
                             raise FileNotFoundError("Processing failed internally - No output generated")

                # 1. è¯»å–å†…å®¹å¹¶è¿›è¡Œæ·±åº¦æ¸…æ´—
                content = ""
                json_content = None
                
                temp_md_files = list(generated_result_dir.rglob("*.md"))
                if temp_md_files:
                    md_file = temp_md_files[0]
                    raw_content = md_file.read_text(encoding="utf-8")
                    
                    # æ·±åº¦æ¸…æ´—
                    content = self._clean_markdown(raw_content)
                    
                    # è¦†ç›–å†™å…¥æ¸…æ´—åçš„å†…å®¹
                    md_file.write_text(content, encoding="utf-8")
                    
                    logger.info(f"âœ… Read and cleaned MD content: {len(content)} chars")
                
                temp_json_files = list(generated_result_dir.rglob("*_content_list.json"))
                if temp_json_files:
                    try:
                        with open(temp_json_files[0], "r", encoding="utf-8") as f:
                            json_content = json.load(f)
                    except: pass

                # 2. å¦‚æœ MD ä¸ºç©ºï¼Œå°è¯•ä» JSON æ¢å¤
                if not content.strip() and json_content:
                    logger.warning("âš ï¸  Markdown file is empty, attempting to recover text from JSON...")
                    recovered_text = []
                    if isinstance(json_content, list):
                        for block in json_content:
                            text = block.get("text", "")
                            text = self._clean_markdown(text)
                            recovered_text.append(text)
                    content = "\n\n".join(recovered_text)
                    
                    if temp_md_files:
                        temp_md_files[0].write_text(content, encoding="utf-8")
                        
                    logger.info(f"â„¹ï¸  Recovered {len(content)} chars from JSON")

                # 3. æ¬è¿æ–‡ä»¶
                logger.info(f"ğŸ“¦ Moving results from {generated_result_dir} to {final_output_dir}")
                if generated_result_dir.exists():
                    for src_path in generated_result_dir.rglob("*"):
                        if src_path.is_file():
                            rel_path = src_path.relative_to(generated_result_dir)
                            dest_path = final_output_dir / rel_path
                            dest_path.parent.mkdir(parents=True, exist_ok=True)
                            shutil.copy2(src_path, dest_path)

                # 4. è¿”å›è·¯å¾„
                final_md_path = None
                final_json_path = None
                
                final_mds = list(final_output_dir.rglob("*.md"))
                if final_mds:
                    final_md_path = str(final_mds[0])
                    # è¦†ç›–å†™å…¥
                    Path(final_md_path).write_text(content, encoding="utf-8")
                
                final_jsons = list(final_output_dir.rglob("*_content_list.json"))
                if final_jsons: final_json_path = str(final_jsons[0])

                if not content.strip():
                    layout_pdfs = list(final_output_dir.rglob("*_layout.pdf"))
                    if layout_pdfs:
                        content = "> âš ï¸ Text extraction returned empty content. Please check layout PDF."
                    else:
                        raise FileNotFoundError("No valid content generated.")

                return {
                    "markdown": content,
                    "result_path": str(final_output_dir),
                    "markdown_file": final_md_path,
                    "json_path": final_json_path,
                    "json_content": json_content
                }

        except Exception as e:
            logger.error(f"âŒ Pipeline processing failed: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            raise

        finally:
            # =========================================================
            # [å…³é”®ä¿®æ”¹]
            # ç§»é™¤ä¹‹å‰çš„å¼ºåˆ¶ self.cleanup()
            # æ”¹ä¸ºæ›´æ–°æ´»è·ƒæ—¶é—´æˆ³ï¼Œè®©åå°çº¿ç¨‹å¤„ç†é‡Šæ”¾
            # =========================================================
            self.is_processing = False
            self.last_active_time = time.time()
            logger.info("ğŸ Task finished. Model remains loaded for fast reuse (Auto-sleep in 5min).")


# å…¨å±€å•ä¾‹
_engine = None

def get_engine(vlm_api_base: str = None) -> MinerUPipelineEngine:
    global _engine
    if _engine is None:
        _engine = MinerUPipelineEngine(vlm_api_base=vlm_api_base)
    return _engine
