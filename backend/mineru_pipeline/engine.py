"""
MinerU Pipeline Engine
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
ä½¿ç”¨ MinerU å¤„ç† PDF å’Œå›¾ç‰‡

ä¿®å¤è¯´æ˜ï¼š
- [æ ¸å¿ƒä¿®å¤] æ·±åº¦æ–‡æœ¬æ¸…æ´— (åŒé‡åè½¬ä¹‰ã€å»é‡ã€æ¸…æ´— LaTeX ç¬¦å·)
- [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨ä¸´æ—¶çº¯è‹±æ–‡ç›®å½•å¤„ç†ï¼Œè§„é¿ä¸­æ–‡è·¯å¾„é—®é¢˜
- [å¢å¼º] å¢åŠ  VLLM æœåŠ¡å¥åº·æ£€æŸ¥ä¸è‡ªåŠ¨ç­‰å¾…æœºåˆ¶
- [å¢å¼º] ä¿®å¤ Markdown å†…å®¹ä¸ºç©ºæ—¶çš„è‡ªåŠ¨æ¢å¤é€»è¾‘
"""

import json
import os
import shutil
import tempfile
import time
import urllib.request
import urllib.error
import re        # <--- æ­£åˆ™è¡¨è¾¾å¼åº“
import html      # <--- HTMLè½¬ä¹‰åº“
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger
import img2pdf


class MinerUPipelineEngine:
    """
    MinerU Pipeline å¼•æ“
    """

    _instance: Optional["MinerUPipelineEngine"] = None
    _lock = Lock()
    _pipeline = None  # è¿™é‡Œçš„ pipeline å®é™…ä¸Šæ˜¯ do_parse å‡½æ•°
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, device: str = "cuda:0", vlm_api_base: str = None):
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            self.vlm_api_base = vlm_api_base

            if "cuda:" in device:
                self.gpu_id = device.split(":")[-1]
            else:
                self.gpu_id = "0"

            self._initialized = True
            logger.info(f"ğŸ”§ MinerU Pipeline Engine initialized on {device}")
            if self.vlm_api_base:
                logger.info(f"   VLLM API Base: {self.vlm_api_base}")

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ MinerU ç®¡é“ (do_parse)"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            logger.info("=" * 60)
            logger.info("ğŸ“¥ Loading MinerU Pipeline (do_parse)...")
            logger.info("=" * 60)

            try:
                from mineru.cli.common import do_parse
                self._pipeline = do_parse
                logger.info("âœ… MinerU Pipeline loaded successfully!")
                return self._pipeline
            except Exception as e:
                logger.error(f"âŒ Error loading MinerU pipeline: {e}")
                raise

    def _wait_for_server(self, server_url: str, timeout: int = 60) -> bool:
        """ç­‰å¾… VLLM æœåŠ¡å°±ç»ª"""
        base_url = server_url.rstrip("/")
        if base_url.endswith("/v1"):
            base_url = base_url[:-3]
            
        health_url = f"{base_url}/v1/models"
        
        logger.info(f"â³ Waiting for VLLM server at {base_url} (Timeout: {timeout}s)...")
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                with urllib.request.urlopen(health_url, timeout=2) as response:
                    if response.status == 200:
                        logger.info(f"âœ… VLLM server is ready: {base_url}")
                        return True
            except (urllib.error.URLError, ConnectionRefusedError):
                pass
            except Exception as e:
                logger.debug(f"Health check warning: {e}")
            
            time.sleep(1)
            
        logger.warning(f"âš ï¸  VLLM server wait timed out after {timeout}s. Process may fail.")
        return False

    def _clean_markdown(self, text: str) -> str:
        """
        [å…³é”®åŠŸèƒ½] æ·±åº¦æ¸…æ´— Markdown æ–‡æœ¬
        è§£å†³ HTML è½¬ä¹‰ã€LaTeX è¿‡åº¦åŒ…è£…ã€éæ¢è¡Œç©ºæ ¼å’Œé‡å¤å†…å®¹é—®é¢˜
        """
        if not text:
            return ""

        # DEBUGæ—¥å¿—ï¼šå¦‚æœä½ åœ¨æ§åˆ¶å°æ²¡çœ‹åˆ°è¿™å¥è¯ï¼Œè¯´æ˜ä»£ç æ²¡ç”Ÿæ•ˆï¼ˆéœ€è¦é‡å¯æœåŠ¡ï¼‰
        if "117" in text or "LVEDd" in text:
            logger.info(f"ğŸ§¹ [DEBUG] Executing _clean_markdown... (Length: {len(text)})")

        # 1. HTML åè½¬ä¹‰ (æ‰§è¡Œä¸¤æ¬¡ä»¥è§£å†³ &amp;gt; è¿™ç§åŒé‡è½¬ä¹‰é—®é¢˜)
        text = html.unescape(text)
        text = html.unescape(text)

        # 2. æš´åŠ›æ›¿æ¢å¸¸è§çš„æœªè½¬ä¹‰å­—ç¬¦ (ä½œä¸º html.unescape çš„å…œåº•)
        # è¿™ä¸€æ­¥èƒ½è§£å†³ &gt; å˜æˆ > çš„é—®é¢˜
        text = text.replace('&gt;', '>').replace('&lt;', '<').replace('&amp;', '&')

        # 3. å»é™¤ LaTeX çš„ \mathrm{} åŒ…è£…
        # ä½¿ç”¨ flags=re.DOTALL ç¡®ä¿èƒ½å¤„ç†è·¨è¡Œå†…å®¹
        text = re.sub(r'\\mathrm\{(.*?)\}', r'\1', text, flags=re.DOTALL)

        # 4. æ¸…æ´— LaTeX ç‰¹æ®Šå­—ç¬¦
        # å°† ~ (LaTeXéæ¢è¡Œç©ºæ ¼) æ›¿æ¢ä¸ºæ™®é€šç©ºæ ¼
        # è¿™ä¸€æ­¥èƒ½è§£å†³ ~cm å˜æˆ cm çš„é—®é¢˜
        text = text.replace('~', ' ')
        
        # 5. å»é™¤æ¨¡å‹å¹»è§‰äº§ç”Ÿçš„ <del> æ ‡ç­¾
        text = text.replace('<del>', '').replace('</del>', '')
        
        # 6. [åŠ å¼ºç‰ˆ] æš´åŠ›å»é‡é€»è¾‘ 
        # è§£å†³ 117\n\n117 (æ•°å­—é‡å¤) å’Œ SD+5\n\nSD+5 (å¸¦ç¬¦å·çš„çŸ­è¯­é‡å¤)
        # é€»è¾‘ï¼šåŒ¹é…ä»»æ„éç©ºå­—ç¬¦å— (\S+)ï¼Œåé¢è·Ÿç€ç©ºç™½ç¬¦ï¼Œå†è·Ÿç€å®Œå…¨ä¸€æ ·çš„å­—ç¬¦å—
        text = re.sub(r'(\S+)([\s\r\n]+)\1', r'\1', text)

        # 7. å»é™¤è¿ç»­çš„å¤šä½™ç©ºè¡Œ (ä¿ç•™æœ€å¤šä¸¤ä¸ªæ¢è¡Œ)
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text

    def cleanup(self):
        """æ¸…ç†æ˜¾å­˜"""
        try:
            from mineru.utils.model_utils import clean_memory
            clean_memory()
            logger.debug("ğŸ§¹ MinerU: Memory cleanup completed")
        except Exception:
            pass

    def parse(self, file_path: str, output_path: str, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶ (å¢å¼ºç‰ˆï¼šä¸´æ—¶ç›®å½• + æœåŠ¡ç­‰å¾… + æ·±åº¦æ¸…æ´—)
        """
        options = options or {}
        
        final_output_dir = Path(output_path)
        final_output_dir.mkdir(parents=True, exist_ok=True)

        file_path_obj = Path(file_path)
        file_ext = file_path_obj.suffix.lower()

        # 1. ç¡®å®š Backend
        user_backend = options.get("parse_mode", "pipeline")
        if user_backend == "auto":
            user_backend = "pipeline"

        backend = user_backend
        server_url = options.get("server_url")

        # æ™ºèƒ½åˆ‡æ¢ VLLM
        if not server_url and self.vlm_api_base:
            if user_backend == "vlm-auto-engine":
                backend = "vlm-http-client"
                server_url = self.vlm_api_base.replace("/v1", "")
                logger.info(f"ğŸ”„ [Accelerate] Switching to {backend} using local vLLM")
            elif user_backend == "hybrid-auto-engine":
                backend = "hybrid-http-client"
                server_url = self.vlm_api_base.replace("/v1", "")
                logger.info(f"ğŸ”„ [Accelerate] Switching to {backend} using local vLLM")

        # æœåŠ¡å¥åº·æ£€æŸ¥
        if "http-client" in backend and server_url:
            self._wait_for_server(server_url)

        # 2. å‡†å¤‡å‚æ•°
        parse_method = options.get("method", "auto")
        if options.get("force_ocr"):
            parse_method = "ocr"

        formula_enable = options.get("formula_enable", True)
        table_enable = options.get("table_enable", True)
        
        f_draw_layout_bbox = options.get("draw_layout_bbox", True)      
        f_draw_span_bbox = options.get("draw_span_bbox", True)          
        f_dump_md = options.get("dump_markdown", True)                  
        f_dump_middle_json = options.get("dump_middle_json", True)      
        f_dump_model_output = options.get("dump_model_output", True)    
        f_dump_content_list = options.get("dump_content_list", True)    
        f_dump_orig_pdf = options.get("dump_orig_pdf", True)            

        start_page_id = options.get("start_page_id", 0)
        end_page_id = options.get("end_page_id", None)
        
        try: start_page_id = int(start_page_id)
        except: start_page_id = 0
        
        try: 
            if end_page_id is not None and str(end_page_id).strip() != "": 
                end_page_id = int(end_page_id)
                if end_page_id == -1: end_page_id = None
            else: end_page_id = None
        except: end_page_id = None

        do_parse_func = self._load_pipeline()

        try:
            with open(file_path, "rb") as f:
                file_bytes = f.read()

            if file_ext in [".png", ".jpg", ".jpeg"]:
                logger.info("ğŸ–¼ï¸  Converting image to PDF...")
                try:
                    pdf_bytes = img2pdf.convert(file_bytes)
                except Exception as e:
                    raise ValueError(f"Image conversion failed: {e}")
            else:
                pdf_bytes = file_bytes

            lang = options.get("lang", "auto")
            if lang == "auto": lang = "ch"

            # ä½¿ç”¨ä¸´æ—¶çº¯è‹±æ–‡ç›®å½•å¤„ç†
            with tempfile.TemporaryDirectory(prefix="mineru_proc_") as temp_dir:
                temp_work_dir = Path(temp_dir)
                logger.info(f"ğŸ› ï¸  Working in temp directory: {temp_work_dir}")
                
                safe_file_name = "result.pdf"
                
                do_parse_func(
                    output_dir=str(temp_work_dir),
                    pdf_file_names=[safe_file_name],
                    pdf_bytes_list=[pdf_bytes],
                    p_lang_list=[lang],
                    
                    backend=backend,
                    parse_method=parse_method,
                    server_url=server_url,
                    
                    start_page_id=start_page_id,
                    end_page_id=end_page_id,
                    formula_enable=formula_enable,
                    table_enable=table_enable,
                    
                    f_draw_layout_bbox=f_draw_layout_bbox,
                    f_draw_span_bbox=f_draw_span_bbox,
                    f_dump_md=f_dump_md,
                    f_dump_middle_json=f_dump_middle_json,
                    f_dump_model_output=f_dump_model_output,
                    f_dump_orig_pdf=f_dump_orig_pdf,
                    f_dump_content_list=f_dump_content_list
                )

                # ç»“æœæå–ä¸æ¬è¿
                generated_result_dir = temp_work_dir / "result"
                
                if not generated_result_dir.exists():
                    temp_md_files = list(temp_work_dir.rglob("*.md"))
                    if temp_md_files:
                        generated_result_dir = temp_md_files[0].parent.parent
                    else:
                        temp_json_files = list(temp_work_dir.rglob("*_content_list.json"))
                        if temp_json_files:
                             generated_result_dir = temp_json_files[0].parent.parent
                        else:
                             raise FileNotFoundError("Processing failed internally - No output generated")

                # 1. è¯»å–å†…å®¹å¹¶è¿›è¡Œæ·±åº¦æ¸…æ´—
                content = ""
                json_content = None
                
                temp_md_files = list(generated_result_dir.rglob("*.md"))
                if temp_md_files:
                    md_file = temp_md_files[0]
                    raw_content = md_file.read_text(encoding="utf-8")
                    
                    # =========================================================
                    # ã€æ ¸å¿ƒä¿®å¤ã€‘è°ƒç”¨ _clean_markdown è¿›è¡Œæ·±åº¦æ¸…æ´—
                    # è§£å†³ &gt;, \mathrm{}, <del> ç­‰é—®é¢˜
                    # =========================================================
                    content = self._clean_markdown(raw_content)
                    
                    # è¦†ç›–å†™å…¥æ¸…æ´—åçš„å†…å®¹
                    md_file.write_text(content, encoding="utf-8")
                    
                    logger.info(f"âœ… Read and cleaned MD content: {len(content)} chars")
                
                temp_json_files = list(generated_result_dir.rglob("*_content_list.json"))
                if temp_json_files:
                    try:
                        with open(temp_json_files[0], "r", encoding="utf-8") as f:
                            json_content = json.load(f)
                    except: pass

                # 2. å¦‚æœ MD ä¸ºç©ºï¼Œå°è¯•ä» JSON æ¢å¤
                if not content.strip() and json_content:
                    logger.warning("âš ï¸  Markdown file is empty, attempting to recover text from JSON...")
                    recovered_text = []
                    if isinstance(json_content, list):
                        for block in json_content:
                            text = block.get("text", "")
                            # æ¢å¤æ—¶ä¹Ÿåšæ¸…æ´—
                            text = self._clean_markdown(text)
                            recovered_text.append(text)
                    content = "\n\n".join(recovered_text)
                    
                    # å¦‚æœæœ‰ MD æ–‡ä»¶ï¼Œæ›´æ–°å®ƒ
                    if temp_md_files:
                        temp_md_files[0].write_text(content, encoding="utf-8")
                        
                    logger.info(f"â„¹ï¸  Recovered {len(content)} chars from JSON")

                # 3. æ¬è¿æ–‡ä»¶
                logger.info(f"ğŸ“¦ Moving results from {generated_result_dir} to {final_output_dir}")
                if generated_result_dir.exists():
                    for src_path in generated_result_dir.rglob("*"):
                        if src_path.is_file():
                            rel_path = src_path.relative_to(generated_result_dir)
                            dest_path = final_output_dir / rel_path
                            dest_path.parent.mkdir(parents=True, exist_ok=True)
                            shutil.copy2(src_path, dest_path)

                # 4. è¿”å›è·¯å¾„
                final_md_path = None
                final_json_path = None
                
                final_mds = list(final_output_dir.rglob("*.md"))
                if final_mds:
                    final_md_path = str(final_mds[0])
                    # è¦†ç›–å†™å…¥æ¸…æ´—åçš„å†…å®¹ (åŒé‡ä¿é™©)
                    Path(final_md_path).write_text(content, encoding="utf-8")
                
                final_jsons = list(final_output_dir.rglob("*_content_list.json"))
                if final_jsons: final_json_path = str(final_jsons[0])

                if not content.strip():
                    layout_pdfs = list(final_output_dir.rglob("*_layout.pdf"))
                    if layout_pdfs:
                        content = "> âš ï¸ Text extraction returned empty content. Please check layout PDF."
                    else:
                        raise FileNotFoundError("No valid content generated.")

                return {
                    "markdown": content,
                    "result_path": str(final_output_dir),
                    "markdown_file": final_md_path,
                    "json_path": final_json_path,
                    "json_content": json_content
                }

        except Exception as e:
            logger.error(f"âŒ Pipeline processing failed: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            raise

        finally:
            self.cleanup()


# å…¨å±€å•ä¾‹
_engine = None

def get_engine(vlm_api_base: str = None) -> MinerUPipelineEngine:
    global _engine
    if _engine is None:
        _engine = MinerUPipelineEngine(vlm_api_base=vlm_api_base)
    return _engine
