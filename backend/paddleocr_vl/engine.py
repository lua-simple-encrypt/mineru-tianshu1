"""
PaddleOCR-VL è§£æžå¼•æ“Ž (PaddleX v3 Local Wrapper)
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡åž‹
æ”¯æŒè‡ªåŠ¨å¤šè¯­è¨€è¯†åˆ«ã€Markdown æ ¼å¼è¾“å‡º

ä¼˜åŒ–æ—¥å¿— (2026-02-15):
1. [æ–°å¢ž] æ™ºèƒ½æ˜¾å­˜ä¼‘çœ  (Auto-Sleep): ç©ºé—² 5 åˆ†é’Ÿè‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜
2. [æ–°å¢ž] è‡ªåŠ¨å”¤é†’ (Auto-Wakeup): æ–°è¯·æ±‚è‡ªåŠ¨åŠ è½½æ¨¡åž‹
3. [ä¼˜åŒ–] ç§»é™¤å•æ¬¡ä»»åŠ¡åŽçš„å¼ºåˆ¶æ¸…ç†ï¼Œæå‡è¿žç»­å¤„ç†æ€§èƒ½
4. [åŸºç¡€] å¼ºåˆ¶å•çº¿ç¨‹ä¸Žç¦ç”¨è”ç½‘æ£€æŸ¥
"""

import os
import sys
import gc
import json
import time
import traceback
import threading
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger

# ==============================================================================
# ðŸš¨ å…¨å±€çŽ¯å¢ƒé…ç½® (å¿…é¡»åœ¨å¯¼å…¥ paddlex ä¹‹å‰è®¾ç½®)
# ==============================================================================
# 1. é™åˆ¶ PaddleX å†…éƒ¨æŽ¨ç†å¹¶å‘æ•°ä¸º 1ï¼Œé˜²æ­¢é«˜å¹¶å‘è¯·æ±‚å¯¼è‡´æ˜¾å­˜æº¢å‡ºæˆ–åº•å±‚ C++ ç«žæ€å†²çª
os.environ["PADDLEX_INFERENCE_PARALLEL_WORKER_NUM"] = "1"
# 2. ç¦ç”¨æ¨¡åž‹æºæ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦ (å†…ç½‘/æ— ç½‘çŽ¯å¢ƒå¿…å¤‡)
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True"
# ==============================================================================

# å°è¯•å¯¼å…¥ paddle å’Œ paddlex
try:
    import paddle
    from paddlex import create_pipeline
    PADDLE_AVAILABLE = True
except ImportError:
    PADDLE_AVAILABLE = False
    logger.warning("âš ï¸ PaddlePaddle or PaddleX not installed. Please install: pip install paddlepaddle-gpu paddlex")

class PaddleOCRVLEngine:
    """
    PaddleOCR-VL è§£æžå¼•æ“Žï¼ˆåŸºäºŽ PaddleX v3 æœ¬åœ°æŽ¨ç†ï¼‰

    ç‰¹æ€§ï¼š
    - å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿è¿›ç¨‹å†…åªæœ‰ä¸€ä¸ªæ¨¡åž‹å®žä¾‹
    - æ™ºèƒ½æ˜¾å­˜ç®¡ç†ï¼šç©ºé—²è‡ªåŠ¨é‡Šæ”¾ï¼Œä½¿ç”¨æ—¶è‡ªåŠ¨åŠ è½½
    - æ ¼å¼æ”¯æŒï¼šè¾“å‡º Markdown å’Œ JSON
    - ç¨³å®šæ€§ï¼šå¼ºåˆ¶ä¸²è¡Œå¤„ç†ï¼Œé˜²æ­¢ OOM
    """

    _instance: Optional["PaddleOCRVLEngine"] = None
    _lock = Lock()
    _pipeline = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, device: str = "cuda:0", model_name: str = "PaddleOCR-VL-1.5-0.9B"):
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            self.model_name = model_name
            self.gpu_id = 0

            # è§£æž GPU ID
            if "cuda" in device.lower():
                try:
                    parts = device.split(":")
                    if len(parts) > 1:
                        self.gpu_id = int(parts[-1])
                except ValueError:
                    self.gpu_id = 0
                    logger.warning(f"âš ï¸ Invalid device format '{device}', defaulting to GPU 0")

            self._check_environment()
            
            # =========================================================
            # [æ–°å¢ž] æ™ºèƒ½æ˜¾å­˜ç®¡ç†çŠ¶æ€å˜é‡
            # =========================================================
            self.last_active_time = time.time()
            self.is_processing = False
            self.is_offloaded = True # åˆå§‹çŠ¶æ€è§†ä¸ºæœªåŠ è½½
            self.idle_timeout = 300  # 5åˆ†é’Ÿæ— æ“ä½œè‡ªåŠ¨å¸è½½

            # å¯åŠ¨ç›‘æŽ§çº¿ç¨‹
            self._monitor_thread = threading.Thread(target=self._auto_sleep_monitor, daemon=True)
            self._monitor_thread.start()
            
            self._initialized = True
            logger.info(f"ðŸ”§ PaddleOCR-VL Local Engine initialized (Model: {self.model_name}, GPU: {self.gpu_id})")
            logger.info(f"â³ Auto-sleep monitor enabled (Timeout: {self.idle_timeout}s)")

    def _check_environment(self):
        """æ£€æŸ¥ GPU å’Œ Paddle çŽ¯å¢ƒ"""
        if not PADDLE_AVAILABLE:
            raise ImportError("PaddlePaddle environment is missing.")

        if not paddle.device.is_compiled_with_cuda():
            logger.error("âŒ PaddlePaddle is installed but NOT compiled with CUDA.")
            raise RuntimeError("PaddlePaddle CUDA version required.")

        try:
            gpu_name = paddle.device.cuda.get_device_name(self.gpu_id)
            logger.info(f"âœ… GPU Detected: {gpu_name}")
        except Exception:
            pass

    def _auto_sleep_monitor(self):
        """
        [åŽå°çº¿ç¨‹] ç›‘æŽ§ç©ºé—²çŠ¶æ€
        """
        while True:
            time.sleep(10)
            try:
                # å¦‚æžœæ­£åœ¨å¤„ç†ï¼Œæˆ–è€…å·²ç»å¸è½½ï¼Œè·³è¿‡
                if self.is_processing or self.is_offloaded:
                    continue
                
                # æ£€æŸ¥ç©ºé—²æ—¶é—´
                if time.time() - self.last_active_time > self.idle_timeout:
                    logger.info(f"ðŸ’¤ PaddleOCR idle for {self.idle_timeout}s. Unloading model to save VRAM...")
                    self.cleanup()
                    self.is_offloaded = True
            except Exception as e:
                logger.error(f"Monitor error: {e}")

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ PaddleX Pipeline"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            logger.info(f"ðŸ“¥ Loading PaddleOCR-VL Pipeline: {self.model_name}...")
            start_time = time.time()

            # è®¾ç½®è®¾å¤‡
            paddle.set_device(f"gpu:{self.gpu_id}")

            # ç¡®å®šæ¨¡åž‹è·¯å¾„ (ä¼˜å…ˆä½¿ç”¨ PADDLEX_HOME ç¼“å­˜)
            pdx_home = os.environ.get("PADDLEX_HOME", "/root/.paddlex")
            local_cache_path = Path(pdx_home) / "official_models" / self.model_name
            
            pipeline_source = self.model_name

            # å¦‚æžœæœ¬åœ°å­˜åœ¨æ¨¡åž‹æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
            if local_cache_path.exists() and any(local_cache_path.iterdir()):
                logger.info(f"ðŸ“‚ Found local model cache: {local_cache_path}")
                pipeline_source = str(local_cache_path)
            else:
                logger.info(f"ðŸŒ Local model not found at {local_cache_path}, attempting auto-download...")
            
            try:
                # åˆ›å»º Pipeline
                self._pipeline = create_pipeline(
                    pipeline=pipeline_source,
                    device=f"gpu:{self.gpu_id}",
                    use_hpip=False # ç¦ç”¨é«˜æ€§èƒ½æŽ¨ç†ä»£ç†ï¼Œä»¥èŽ·å¾—æ›´å¥½å…¼å®¹æ€§
                )
                
                logger.success(f"âœ… Pipeline loaded in {time.time() - start_time:.2f}s")
                return self._pipeline

            except Exception as e:
                logger.error(f"âŒ Failed to load pipeline: {e}")
                logger.error(traceback.format_exc())
                raise RuntimeError(f"PaddleOCR-VL load failed: {e}")

    def cleanup(self):
        """
        [å¢žå¼ºç‰ˆ] æ¸…ç†æ˜¾å­˜ä¸Žæ¨¡åž‹å¼•ç”¨
        """
        with self._lock:
            # 1. é”€æ¯ Pipeline å¼•ç”¨
            self._pipeline = None
            
            # 2. å¼ºåˆ¶åžƒåœ¾å›žæ”¶
            gc.collect()
            
            # 3. æ¸…ç† CUDA ç¼“å­˜
            try:
                if PADDLE_AVAILABLE and paddle.device.is_compiled_with_cuda():
                    paddle.device.cuda.empty_cache()
                    # éƒ¨åˆ†ç‰ˆæœ¬çš„ Paddle å¯èƒ½éœ€è¦é¢å¤–è°ƒç”¨ ipu/xpu æ¸…ç†ï¼Œæ­¤å¤„ä»…å¤„ç† cuda
                logger.info("âœ… PaddleOCR-VL model unloaded and VRAM released.")
            except Exception as e:
                logger.debug(f"Cleanup warning: {e}")

    def parse(self, file_path: str, output_path: str, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œè§£æž (å¢žå¼ºç‰ˆï¼šè‡ªåŠ¨å”¤é†’ + çŠ¶æ€ç»´æŠ¤)
        """
        # =========================================================
        # 1. çŠ¶æ€æ›´æ–°ä¸Žè‡ªåŠ¨å”¤é†’
        # =========================================================
        self.is_processing = True
        self.last_active_time = time.time()
        
        if self.is_offloaded:
            logger.info("ðŸš€ New task received. Waking up PaddleOCR-VL engine...")
            self.is_offloaded = False
            # _load_pipeline() ä¼šè‡ªåŠ¨é‡å»ºæ¨¡åž‹

        try:
            file_path = Path(file_path)
            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)

            logger.info(f"ðŸ¤– Processing: {file_path.name}")
            
            pipeline = self._load_pipeline()
            
            # å‚æ•°æ˜ å°„è¡¨ (API é©¼å³° -> PaddleX ä¸‹åˆ’çº¿)
            param_mapping = {
                "useDocOrientationClassify": "use_doc_orientation_classify",
                "useDocUnwarping": "use_doc_unwarping",
                "useLayoutDetection": "use_layout_parsing",
                "useChartRecognition": "use_chart_recognition",
                "useSealRecognition": "use_seal_recognition",
                "useOcrForImageBlock": "use_ocr_for_image_block",
                "layoutNms": "layout_nms",
                "markdownIgnoreLabels": "markdown_ignore_labels",
                "mergeTables": "merge_tables",
                "relevelTitles": "relevel_titles",
                "restructurePages": "restructure_pages",
                "layoutShapeMode": "layout_shape_mode",
                "minPixels": "min_pixels",
                "maxPixels": "max_pixels",
            }

            # 1. è§„èŒƒåŒ–å‚æ•°
            predict_params = {}
            for k, v in kwargs.items():
                if k in param_mapping:
                    predict_params[param_mapping[k]] = v

            # åŠ¨æ€æ£€æŸ¥ pipeline æ˜¯å¦å…·å¤‡é¢„å¤„ç†èƒ½åŠ›
            has_preprocessor = hasattr(pipeline, "doc_preprocessor_pipeline") and pipeline.doc_preprocessor_pipeline is not None
            
            req_orientation = predict_params.get("use_doc_orientation_classify", False)
            req_unwarping = predict_params.get("use_doc_unwarping", False)

            if (req_orientation or req_unwarping) and not has_preprocessor:
                logger.warning("âš ï¸ è¯·æ±‚äº†æ–‡æ¡£çŸ«æ­£/åˆ†ç±»ï¼Œä½†æ¨¡åž‹ç¼ºå°‘é¢„å¤„ç†æ¨¡å—ã€‚å·²è‡ªåŠ¨ç¦ç”¨ä»¥é˜²æ­¢å´©æºƒã€‚")
                predict_params["use_doc_orientation_classify"] = False
                predict_params["use_doc_unwarping"] = False
            
            # é»˜è®¤å‚æ•°å…œåº•
            if "use_layout_parsing" not in predict_params:
                predict_params["use_layout_parsing"] = True
            if "use_seal_recognition" not in predict_params:
                predict_params["use_seal_recognition"] = True

            predict_params["input"] = str(file_path)

            log_params = {k: v for k, v in predict_params.items() if k != "input"}
            logger.info(f"ðŸš€ å¼€å§‹æŽ¨ç† (å‚æ•°: {json.dumps(log_params, default=str, ensure_ascii=False)})")
            
            # æ‰§è¡ŒæŽ¨ç† (æµå¼)
            output_generator = pipeline.predict(**predict_params)
            
            markdown_pages = []
            json_list = []
            page_count = 0
            
            for res in output_generator:
                page_count += 1
                
                # ðŸ›¡ï¸ å…³é”®é˜²å¾¡
                if res is None:
                    logger.error(f"âŒ Page {page_count} returned None result")
                    continue

                page_dir = output_path / f"page_{page_count}"
                page_dir.mkdir(parents=True, exist_ok=True)

                # 1. ä¿å­˜å›¾ç‰‡å’ŒJSON
                try:
                    if hasattr(res, "save_to_img"): res.save_to_img(str(page_dir))
                    if hasattr(res, "save_to_json"): res.save_to_json(str(page_dir))
                except Exception as e:
                    logger.warning(f"âš ï¸ Page {page_count}: Failed to save assets: {e}")

                # 2. æ”¶é›† JSON æ•°æ®
                if hasattr(res, "json") and res.json:
                    json_list.append(res.json)

                # 3. æå– Markdown
                page_md = ""
                try:
                    if hasattr(res, "markdown") and res.markdown:
                        if isinstance(res.markdown, dict):
                            page_md = res.markdown.get('markdown_texts', '') or res.markdown.get('text', '')
                        elif hasattr(res.markdown, 'markdown_texts'):
                            page_md = res.markdown.markdown_texts
                        else:
                            page_md = str(res.markdown)
                    elif hasattr(res, "str") and res.str:
                        page_md = str(res.str)
                except Exception as e:
                    logger.warning(f"âš ï¸ Page {page_count}: Markdown extraction error: {e}")
                
                # 4. å…œåº•è¯»å–æ–‡ä»¶
                if not page_md and hasattr(res, "save_to_markdown"):
                    try:
                        res.save_to_markdown(str(page_dir))
                        saved_mds = list(page_dir.glob("*.md"))
                        if saved_mds:
                            page_md = saved_mds[0].read_text(encoding="utf-8")
                    except Exception:
                        pass

                if page_md:
                    markdown_pages.append(page_md)
                else:
                    logger.warning(f"âš ï¸ Page {page_count}: No markdown content extracted.")

            logger.info(f"ðŸ“„ Successfully processed {page_count} pages")

            # åˆå¹¶ç»“æžœ
            full_markdown = "\n\n---\n\n".join(markdown_pages)
            
            final_md_path = output_path / "result.md"
            final_md_path.write_text(full_markdown, encoding="utf-8")
            
            final_json_path = output_path / "result.json"
            combined_data = {
                "total_pages": page_count,
                "pages": json_list
            }
            with open(final_json_path, "w", encoding="utf-8") as f:
                json.dump(combined_data, f, ensure_ascii=False, indent=2)

            return {
                "success": True,
                "result_path": str(output_path),
                "markdown": full_markdown,
                "markdown_file": str(final_md_path),
                "json_file": str(final_json_path)
            }

        except Exception as e:
            logger.error(f"âŒ Inference failed: {e}")
            logger.error(traceback.format_exc())
            raise
        finally:
            # =========================================================
            # [å…³é”®ä¿®æ”¹]
            # ç§»é™¤å¼ºåˆ¶ self.cleanup()ï¼Œè®©æ¨¡åž‹ä¿æŒåŠ è½½çŠ¶æ€
            # æ›´æ–°æ—¶é—´æˆ³ï¼Œè®©åŽå°çº¿ç¨‹åœ¨ç©ºé—²5åˆ†é’ŸåŽå¤„ç†é‡Šæ”¾
            # =========================================================
            self.is_processing = False
            self.last_active_time = time.time()
            logger.info("ðŸ Task finished. Model remains loaded for fast reuse (Auto-sleep in 5min).")

# å…¨å±€å•ä¾‹
_engine_instance = None

def get_engine(model_name: str = "PaddleOCR-VL-1.5-0.9B") -> PaddleOCRVLEngine:
    global _engine_instance
    if _engine_instance is None:
        _engine_instance = PaddleOCRVLEngine(model_name=model_name)
    return _engine_instance
