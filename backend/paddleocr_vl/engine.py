"""
PaddleOCR-VL è§£æžå¼•æ“Ž (PaddleX v3 Local Wrapper)
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡åž‹
æ”¯æŒè‡ªåŠ¨å¤šè¯­è¨€è¯†åˆ«ã€Markdown æ ¼å¼è¾“å‡º

ä¼˜åŒ–æ—¥å¿—ï¼š
1. å¼ºåˆ¶å•çº¿ç¨‹ (PARALLEL_WORKER_NUM=1) ä»¥è§£å†³ "Already borrowed" ç«žæ€å´©æºƒ
2. ç¦ç”¨æ¨¡åž‹æºè”ç½‘æ£€æŸ¥ (DISABLE_MODEL_SOURCE_CHECK) åŠ å¿«å¯åŠ¨
3. å¢žåŠ ç»“æžœå¤„ç†çš„é˜²å¾¡æ€§æ£€æŸ¥ (NoneType protection)
"""

import os
import sys
import gc
import json
import time
import traceback
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger

# ==============================================================================
# ðŸš¨ å…¨å±€çŽ¯å¢ƒé…ç½® (å¿…é¡»åœ¨å¯¼å…¥ paddlex ä¹‹å‰è®¾ç½®)
# ==============================================================================
# 1. é™åˆ¶ PaddleX å†…éƒ¨æŽ¨ç†å¹¶å‘æ•°ä¸º 1ï¼Œé˜²æ­¢é«˜å¹¶å‘è¯·æ±‚å¯¼è‡´æ˜¾å­˜æº¢å‡ºæˆ–åº•å±‚ C++ ç«žæ€å†²çª
os.environ["PADDLEX_INFERENCE_PARALLEL_WORKER_NUM"] = "1"
# 2. ç¦ç”¨æ¨¡åž‹æºæ£€æŸ¥ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦ (å†…ç½‘/æ— ç½‘çŽ¯å¢ƒå¿…å¤‡)
os.environ["PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK"] = "True"
# ==============================================================================

# å°è¯•å¯¼å…¥ paddle å’Œ paddlex
try:
    import paddle
    from paddlex import create_pipeline
    PADDLE_AVAILABLE = True
except ImportError:
    PADDLE_AVAILABLE = False
    logger.warning("âš ï¸ PaddlePaddle or PaddleX not installed. Please install: pip install paddlepaddle-gpu paddlex")

class PaddleOCRVLEngine:
    """
    PaddleOCR-VL è§£æžå¼•æ“Žï¼ˆåŸºäºŽ PaddleX v3 æœ¬åœ°æŽ¨ç†ï¼‰

    ç‰¹æ€§ï¼š
    - å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿è¿›ç¨‹å†…åªæœ‰ä¸€ä¸ªæ¨¡åž‹å®žä¾‹
    - æ˜¾å­˜ç®¡ç†ï¼šæ”¯æŒæŽ¨ç†åŽæ¸…ç†æ˜¾å­˜
    - æ ¼å¼æ”¯æŒï¼šè¾“å‡º Markdown å’Œ JSON
    - ç¨³å®šæ€§ï¼šå¼ºåˆ¶ä¸²è¡Œå¤„ç†ï¼Œé˜²æ­¢ OOM
    """

    _instance: Optional["PaddleOCRVLEngine"] = None
    _lock = Lock()
    _pipeline = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, device: str = "cuda:0", model_name: str = "PaddleOCR-VL-1.5-0.9B"):
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.device = device
            self.model_name = model_name
            self.gpu_id = 0

            # è§£æž GPU ID
            if "cuda" in device.lower():
                try:
                    parts = device.split(":")
                    if len(parts) > 1:
                        self.gpu_id = int(parts[-1])
                except ValueError:
                    self.gpu_id = 0
                    logger.warning(f"âš ï¸ Invalid device format '{device}', defaulting to GPU 0")

            self._check_environment()
            
            self._initialized = True
            logger.info(f"ðŸ”§ PaddleOCR-VL Local Engine initialized (Model: {self.model_name}, Device: GPU {self.gpu_id})")

    def _check_environment(self):
        """æ£€æŸ¥ GPU å’Œ Paddle çŽ¯å¢ƒ"""
        if not PADDLE_AVAILABLE:
            raise ImportError("PaddlePaddle environment is missing.")

        if not paddle.device.is_compiled_with_cuda():
            logger.error("âŒ PaddlePaddle is installed but NOT compiled with CUDA.")
            raise RuntimeError("PaddlePaddle CUDA version required.")

        try:
            gpu_name = paddle.device.cuda.get_device_name(self.gpu_id)
            logger.info(f"âœ… GPU Detected: {gpu_name}")
        except Exception:
            pass

    def _load_pipeline(self):
        """å»¶è¿ŸåŠ è½½ PaddleX Pipeline"""
        if self._pipeline is not None:
            return self._pipeline

        with self._lock:
            if self._pipeline is not None:
                return self._pipeline

            logger.info(f"ðŸ“¥ Loading PaddleOCR-VL Pipeline: {self.model_name}...")
            start_time = time.time()

            # è®¾ç½®è®¾å¤‡
            paddle.set_device(f"gpu:{self.gpu_id}")

            # ç¡®å®šæ¨¡åž‹è·¯å¾„ (ä¼˜å…ˆä½¿ç”¨ PADDLEX_HOME ç¼“å­˜)
            # é»˜è®¤å®˜æ–¹æ¨¡åž‹è·¯å¾„é€šå¸¸åœ¨ ~/.paddlex/official_models/
            pdx_home = os.environ.get("PADDLEX_HOME", "/root/.paddlex")
            local_cache_path = Path(pdx_home) / "official_models" / self.model_name
            
            pipeline_source = self.model_name # é»˜è®¤ä¸ºæ¨¡åž‹åç§°ï¼Œè®© PaddleX è‡ªåŠ¨å¤„ç†

            # å¦‚æžœæœ¬åœ°å­˜åœ¨æ¨¡åž‹æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œé¿å…å°è¯•ä¸‹è½½
            if local_cache_path.exists() and any(local_cache_path.iterdir()):
                logger.info(f"ðŸ“‚ Found local model cache: {local_cache_path}")
                pipeline_source = str(local_cache_path)
            else:
                logger.info(f"ðŸŒ Local model not found at {local_cache_path}, attempting auto-download...")
            
            try:
                # åˆ›å»º Pipeline
                self._pipeline = create_pipeline(
                    pipeline=pipeline_source,
                    device=f"gpu:{self.gpu_id}",
                    use_hpip=False # ç¦ç”¨é«˜æ€§èƒ½æŽ¨ç†ä»£ç†ï¼Œä»¥èŽ·å¾—æ›´å¥½å…¼å®¹æ€§
                )
                
                logger.success(f"âœ… Pipeline loaded in {time.time() - start_time:.2f}s")
                return self._pipeline

            except Exception as e:
                logger.error(f"âŒ Failed to load pipeline: {e}")
                logger.error(traceback.format_exc())
                raise RuntimeError(f"PaddleOCR-VL load failed: {e}")

    def parse(self, file_path: str, output_path: str, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œè§£æž

        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºç›®å½•
            **kwargs: æ”¯æŒ PaddleOCR-VL çš„æ‰€æœ‰å‚æ•° (æ”¯æŒé©¼å³°æˆ–ä¸‹åˆ’çº¿å‘½å)
        """
        file_path = Path(file_path)
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"ðŸ¤– Processing: {file_path.name}")
        
        pipeline = self._load_pipeline()
        
        # å‚æ•°æ˜ å°„è¡¨ (API é©¼å³° -> PaddleX ä¸‹åˆ’çº¿)
        param_mapping = {
            "useDocOrientationClassify": "use_doc_orientation_classify",
            "useDocUnwarping": "use_doc_unwarping",
            "useLayoutDetection": "use_layout_parsing",
            "useChartRecognition": "use_chart_recognition",
            "useSealRecognition": "use_seal_recognition",
            "useOcrForImageBlock": "use_ocr_for_image_block",
            "layoutNms": "layout_nms",
            "markdownIgnoreLabels": "markdown_ignore_labels",
            "mergeTables": "merge_tables",
            "relevelTitles": "relevel_titles",
            "restructurePages": "restructure_pages",
            "layoutShapeMode": "layout_shape_mode",
            "minPixels": "min_pixels",
            "maxPixels": "max_pixels",
        }

        # 1. è§„èŒƒåŒ–å‚æ•°å¹¶è¿‡æ»¤æ— å…³å‚æ•°
        predict_params = {}
        for k, v in kwargs.items():
            if k in param_mapping:
                predict_params[param_mapping[k]] = v

        try:
            # =================================================================
            # åŠ¨æ€æ£€æŸ¥ pipeline æ˜¯å¦å…·å¤‡é¢„å¤„ç†èƒ½åŠ›
            # =================================================================
            has_preprocessor = hasattr(pipeline, "doc_preprocessor_pipeline") and pipeline.doc_preprocessor_pipeline is not None
            
            req_orientation = predict_params.get("use_doc_orientation_classify", False)
            req_unwarping = predict_params.get("use_doc_unwarping", False)

            # å¦‚æžœè¯·æ±‚äº†é¢„å¤„ç†åŠŸèƒ½ä½†æ¨¡åž‹ä¸æ”¯æŒï¼Œå¼ºåˆ¶å…³é—­å¹¶è­¦å‘Š
            if (req_orientation or req_unwarping) and not has_preprocessor:
                logger.warning("âš ï¸ è¯·æ±‚äº†æ–‡æ¡£çŸ«æ­£/åˆ†ç±»ï¼Œä½†æ¨¡åž‹ç¼ºå°‘é¢„å¤„ç†æ¨¡å—ã€‚å·²è‡ªåŠ¨ç¦ç”¨ä»¥é˜²æ­¢å´©æºƒã€‚")
                predict_params["use_doc_orientation_classify"] = False
                predict_params["use_doc_unwarping"] = False
            
            # é»˜è®¤å‚æ•°å…œåº•
            if "use_layout_parsing" not in predict_params:
                predict_params["use_layout_parsing"] = True
            if "use_seal_recognition" not in predict_params:
                predict_params["use_seal_recognition"] = True # é»˜è®¤å¼€å¯å°ç« è¯†åˆ«

            # è®¾ç½®è¾“å…¥æ–‡ä»¶
            predict_params["input"] = str(file_path)

            log_params = {k: v for k, v in predict_params.items() if k != "input"}
            logger.info(f"ðŸš€ å¼€å§‹æŽ¨ç† (å‚æ•°: {json.dumps(log_params, default=str, ensure_ascii=False)})")
            
            # æ‰§è¡ŒæŽ¨ç† (æµå¼ç”Ÿæˆå™¨)
            # æ³¨æ„ï¼šç”±äºŽæˆ‘ä»¬åœ¨æ–‡ä»¶å¤´è®¾ç½®äº† PARALLEL_WORKER_NUM=1ï¼Œè¿™é‡Œä¼šä¸²è¡Œå¤„ç†
            output_generator = pipeline.predict(**predict_params)
            
            markdown_pages = []
            json_list = []
            page_count = 0
            
            for res in output_generator:
                page_count += 1
                
                # ðŸ›¡ï¸ å…³é”®é˜²å¾¡ï¼šé˜²æ­¢ None ç»“æžœå¯¼è‡´å´©æºƒ
                if res is None:
                    logger.error(f"âŒ Page {page_count} returned None result")
                    continue

                page_dir = output_path / f"page_{page_count}"
                page_dir.mkdir(parents=True, exist_ok=True)

                # 1. ä¿å­˜å›¾ç‰‡å’ŒJSON
                try:
                    if hasattr(res, "save_to_img"): res.save_to_img(str(page_dir))
                    if hasattr(res, "save_to_json"): res.save_to_json(str(page_dir))
                except Exception as e:
                    logger.warning(f"âš ï¸ Page {page_count}: Failed to save assets: {e}")

                # 2. æ”¶é›† JSON æ•°æ®
                if hasattr(res, "json") and res.json:
                    json_list.append(res.json)

                # 3. æå– Markdown (å¢žå¼ºå…¼å®¹æ€§)
                page_md = ""
                try:
                    if hasattr(res, "markdown") and res.markdown:
                        if isinstance(res.markdown, dict):
                            # PaddleX æŸäº›ç‰ˆæœ¬è¿”å›ž dict: {'markdown_texts': '...', 'text': '...'}
                            page_md = res.markdown.get('markdown_texts', '') or res.markdown.get('text', '')
                        elif hasattr(res.markdown, 'markdown_texts'):
                            # PaddleX æŸäº›ç‰ˆæœ¬è¿”å›žå¯¹è±¡
                            page_md = res.markdown.markdown_texts
                        else:
                            # æˆ–è€…æ˜¯ç›´æŽ¥çš„å­—ç¬¦ä¸²
                            page_md = str(res.markdown)
                    elif hasattr(res, "str") and res.str:
                        # æŸäº›ç‰¹å®š pipeline å¯èƒ½å­˜å‚¨åœ¨ str å±žæ€§
                        page_md = str(res.str)
                except Exception as e:
                    logger.warning(f"âš ï¸ Page {page_count}: Markdown extraction error: {e}")
                
                # 4. å…œåº•ï¼šå°è¯•è¯»å– save_to_markdown ç”Ÿæˆçš„æ–‡ä»¶
                if not page_md and hasattr(res, "save_to_markdown"):
                    try:
                        res.save_to_markdown(str(page_dir))
                        saved_mds = list(page_dir.glob("*.md"))
                        if saved_mds:
                            page_md = saved_mds[0].read_text(encoding="utf-8")
                    except Exception:
                        pass

                if page_md:
                    markdown_pages.append(page_md)
                else:
                    logger.warning(f"âš ï¸ Page {page_count}: No markdown content extracted.")

            logger.info(f"ðŸ“„ Successfully processed {page_count} pages")

            # åˆå¹¶ç»“æžœ
            full_markdown = "\n\n---\n\n".join(markdown_pages)
            
            final_md_path = output_path / "result.md"
            final_md_path.write_text(full_markdown, encoding="utf-8")
            
            # ä¿å­˜å®Œæ•´ JSON
            final_json_path = output_path / "result.json"
            combined_data = {
                "total_pages": page_count,
                "pages": json_list
            }
            with open(final_json_path, "w", encoding="utf-8") as f:
                json.dump(combined_data, f, ensure_ascii=False, indent=2)

            return {
                "success": True,
                "result_path": str(output_path),
                "markdown": full_markdown,
                "markdown_file": str(final_md_path),
                "json_file": str(final_json_path)
            }

        except Exception as e:
            logger.error(f"âŒ Inference failed: {e}")
            logger.error(traceback.format_exc())
            raise
        finally:
            self.cleanup()

    def cleanup(self):
        """æ¸…ç†æ˜¾å­˜"""
        try:
            if PADDLE_AVAILABLE and paddle.device.is_compiled_with_cuda():
                paddle.device.cuda.empty_cache()
            gc.collect()
            logger.debug("ðŸ§¹ Memory cleanup completed")
        except Exception as e:
            logger.debug(f"Cleanup warning: {e}")

# å…¨å±€å•ä¾‹
_engine_instance = None

def get_engine(model_name: str = "PaddleOCR-VL-1.5-0.9B") -> PaddleOCRVLEngine:
    global _engine_instance
    if _engine_instance is None:
        _engine_instance = PaddleOCRVLEngine(model_name=model_name)
    return _engine_instance
